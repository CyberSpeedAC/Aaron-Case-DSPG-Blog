{
  "hash": "703c16ce49afcd4b58776d9839dae0cc",
  "result": {
    "markdown": "---\ntitle: \"Web Scraping\"\nauthor: \"Aaron Case\"\ndate: \"2023-07-14\"\ncategories: [Web Scraping, Code]\n---\n\nWeb scraping is the process of extracting data from websites using automated software. In the context of web scraping, a Spider (also known as a web crawler) is a program or script designed to navigate through one or multiple websites and automatically extract data from multiple pages. The Spider \"crawls\" along the web, hence its name.\n\nWeb scraping in essence is the process of extracting data from websites using automated software. As for the process, the web-scraping pipeline goes as follows.\n\n1.  Setup (Objective) - Firstly, we need to clarify our goal and identify relevant sources to assist us in achieving it.\n\n2.  Acquisition\n    -   Read in the raw data from online.\n    -   Format these data to be usable.\n    -   Accessing the data.\n    -   Parsing this information.\n    -   Extracting these data into meaningful and useful data structures.\n\n2. Acquisition - This is where the spiders come into play. They are responsible for:\n- Reading the raw data from online sources.\n- Formatting the data to be usable.\n- Accessing the data.\n- Parsing the information.\n- Extracting the data into meaningful and useful data structures.\n\n3. Processing - There are various options available for this step, but the primary objective is to run the downloaded data through the necessary analyses or processes, including data analysis, to accomplish the desired goal.\n\nBefore going any further there are a few concepts/topics that you need to understand.\n- The basics of HTML\n- Basic programming experience\n- \n\n\nThere are many web scraping tools available, but two of the one's that I've the most success in are Scrapy and Selenium.\n\nScrapy is a Python package that is primarily used for web scraping. Scrapy provides a framework that offers a simple web crawling tool to extract data from websites. These tools are commonly known as spiders and allow for easy extraction of desired information. A spider program can crawl through websites and extracts data from them (Just like real spiders crawl through there webs). Scrapy works by sending HTTP requests to websites and receiving HTML responses, then the spider parses the HTML to extract the data it needs. There is a Data camp course that is currently available to use and is highly recommended watching. These are some of benefit of using Scrapy that I've found:\n\n1.  Scrapy is designed to be fast and can handle large volumes of data. It's built on top of Twisted, an asynchronous networking framework, which in simple terms makes it very fast. It can handle thousands of requests per second without slowing down. This makes it really handy for the scope of this project.\n2.  Scrapy is highly customizable and can be configured to work with many different websites and data formats. Allowing us to specify how the spider should act when navigating through websites and extract data that we told it to collect. Scrapy also provides features for handling cookies, redirects, and other HTTP features.\n3.  Scrapy comes with built-in support for XPath and CSS selectors. With the right skill set these tools provide a powerful and easy way to extract data from HTML and XML documents.\n4.  Scrapy automatically comes with built-in support for data cleaning, Making the the cleaning and normalize scraped data much easier.\n\nAs for the cons of Scrapy:\n\n1.  Scrapy does have a relatively steep learning curve to it. Especially if you are new to web scraping, XPath/CSS selectors, or asynchronous programming. Unless you have more than a week to spend learning and doing web scrapping it is best to spend the time doing another method. Not to mention the fact that you need to have some knowledge of Python to use it effectively\n2.  Scrapy does not have built-in support for JavaScript, which means that it cannot scrape websites and other dynamic web content that rely heavily on JavaScript for rendering content. Which a vast amount of websites rely on to function.\n3.  This tool is not perfect nor undetectable. Some websites may block Scrapy from accessing their content, especially if they detect that the traffic is coming from a bot.\n4.  Scrapy does help you in cleaning data however it is not perfect. You still need to spend some time cleaning the data that was collected and may require additional libraries, such as Pandas or NumPy, to process and analyze scraped data.\n\nSelenium is another Python package which is used for automating web browsers. It allows you to control a browser programatically, which mimics the actions of a what a user will do. Selenium uses a web driver to control your web browser and can interact with several different browsers (such as Chrome, Firefox, and Safari). Selenium works by opening up a web browser and navigates to the target website. The user can then interact with the website using Selenium commands. Just like Scrapy, Selenium can also extract data from the website using XPath and CSS selectors. As for some of benefit of using Selenium:\n\n1.  Selenium has the ability to handle JavaScript and other dynamic web content. Allowing it to handle websites that Scrapy is unable too.\n2.  Selenium can handle more complex interactions with websites, such as clicking buttons, filling out forms, scrolling, etc. which can simulate user behavior. Making Selenium more diverse and a power tool to use when doing specific tasks that require a more human like approach.\n\nFor the Cons of Selenium:\n\n1.  Selenium can be really slow and very resource-intensive. Especially when automating complex tasks or interacting with multiple pages. It requires your web browser to be opened to run meaning If your internet or computer is slow it has a really hard time doing anything you want it. This time that you have to wait for the program to preform any simple task is not ideal when working through large data sets.\n2.  It is prone to errors and requires extensive debugging and testing. For example, if say that you close your browser or if it freezes (for long periods of time) the program will not function correctly and throw an error.\n3.  It can be very challenging to set up and configure. Not to mention that It can be difficult to maintain test scripts when the web application is updated or changed. Meaning if you are not dedicated in learning or maintaining your code it will come with a bunch of headaches down the road.\n\nIdeally for this project I strongly recommend using Scrapy spiders were ever possible. Scrapy's speed and efficiency and overall better performance makes this part of the process much more tolerable in building spiders. However, when it comes to more complex tasks and websites Selenium is the way to go. I would also like to acknowledge that there are other tools and resources that could have been explored and used which may be better or more efficient for the scope of the project. These two tools are just some that I found and was able to implement in the time frame I was given. For example, there is a python package called Scrapy Splash. This works as an extension for Scrapy which provides a JavaScript rendering service. Allowing Scrapy to handle JavaScript and other dynamic web content, similar to Selenium. I was not able to master this tool, nor could I successfully get it this tool to work. If I had more time to develop a clear comprehension of this utility and fully understood how to get it to work. It would have been the tool of choice to handle such websites. I used Selenium since I had more success in getting it to work in comparison.\n\nThis code covers the bare minimum of Scrapy and is rather (hopefully) intuitive. This can be edited/modified to fit your needs and is just an example.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Imports for Scrapy Example\nimport scrapy #Required Import\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.log import configure_logging # For debugging\n\n# The Class name can be anything but you need to call this later\nclass SpiderClassName(scrapy.Spider):\n    name = \"spider name\" \n\n    def start_requests( self ): # This is required to start the spider\n        # For one url\n        Desired_Website = [ 'https://www.example.com' ]\n        yield scrapy.Request( url = Desired_Website, callback = self.Some_Function_Name)\n\n        # For many urls\n        urls = [ 'https://www.examples.com', ... ]\n        for url in urls:\n            yield scrapy.Request( url = url, callback = self.parseAndFollow)\n\n        # When passing in varables from one function to the next you use meta\n        for url in urls:\n            yield scrapy.Request( url = url, callback = self.Meta_Example, meta={'Meta_Name': data, ...})\n    \n    def parseAndFollow( self, response ):\n        # Direct to the links in the xpath\n        Extrated_XPath = '//tag-name(@attribute, \"attrib info\")'\n        XPath_links = Extrated_XPath.xpath( './@href' )\n        # Anoter way\n        XPath_links = Extrated_XPath.xpath('//tag-name(@attribute, \"attrib info\")/@href')\n        # Extract the links (as a list of strings)\n        links_to_follow = XPath_links.extract()\n        # Follow the links to the next parser\n        for url in links_to_follow:\n            yield response.follow( url = url, callback = self.parse )\n\t\n    def parse( self, response ):\n        # To get the text of the xpath\n        Title_Example = response.xpath('//h1[contains(@class,\"title\")]/text()') \n        # Extract text title\n        Title_Extracted = Title_Example.extract_first()\n        # Clean the title text\n        Title_Text = Title_Extracted.strip()\n        # Saving the text so that we dont lose what we just did\n        Text_Element_List.append(Title_Text)\n\n    def Meta_Example( self, response ):\n        #This is how pass in your arguments\n        Meta_Data = response.meta.get('Meta_Name')\n    \nText_Element_List = []\n# To see the inner mechanics of the spider helpful to have not required\nconfigure_logging()\n# initiate a CrawlerProcess\nprocess = CrawlerProcess()\n# Tell the process which spider to use\nprocess.crawl(SpiderClassName)\n# Start the crawling process\nprocess.start()\n# This is to stop the spider\nprocess.stop()\n# At this point the spider is finished and now we have everything that was saved\nprint(Text_Element_List)\n```\n:::\n\n\nAs for Selenium this code covers the bare minimum and serves as a deminstration of how to handle JavaScript websites. Once again this can be edited/modified to fit your needs and is just an example.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Imports for Scraping\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# Since I use Firefox this is how I would typically set it up.\n# Note: This is not the only way to do it. Its just how I do this part \nfrom selenium.webdriver.firefox.service import Service as FirefoxService\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom os import path\ndriver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n\n# Another example of how you can do it \n# Set the path to the driver\ndriver_path = \"/path/to/chromedriver\"\n# Create a new instance of the Chrome driver\ndriver = webdriver.Chrome(driver_path)\n\n# Navigate to the website page\ndriver.get(\"https://example.com/\")\n\n# Setup\nwaitTime = 10\n\n# I like using XPaths because I am most comfortable using them \nTitle_XPath = '//h1(@class,\"title\")'\n# We need to wait for the page to render before we do anything otherwise the data we want wont be present.\nelements = WebDriverWait(driver, waitTime).until(EC.visibility_of_element_located((By.XPATH, Title_XPath)))\n# You can also do it like this for CSS\nelements = WebDriverWait(driver, waitTime).until(EC.visibility_of_element_located((By.CSS_SELECTOR, Title_CSS)))\n# Extracts the text of the element\nText = elements[0].text\n\n# Close the browser\ndriver.quit()\n\n#Data Has been extracted\nprint(Text)\n```\n:::\n\n\n",
    "supporting": [
      "Aaron_C_WebScraping_files"
    ],
    "filters": [],
    "includes": {}
  }
}