[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A link to the DSPG Blog: DSPG 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aaron Case DSPG Blog",
    "section": "",
    "text": "Local Food Project\n\n\n\n\n\n\n\nPresentation\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nAI/Local Food Team\n\n\n\n\n\n\n  \n\n\n\n\nWeb Scraping\n\n\n\n\n\n\n\nWeb Scraping\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nDemand and supply estimation\n\n\n\n\n\n\n\nPresentation\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\nMohammad Ahnaf Sadat\n\n\n\n\n\n\n  \n\n\n\n\nAI/Local Food Team Week Eight Wrap Up\n\n\n\n\n\n\n\nWeek Eight\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\nAI/Local Food Team\n\n\n\n\n\n\n  \n\n\n\n\nWeek Seven\n\n\n\n\n\n\n\nWeek Seven\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nAI/Local Food Team Week Seven Wrap Up\n\n\n\n\n\n\n\nWeek Seven\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nAI/Local Food Team\n\n\n\n\n\n\n  \n\n\n\n\nWeek Six\n\n\n\n\n\n\n\nWeek Six\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nSpiders\n\n\n\n\n\n\n\nWeek Four\n\n\nWeek Five\n\n\nWeb Scraping\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Five\n\n\n\n\n\n\n\nWeek Five\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Four\n\n\n\n\n\n\n\nWeek Four\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nAI/Local Food Week Three Wrap Up\n\n\n\n\n\n\n\nWeek Three\n\n\nWrap Up\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nAI/Local Food Team\n\n\n\n\n\n\n  \n\n\n\n\nWeek Three\n\n\n\n\n\n\n\nWeek Three\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Three Research\n\n\n\n\n\n\n\nWeek Three\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Two\n\n\n\n\n\n\n\nWeek Two\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nImporting Census Data\n\n\n\n\n\n\n\nWeek Two\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek One\n\n\n\n\n\n\n\nWeek One\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nCensus Visual\n\n\n\n\n\n\n\nWeek Two\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAaron Case\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Aaron_C_FinalWeek/estimation.html",
    "href": "posts/Aaron_C_FinalWeek/estimation.html",
    "title": "Demand and supply estimation",
    "section": "",
    "text": "Demand Estimation:\n\nIn 2017, fresh market consumption was 20.3 pounds per capita. (https://www.agmrc.org/commodities-products/vegetables/tomatoes)\nWe will allocate this demand to each county of Iowa by population.\n\n\n\n\n\n\n\n\nSupply Estimation:\n\nThe growing season of tomatoes is between May to Mid-September in Iowa. (https://www.tomatofest.com/Tomato_Growing_Zone_Maps_s/164.htm)\nWe have 135 days between planting and harvesting tomatoes.\nTomatoes require 100 days to fully mature. However, there are some special varieties of tomatoes that require 50-60 days to mature. (https://www.gardeningknowhow.com/edible/vegetables/tomato/planting-time-for-tomatoes.htm)\nSo, we are considering on average tomatoes take 80 days to be harvested.\nIn this context, we assumed that being mature represents the time between sowing seed and harvesting full-grown tomatoes.\nWe are only considering single cultivation of tomatoes during a year.\nSo, the tomatoes will be harvested during the timeline day 80-135 or, during late July to mid-September, that is 7.85 or 8 weeks.\nIn our calculations, we will estimate the weekly supply of tomatoes from late July to mid-September.\nIn 2017, approximately 157 acres of tomatoes were harvested in the open field. (https://quickstats.nass.usda.gov/)\nIn 2018, the yield for tomatoes in Iowa was 246 CWT per acre. (https://quickstats.nass.usda.gov/)\nConsidering the yield of tomatoes in 2018 and 2017 same, the total production of tomatoes in Iowa in 2017 was 38622 CWT or 1962 Metric Tons.\nAs the production of tomatoes is distributed over 8 weeks (as mentioned in no 8), the weekly supply quantity of tomatoes is (1962/8) or 245.25 Metric Tons.\nConsidering the average farm size in each county of Iowa is the same. The supply of tomatoes can be obtained from the number of farms.\n\n\n\n\n\n\nSupply-Demand Ratio:\n\nThis process leads to total supply of 566.6359 Metric Tons/week and demand of 245.2503 Metric Tons/week for 8 weeks (Late July- Mid-September).\nAs there is no production of tomatoes other than these 8 weeks, the total supply for tomatoes is (245.2503x8) or 1962.0024 Metric Tons.\nOn the other hand, the total annual demand for tomatoes is (566.6359 x 52) or 29465.0668 Metric Tons.\nThis number leads to a supply-demand ratio of 0.06658.\nIn the IMPLAN model for vegetables and melons (not specifically tomatoes), the supply-demand ratio is 0.0380, which is very close to our estimation."
  },
  {
    "objectID": "posts/Aaron_C_FinalWeek/Presentation.html#introduction",
    "href": "posts/Aaron_C_FinalWeek/Presentation.html#introduction",
    "title": "Local Food Project",
    "section": "Introduction",
    "text": "Introduction\nThis project aims to enhance local food markets by providing valuable insights and optimizing the flow of crops. To achieve this, we have chosen three locally produced, non-processed food items: eggs, bacon, and tomatoes.\nWe initially focused on developing web-scraping spiders to collect data on crop prices. This data was utilized to create a comprehensive map of eggs and bacon prices across counties and cities. Subsequently, we conducted an in-depth analysis to understand the pricing dynamics of eggs and bacon in different counties. Finally, we employed optimization techniques to optimize the flow of crops, from supply to demand, with the aim of maximizing overall profit. By integrating web scraping, data analysis, and optimization techniques, our project provides valuable insights and tools for decision-making in the agricultural market.\n\n\n\n\n\n\nWhat we plan to achieve:\n\nOur primary goal is to create a comprehensive map showcasing the prices of eggs and bacon across different counties. This map will enable us to identify pricing trends, patterns, and customer preferences, thereby assisting businesses in making informed decisions.\nIn addition, we aim to develop efficient web-scraping spiders that automate data collection, saving time and enhancing the accuracy of the collected information.\nFurthermore, we will showcase the effectiveness of these spiders by providing a specific crop example, demonstrating their ability to extract relevant data\nFinally, we will optimize the flow of crops to maximize overall profit, taking into consideration factors such as demand estimation, supply dynamics, and transportation costs\nUltimately, we aim to provide insights, tools, and strategies that enhance decision-making and profitability within the local food market.\n\n\n\nHow we plan to achieve:\nTo achieve our objectives,\n\nWe will employ a multi-faceted approach that combines web scraping, data analysis, and optimization techniques. We will collect comprehensive pricing data for eggs and bacon across different counties, employing data mining and analysis to identify trends and patterns. The collected data will be visualized in a comprehensive map that highlights these key insights.\nWe will develop web-scraping spiders specifically tailored to gather data from selected websites. These spiders will automate the data collection process, ensuring a continuous and efficient flow of information for our comprehensive product database.\nWith specific crops, we will showcase the capabilities of our web-scraping spiders, demonstrating their effectiveness in retrieving data for various agricultural products. This demonstration will highlight their value in supporting data-driven decision-making processes.\nTo optimize the crop flow, we will employ demand estimation methodologies, analyze supply dynamics, consider transportation costs, and apply optimization techniques. By considering these factors, we can identify the most profitable routes and strategies for crop distribution, resulting in increased profitability and improved resource allocation.\n\n\n\nWhy it is important:\nThis project holds immense importance for several reasons.\n\nUnderstanding the pricing dynamics of eggs and bacon is crucial for businesses to adapt their strategies, optimize pricing, and stay competitive. A comprehensive map will offer valuable insights into market trends, customer preferences, and suitable selling locations, empowering businesses to make informed decisions.\nAutomating data collection through web-scraping is crucial for timely and accurate information gathering. It saves time, improves data accuracy, and facilitates the creation of a comprehensive product database. This enables businesses to stay up-to-date with market trends and make informed decisions.\nOptimizing crop flow is vital for maximizing profitability and resource allocation. By analyzing demand and supply dynamics, transportation costs, and market trends, businesses can identify the most profitable routes and strategies for crop distribution. This optimization minimizes waste, improves efficiency, and promotes sustainable agricultural practices."
  },
  {
    "objectID": "posts/Aaron_C_FinalWeek/Presentation.html#web-scraping-automating-data-collection",
    "href": "posts/Aaron_C_FinalWeek/Presentation.html#web-scraping-automating-data-collection",
    "title": "Local Food Project",
    "section": "Web Scraping: Automating Data Collection",
    "text": "Web Scraping: Automating Data Collection\nFrom the success of last year’s project, we were able to gather extensive data on products using web scrapers. This year, we were assigned a similar task but with different products. However, we encountered a significant challenge.\nOver the past year, the website we were scraping underwent significant changes, rendering our previous methods and processes obsolete. Upon closer examination, we discovered numerous unintentional flaws in our approach. Unfortunately, the scraper we had built was only capable of functioning under very specific circumstances, severely limiting our ability to collect data effectively even if we were able to repair it.\nTo ensure the success of our current project, it is crucial to gather a substantial amount of data. However, manually performing this task is not only time-consuming but also prone to errors and incredibly tedious. To overcome these challenges, we have developed web crawlers or spiders to automate the process. This automation not only saves time but also guarantees accurate and up-to-date data collection.\nIn simple terms, a Spider or Web crawler is a program or script specifically designed to automate website navigation and extract desired data. These spiders are not only capable of replicating our previous work but also surpassing the achievements of the previous year’s project by a considerable margin.\n\nGeneral Spider Procedure\n\nSome examples of what the spider would be able to capture include various elements present on the web page. This encompasses text content, images, links, and more. Its vision would something similar to this:\n\n&lt;tag-name attribute-name= “attribute info”&gt; … what we want (Element contexts) … &lt;/tag-name&gt;\n\n\n\nOur Spider Procedure\n\n\n\n\n\nWe needed to make multiple spiders to handle there own specific websites because not all websites are the same. Which is why our procedure is slightly more complex in comparison.\n\n\nWhat the spiders were able to achieve\n\n\n\n\n\n\nWe were able to scrape all these stores\n\n\n\n\n\n\n\n\nGateway Store\n\n\n\n\n\n\n\n\nThe output of the spiders\n\n\n\n\n\nThese Spiders are more than capable to expand to other products!"
  },
  {
    "objectID": "posts/Aaron_C_FinalWeek/Presentation.html#data-analysis",
    "href": "posts/Aaron_C_FinalWeek/Presentation.html#data-analysis",
    "title": "Local Food Project",
    "section": "Data Analysis",
    "text": "Data Analysis\nData analysis is an important tool for businesses and organizations to make better decisions. Through this analysis, we aim to uncover valuable insights, identify trends, patterns, and customer preferences, and make informed decisions to enhance profitability in the local food market. Let’s explore our data analysis approach and the exciting outcomes it will provide.\nFirst let’s understand how data analysis works:\n\n\n\n\n\n\nOur Data Analysis approach:\nWe conducted data collection on three products, Bacon, eggs and tomatoes, creating three new files for each. We then individually analyzed these files to extract useful insights. let’s see what useful information we could get from those data.\n\nProduct: Bacon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct: Eggs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nProduct: Tomatoes"
  },
  {
    "objectID": "posts/Aaron_C_FinalWeek/Presentation.html#optimization-of-the-crop-flow",
    "href": "posts/Aaron_C_FinalWeek/Presentation.html#optimization-of-the-crop-flow",
    "title": "Local Food Project",
    "section": "Optimization of the crop flow",
    "text": "Optimization of the crop flow\nIn the previous section,\n\nThe price of the product varies across different stores and cities\nLocal food farms have opportunities to sell their product in high-priced areas\n\nIn this section,\n\nAnalyze crop distribution strategies for local food farms to maximize profit\nObjective: Maximizing revenue through crop sales while considering transportation costs\nIgnored production costs as they have no influence on crop distribution\n\nWe have chosen tomatoes as an example to illustrate our approach. Our analysis begins with estimating the demand and supply of tomatoes in various counties across Iowa. We will then focus on optimizing the distribution of tomatoes over an 8-week timeline, specifically from late July to mid-September.\n\n\n\n\n\n\nDemand and supply estimation\n\n\n\n\n\nOur estimation of the supply-demand ratio for tomatoes in Iowa is 0.06658, which closely aligns with the supply-demand ratio of 0.0380 mentioned in the IMPLAN model for Iowa vegetables and melons (not specifically tomatoes). This indicates that our estimate is a reasonable approximation.\nFor a detailed explanation of how we estimated the demand and supply, please refer to HERE.\n\n\nMap visualization of demand and supply\n\n\n\nThe supply tab of the visualization indicates the level of demand for each county, with darker shades of blue representing higher demand. Conversely, the demand tab shows the required supply units for each county, with darker shades of golden to red indicating a greater need for supply. Finally, the demand-supply difference tab allows us to observe the disparity between demand and supply. In this tab, darker shades of red indicate a significant deficiency in supply, while darker shades of blue represent a surplus of supply.\n\n\nWhy a combined strategy is necessary?\nLet’s imagine a county acting alone. In order to maximize its profit, the county will aim to sell its product to the counties that offers the highest price, while also taking transportation costs into account. In this scenario, every county will try to follow the same strategy.\nAs a result, there would be an influx of products, potentially leading to an oversupply where the demand may not match. Consequently, the price could decrease, while certain counties might face a shortage of tomatoes.\nTherefore, it becomes crucial for the farmers (counties) to adopt a collaborative strategy.\n\n\nDemonstration of the optimization model\nTo provide a demonstration, we select a small subset of counties (24 out of a total of 99 counties) where the gap between supply and demand is relatively modest.\nIn these selected counties, we simulated the prices of tomatoes using a normal distribution with a mean of 2.25 and a standard deviation of 0.25.\nFor our analysis, we took into account the Ford F-350 truck with a carrying capacity of 7640 lb and a fuel efficiency of 15 mpg. We assumed that the truck would travel in a straight line (Euclidean distance) from the origin to the destination. Additionally, we considered a gas price of $3.5/gallon.\nWe used Gurobi to solve the optimization problem.\n\n\n\n\n\n\n\nSolution of the distribution problem\n\n\nIn the first image, the origin is represented by a bubble. When we hover over these bubbles, we can see the counties and the amounts that need to be transported from that specific county. In the next image, the bubble represents the destination. Hovering over the bubble reveals the quantities that should be received from different counties.\n\n\nProfit retrained in collaborative strategy compared to operating individually\n\n\nThe chart displays the ratio between the highest possible profit that could be achieved if there was only one supplier and all the receivers, and the actual profit being generated with the current strategy. This ratio provides insights into the efficiency and effectiveness of the current approach.\n\n\nLimitations\n\nConsideration of Euclidean distance instead of real-world road distance\nConsideration of $/Metric Ton/Mile instead of $/Truck-load/Mile\n\n\n\n\nFuture Work for the project\n\nGeneral optimization and improvements to the spiders.\nImplementing parallel processing: Splits execution of code and running them simultaneously\nMore spiders for additional websites\nExploration of different web scraping methods\nAfter the data scrap, further insights can be generated by analyzing more data.\nFocus on the profit retained section during the crop flow optimization.\nIntegration of where to plant the crops to the crop flow optimization model."
  },
  {
    "objectID": "posts/Aaron_C_FinalWeek/Presentation.html#conclusion",
    "href": "posts/Aaron_C_FinalWeek/Presentation.html#conclusion",
    "title": "Local Food Project",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, our project aimed to enhance local food markets by providing valuable insights and tools.\nOur web-scraping spiders automate the process of data collection, which not only saves time but also improves accuracy. These spiders are designed to extract relevant data, thereby supporting data-driven decision-making. This tool serves a critical role in constructing a comprehensive dataset that is essential for AI applications, particularly in demand forecasting and crop management.\nThe insights and comprehensive map we have presented regarding the prices of eggs and bacon across various counties provide valuable information on trends and patterns. This empowers local food farms to make informed decisions and adjust their strategies accordingly, ensuring they stay competitive in the market.\nWe demonstrated an optimization model of crop flow aimed at maximizing profit. While we utilized estimated and mock data for the demonstration, having access to the original dataset in the future would enable accurate logistics and further enhance the effectiveness of the model.\nIn summary, this project holds significance as it provides valuable insights and tools that greatly enhance decision-making within the local food market. It empowers businesses to stay informed about market trends and optimize the allocation of resources, thereby contributing to the growth and long-term sustainability of the local food farm sector."
  },
  {
    "objectID": "posts/Aaron_C_FinalWeek/Presentation.html#data-sources",
    "href": "posts/Aaron_C_FinalWeek/Presentation.html#data-sources",
    "title": "Local Food Project",
    "section": "Data Sources",
    "text": "Data Sources\n\nStore Websites Scraped\n\nFresh Thyme\nHy-Vee\nGateway Market\nNew Pioneer Co-op\nRuss’s Market\nIowa Food Hub\nJoia Food Farm\nagmrc.org\nquickstats.nass.usda.gov\n\n\n\nSoftware and tools:\n\nPython and packages (pandas, selenium, scrapy, gurobipy)\nTableau\nMicrosoft excel \nR - Quarto Blog\nPower BI\nGoogle geocoder API\nGitHub for Documentation\nGurobi Optimizer"
  },
  {
    "objectID": "posts/Aaron_C_MultiWeek/Aaron_C_Spiders.html",
    "href": "posts/Aaron_C_MultiWeek/Aaron_C_Spiders.html",
    "title": "Spiders",
    "section": "",
    "text": "Web scraping in essence is the process of extracting data from websites using automated software. As for the process, the web-scraping pipeline goes as follows.\n\nSetup - Understand what we want to do and find sources to help us do it.\nAcquisition\n\nRead in the raw data from online.\nFormat these data to be usable.\nAccessing the data.\nParsing this information.\nExtracting these data into meaningful and useful data structures.\n\nProcessing - There are many options to this but the main goal is to run the downloaded data through whatever analyses or processes needed to achieve the desired goal.\n\nThere are many web scraping tools available, but two of the one’s that I’ve the most success in are Scrapy and Selenium.\nScrapy is a Python package that is primarily used for web scraping. Scrapy provides a framework that offers a simple web crawling tool to extract data from websites. These tools are commonly known as spiders and allow for easy extraction of desired information. A spider program can crawl through websites and extracts data from them (Just like real spiders crawl through there webs). Scrapy works by sending HTTP requests to websites and receiving HTML responses, then the spider parses the HTML to extract the data it needs. These are some of benefit of using Scrapy that I’ve found:\n\nScrapy is designed to be fast and can handle large volumes of data. It’s built on top of Twisted, an asynchronous networking framework, which in simple terms makes it very fast. It can handle thousands of requests per second without slowing down. This makes it really handy for the scope of this project.\nScrapy is highly customizable and can be configured to work with many different websites and data formats. Allowing us to specify how the spider should act when navigating through websites and extract data that we told it to collect. Scrapy also provides features for handling cookies, redirects, and other HTTP features.\nScrapy comes with built-in support for XPath and CSS selectors. With the right skill set these tools provide a powerful and easy way to extract data from HTML and XML documents.\nScrapy automatically comes with built-in support for data cleaning, Making the the cleaning and normalize scraped data much easier.\n\nAs for the cons of Scrapy:\n\nScrapy does have a relatively steep learning curve to it. Especially if you are new to web scraping, XPath/CSS selectors, or asynchronous programming. Unless you have more than a week to spend learning and doing web scrapping it is best to spend the time doing another method. Not to mention the fact that you need to have some knowledge of Python to use it effectively\nScrapy does not have built-in support for JavaScript, which means that it cannot scrape websites and other dynamic web content that rely heavily on JavaScript for rendering content. Which a vast amount of websites rely on to function.\nThis tool is not perfect nor undetectable. Some websites may block Scrapy from accessing their content, especially if they detect that the traffic is coming from a bot.\nScrapy does help you in cleaning data however it is not perfect. You still need to spend some time cleaning the data that was collected and may require additional libraries, such as Pandas or NumPy, to process and analyze scraped data.\n\nSelenium is another Python package which is used for automating web browsers. It allows you to control a browser programatically, which mimics the actions of a what a user will do. Selenium uses a web driver to control your web browser and can interact with several different browsers (such as Chrome, Firefox, and Safari). Selenium works by opening up a web browser and navigates to the target website. The user can then interact with the website using Selenium commands. Just like Scrapy, Selenium can also extract data from the website using XPath and CSS selectors. As for some of benefit of using Selenium:\n\nSelenium has the ability to handle JavaScript and other dynamic web content. Allowing it to handle websites that Scrapy is unable too.\nSelenium can handle more complex interactions with websites, such as clicking buttons, filling out forms, scrolling, etc. which can simulate user behavior. Making Selenium more diverse and a power tool to use when doing specific tasks that require a more human like approach.\n\nFor the Cons of Selenium:\n\nSelenium can be really slow and very resource-intensive. Especially when automating complex tasks or interacting with multiple pages. It requires your web browser to be opened to run meaning If your internet or computer is slow it has a really hard time doing anything you want it. This time that you have to wait for the program to preform any simple task is not ideal when working through large data sets.\nIt is prone to errors and requires extensive debugging and testing. For example, if say that you close your browser or if it freezes (for long periods of time) the program will not function correctly and throw an error.\nIt can be very challenging to set up and configure. Not to mention that It can be difficult to maintain test scripts when the web application is updated or changed. Meaning if you are not dedicated in learning or maintaining your code it will come with a bunch of headaches down the road.\n\nIdeally for this project I strongly recommend using Scrapy spiders were ever possible. Scrapy’s speed and efficiency and overall better performance makes this part of the process much more tolerable in building spiders. However, when it comes to more complex tasks and websites Selenium is the way to go. I would also like to acknowledge that there are other tools and resources that could have been explored and used which may be better or more efficient for the scope of the project. These two tools are just some that I found and was able to implement in the time frame I was given. For example, there is a python package called Scrapy Splash. This works as an extension for Scrapy which provides a JavaScript rendering service. Allowing Scrapy to handle JavaScript and other dynamic web content, similar to Selenium. I was not able to master this tool, nor could I successfully get it this tool to work. If I had more time to develop a clear comprehension of this utility and fully understood how to get it to work. It would have been the tool of choice to handle such websites. I used Selenium since I had more success in getting it to work in comparison."
  },
  {
    "objectID": "posts/Aaron_C_MultiWeek/Aaron_C_WebScraping.html",
    "href": "posts/Aaron_C_MultiWeek/Aaron_C_WebScraping.html",
    "title": "Web Scraping",
    "section": "",
    "text": "Web scraping is the process of extracting data from websites using automated software. In the context of web scraping, a Spider (also known as a web crawler) is a program or script designed to navigate through one or multiple websites and automatically extract data from multiple pages. The Spider “crawls” along the web, hence its name.\nWeb scraping in essence is the process of extracting data from websites using automated software. As for the process, the web-scraping pipeline goes as follows.\n\nSetup (Objective) - Firstly, we need to clarify our goal and identify relevant sources to assist us in achieving it.\nAcquisition\n\nRead in the raw data from online.\nFormat these data to be usable.\nAccessing the data.\nParsing this information.\nExtracting these data into meaningful and useful data structures.\n\nAcquisition - This is where the spiders come into play. They are responsible for:\n\n\nReading the raw data from online sources.\nFormatting the data to be usable.\nAccessing the data.\nParsing the information.\nExtracting the data into meaningful and useful data structures.\n\n\nProcessing - There are various options available for this step, but the primary objective is to run the downloaded data through the necessary analyses or processes, including data analysis, to accomplish the desired goal.\n\nBefore going any further I highly recommend learning or brushing up on a few concepts/topics.\n\nThe basics of HTML\nBasic programming experience\nPathing and Selectors\n\nThe basics of HTML and programming can be easily found online and my best advice is to do some small projects to start and grow from there. The language I used for this project was python but you aren’t limited to staying with it since other languages have web scraping tools and resources at your disposal.\nOnce you’ve been aquanted with HTML. When we are scraping data from a website, we need a simple way to identify the specific elements on the page. Information within HTML tags are increadably valuable since the information we need to access is held within the HTML tags themselves.\n\n&lt;tag-name attribute-name= \"attribute info\"&gt; ... what we want (Element contexts) ... &lt;/tag-name&gt;\n\nThere are two common ways of doing this.\n\n\nXPaths:\n\nUseful for web scraping tasks that involve complex page structures or nested elements\nCan be more difficult to write than CSS selectors, but they offer more flexibility in selecting elements on the page\n\n\nxpath = ‘//*[@class=“class1”]’\n&lt;p class=“class-1”&gt; … &lt;/p&gt; Can see this\n&lt;div class=“class-1 class-2”&gt; … &lt;/div&gt; Can see not this\n&lt;p class=“class-12”&gt; … &lt;/p&gt; Can not see this\n\n\nCSS selectors:\n\nAre often simpler and easier to write than XPath expressions, especially for basic web scraping tasks.\nMay not be as flexible as XPath expressions in selecting elements on the page, especially for complex page structures or nested elements.\n\n\ncss = ‘.class1’\n&lt;p class=“class-1”&gt; … &lt;/p&gt; Can see this\n&lt;div class=“class-1 class-2”&gt; … &lt;/div&gt; Can see this\n&lt;p class=“class-12”&gt; … &lt;/p&gt; Can not see this\n\n\n\nBoth methods have their benefits, so the choice between them depends entirely on your specific needs. It is important to note that each website is structured differently, meaning that the pathing or scraping approach used for one site may not necessarily work for another. Additionally, not only does the structure of websites vary, but also the type of website itself.\n\n\n\nStatic websites\n\nMade up of fixed web pages that are delivered your web browser exactly as they were created. Meaning there’s no magic that happens upon creation.\nTypically they coded in HTML and CSS and may include some basic JavaScript for interactive elements such as dropdown menus or image sliders. (Normally this does not affect us when Scraping)\nExamples: company brochures, personal blogs, and simple online portfolios, etc.\nThese are easier to create and maintain, but they are less flexible and cannot handle large amounts of data or user interactions.\nThese are much faster to scrape. Everything is there and we don’t have to wait for the page to load!\n\n\n\n\nDynamic websites\n\nDynamic websites are generated on the fly by a web application or content management system (CMS). Meaning a lot of magic happens upon loading a page.\nThe content is not fixed, and it can change based on user interactions, database queries, or other dynamic factors.\nExamples: Social media platforms, E-commerce sites, and most online marketplaces.\nThese are more complex to create and maintain, but they are more flexible and can handle large amounts of data or user interactions\nThese are much slower to scrape. We need to wait for the page to load before we can start scraping the page!\n\n\nThere are many web scraping tools available, but two of the one’s that I’ve the most success in are Scrapy and Selenium.\nScrapy is a Python package that is primarily used for web scraping. Scrapy provides a framework that offers a simple web crawling tool to extract data from websites. These tools are commonly known as spiders and allow for easy extraction of desired information. A spider program can crawl through websites and extracts data from them (Just like real spiders crawl through there webs). Scrapy works by sending HTTP requests to websites and receiving HTML responses, then the spider parses the HTML to extract the data it needs. There is a Data camp course that is currently available to use and is highly recommended watching.\nPros of Scrapy:\n\nSpeed: Scrapy is designed to be fast and efficient, thanks to its integration with the asynchronous networking framework, Twisted. It can handle large volumes of data and process thousands of requests per second without slowing down.\nScalability: Scrapy is capable of handling large-scale projects, making it ideal for projects that require scalability. It can efficiently handle a high volume of data and requests, ensuring smooth and uninterrupted scraping operations.\nCustomizability: Scrapy is highly customizable and can be easily configured to work with various websites and data formats. It allows us to specify how the spider should navigate through websites and extract the desired data, providing flexibility and control.\nRobustness: Scrapy is a reliable web scraping framework that can crawl websites, automatically discover URLs, extract data, and process it through item pipelines. It offers robust functionality for automating the scraping process, ensuring efficient and accurate data extraction. 5. Data Cleaning Support: Scrapy comes with built-in support for data cleaning, making it easier to clean and normalize scraped data. This feature simplifies the process of ensuring the quality and consistency of the extracted data.\n\nCons of Scrapy:\n\nLearning Curve: Scrapy can have a steep learning curve, especially for those new to web scraping, XPath/CSS selectors, or asynchronous programming. It may require a significant time investment to learn and effectively use Scrapy, making it less suitable if you have limited time available. Additionally, some knowledge of Python is necessary to utilize Scrapy effectively.\nJavaScript Limitation: Scrapy does not have built-in support for JavaScript, which means it cannot scrape websites and other dynamic web content heavily reliant on JavaScript for rendering. Since many websites depend on JavaScript, Scrapy may not be suitable for scraping such content.\nData Cleaning Limitations: While Scrapy provides data cleaning assistance, additional time may still be required to manually clean and process the data that was collected.\n\nIn summary, Scrapy offers speed, scalability, customizability, robustness, and data cleaning support. However, it has a steep learning curve, and lacks JavaScript support.\nSelenium is another Python package which is used for automating web browsers. It allows you to control a browser programatically, which mimics the actions of a what a user will do. Selenium uses a web driver to control your web browser and can interact with several different browsers (such as Chrome, Firefox, and Safari). Selenium works by opening up a web browser and navigates to the target website. The user can then interact with the website using Selenium commands. Just like Scrapy, Selenium can also extract data from the website using XPath and CSS selectors.\nPros of Selenium:\n\nCross-Browser Compatibility: Selenium supports a wide range of web browsers, including Chrome, Firefox, Safari, and others. This cross-browser compatibility ensures that Selenium can be used effectively across different browser environments.\nPowerful for Web Scraping: Selenium enables automation of a web browser, making it a powerful tool for web scraping. It can navigate through web pages, interact with elements, and extract desired data, providing extensive capabilities for scraping purposes.\nDynamic Web Handling: Selenium is capable of handling JavaScript, AJAX, and cookies. This allows it to effectively interact with dynamic web content and perform actions that rely on these technologies, ensuring comprehensive web scraping capabilities.\nComplex Web Interactions: Selenium excels in handling more complex interactions with websites, such as clicking buttons, filling out forms, scrolling, and more. This capability allows Selenium to simulate user behavior and perform tasks that require a more human-like approach, making it a versatile and powerful tool for specific tasks.\nRelatively Easy to Use: Selenium is known for its user-friendly nature, making it relatively easy to use, especially for those familiar with web development and programming concepts. Its straightforward API and documentation contribute to its ease of use.\n\nCons of Selenium:\n\nPerformance and Resource Intensive: Selenium can be slow and resource-intensive, especially when automating complex tasks or interacting with multiple pages. It relies on having a web browser open to run, which can be problematic if your internet or computer is slow. This can result in significant delays when working with large datasets, making it less ideal for such scenarios.\nProne to Errors: Selenium is prone to errors and requires extensive debugging and testing. This necessitates thorough testing and troubleshooting to ensure the reliability of the automation process.\nSetup and Configuration Challenges: Setting up and configuring Selenium can be challenging, particularly when dealing with complex web pages or unusual website configurations. Additionally, maintaining test scripts can be difficult as web applications are updated or changed.\n\nIn summary, Selenium offers cross-browser compatibility, powerful web scraping capabilities, the ability to handle dynamic web content, and support for complex web interactions. However, it can be slow and resource-intensive, prone to errors, and challenging to set up and configure at the start.\nComparing the two:\n\nBuilt-in Support for XPath and CSS Selectors: Both Scrapy and Selenium offer built-in support for XPath and CSS selectors, making it easier to locate and extract specific elements from web pages.\nRequires Technical Knowledge: Both Scrapy and Selenium require technical knowledge, particularly in Python and web development concepts such as HTML, JavaScript, and selectors. This knowledge is necessary to effectively utilize the features and capabilities of these tools.\nFlexibility and Customizability: Both Scrapy and Selenium are highly flexible and customizable to suit specific tasks. Scrapy allows for the customization of spider behavior, data extraction, and item processing pipelines. Selenium, on the other hand, enables the customization of interactions with web elements and the browser.\nNot Suitable for All Websites: Both Scrapy and Selenium can face challenges when dealing with certain websites. They can be stopped by CAPTCHAs or face IP blocking, which can restrict their access to websites that have implemented such measures.\nResource-Intensive: Both Scrapy and Selenium can be resource-intensive, but Selenium tends to be more resource-intensive due to its reliance on having a web browser open. This can consume significant system resources, especially when automating complex tasks or interacting with multiple web pages.\nIntegration with Other Python Tools: Both Scrapy and Selenium can be used in conjunction with other Python tools. For example, Scrapy can be combined with libraries like Beautiful Soup and Requests to enhance data extraction and processing capabilities.\nOpen Source: Both Scrapy and Selenium are open-source projects, meaning they are freely available and can be modified and customized as per the user’s requirements.\nLarge and Active Community: Both Scrapy and Selenium have large and active communities of developers. This means that there is ample documentation, tutorials, and support available, making it easier to get help and contribute to the development of these tools.\n\nIn summary, Scrapy and Selenium share similarities such as built-in support for XPath and CSS selectors, the requirement of technical knowledge, flexibility, and customization. However, they also have differences in terms of suitability for different websites, resource-intensiveness (with Selenium being more resource-intensive), integration with other Python tools, and the size and activity of their respective communities.\nFor this and similar projects, I strongly recommend using Scrapy spiders whenever possible. Scrapy’s speed, efficiency, and overall better performance make the process of building spiders much more tolerable. However, when it comes to more complex tasks and websites, Selenium is the way to go. It’s important to acknowledge that there are other tools and resources available that may be better or more efficient for the project scope. One example is Scrapy Splash, a Python package that works as an extension for Scrapy and provides a JavaScript rendering service. This allows Scrapy to handle JavaScript and other dynamic web content, similar to Selenium. Although I didn’t have enough time to fully explore and understand Scrapy Splash, it would have been the preferred tool for handling such websites if I had more time to master it. Instead, I used Selenium since I had more success in getting it to work.\nIt is important to note that this post is a starting point and is intended for educational purposes only. There are both positive and negative aspects associated with web scraping, so it is crucial to approach it with a strong ethical mindset. The web scrapers I have developed for this project adhere to specific rules.\nWhen scraping websites, I actively monitor the process. If any issues arise, I promptly halt the code and wait for 20 to 30 minutes before retrying. Additionally, when making multiple requests, I ensure to space them out and introduce a few seconds of delay. This approach helps prevent overloading the website’s server and negatively impacting its performance. Furthermore, I make a conscious effort to refrain from interfering with the website’s functionality, only interacting with buttons or elements when necessary. Unless explicitly required for the scraping task, I avoid downloading any unnecessary content. In the case of images, I follow and save the image URL rather than downloading the image immediately, ensuring that I only retrieve the specific images needed at any given point.\nBy following these practices, I aim to maintain a responsible and respectful approach to web scraping, minimizing any potential negative effects on the websites being scraped."
  },
  {
    "objectID": "posts/Aaron_C_Week1/Aaron_C_Week1.html",
    "href": "posts/Aaron_C_Week1/Aaron_C_Week1.html",
    "title": "Week One",
    "section": "",
    "text": "During the first week, I had the chance to provide a brief introduction of myself to the client. This involved taking the necessary time to clearly communicate my role in the project, thereby establishing a professional connection with the Employer.\nAfter concluding the introduction, we proceeded with the debriefing process of the project with the Employer. We discussed the project’s background, with a specific focus on the progress made during the first year. Additionally, we received a comprehensive overview of the previous work that had been completed. Once we were caught up with this overview, we dived into the project’s scope, addressing the specific tasks and goals that needed to be accomplished. This in-depth discussion provided us with a better understanding of the project and clarified what needed to be done.\nRegarding the goals for week one, my primary objective was to acquire as much knowledge and experience about Python and R in Datacamp.\nLessons completed:\n\nIntroduction to R\nIntermediate R\nCleaning Data in R\nIntroduction to Python\nIntermediate Python\nData Manipulation with pandas\nAI Fundamentals\nGitHub Concepts"
  },
  {
    "objectID": "posts/Aaron_C_Week2/Aaron_C_Week2.html",
    "href": "posts/Aaron_C_Week2/Aaron_C_Week2.html",
    "title": "Week Two",
    "section": "",
    "text": "During week two, the plan in place was to continue learning using Datacamp. I successfully completed two lectures, Writing Efficient Python Code and Web Scraping in Python.\nAlso during week two I was tasked to learn Tidycensus in R for data visualization. In Census Visual provides a demonstration of the code I have written and showcases the knowledge and skills I have acquired using Tidycensus with the American Community Survey Data.\nDuring the week DSPG needed to extract a large amount of data from the American Community Survey. Since this process was lengthy, it became one of the main problems that the DSPG encountered during the week. However, I was able to solve it using the R code provided in Importing Census Data."
  },
  {
    "objectID": "posts/Aaron_C_Week2/Importing Census Data.html",
    "href": "posts/Aaron_C_Week2/Importing Census Data.html",
    "title": "Importing Census Data",
    "section": "",
    "text": "Since I needed to extract a large amount of data from the American Community Survey (ASC) and convert it into a CSV file. In light of this I wrote this code to extract ASC data from the table codes. The Code takes in a list of names that you want the file name to be. Along with the corresponding table code. It also takes in a folder name. It then makes (if needed) and adds the CVS files to the specified folder (For a clean directory). Its important to note that where you run the R file is where a folder is made. I made this with the intentions of it being editable (and hopefully user friendly).\n\n####################\n#  Inserting Data  #\n####################\n\n\nfolder &lt;- \"Folder_Name_Here\" # &lt;---------- Change this value FIRST!\n\nACSList &lt;- c(\n    # \"Data Name\",\"DataCode\", \n    # ...\n  ) \nACSListToCSV(ACSList,folder)\n\n\n##########################################\n#  Global Variables That can be Changed  #\n##########################################\n\n#To change the get_acs() geography variable\ngeographyType &lt;- \"county\"\n\n#To change the get_acs() servay variable\nservayType &lt;- \"ACS5\"\n\n#Change to NULL if no state\nstateType &lt;- \"IA\"\n\n#This will make a geometry file as well if TRUE\nwithGeometry &lt;- FALSE\n\n#checking the year\nyear = NULL\n\n###################################\n#  Functions that make life easy  #\n###################################\n\n#Imports \nlibrary(tidycensus) #For ACS extractions\nlibrary(stringi) #For folderNameFixer()\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\n\n\n#File name changers. This will set the name of the file. Feel free to edit this\nrenameFile &lt;- function(tableTitle, tableCode, isGeometric){\n  fileName &lt;- paste(tableTitle, \" (\", tableCode, sep='')\n  if(isGeometric){\n    fileName = paste(fileName, \", \",capFirst(\"tract\"), sep='')\n  } else {\n    fileName = paste(fileName, \", \",capFirst(geographyType), sep='')\n  }\n  if(is.null(stateType) == FALSE){\n    fileName = paste(fileName, \", \", stateType, sep='')\n  }\n  if(is.null(year) == FALSE){\n    fileName = paste(fileName, \", \", year, sep='')\n  }\n  fileName = paste(fileName, \", \", servayType, sep='')\n  if(isGeometric){\n    fileName = paste(fileName, \", Geometry).csv\", sep='')\n  } else {\n    fileName = paste(fileName, \").csv\", sep='')\n  }\n  return(fileName)\n}\n\n#ACS extractions.\n#Feel free to add to this list.\ngeoACSDataFrame = function(tableCode){\n  get_acs(\n    #Add Changes here\n    geography = \"tract\",\n    table = tableCode,\n    servay = servayType,\n    state = stateType,\n    geometry = TRUE\n  )\n}\n\ndefaultACSDataFrame = function(tableCode){\n  get_acs(\n    #Add Changes here\n    geography = geographyType,\n    table = tableCode,\n    servay = servayType,\n    state = stateType\n  )\n}\n\n#Conditions for files fell free to edit this\nfileImplications &lt;- function(tableTitle, tableCode){\n  #Add a condition and apply both the ACS extraction and the File name changer\n  #Example\n  if(withGeometry){\n    fileName &lt;- renameFile(tableTitle, tableCode, TRUE)\n    #output with Geometry\n    dataToCSV(geoACSDataFrame(tableCode), fileName)\n  }\n  \n  fileName &lt;- renameFile(tableTitle, tableCode, FALSE)\n  #Default\n  #This Makes the CSV File\n  #format dataToCSV(your ACS DataFrame, File name changer() )\n  dataToCSV(defaultACSDataFrame(tableCode), fileName)\n}\n\n\n#For clarity capitalizes the first letter in a string and lowercases the rest (Feel free to Use)\ncapFirst = function(xStr){\n   paste(toupper(substring(xStr, 1, 1)), tolower(substring(xStr, 2, nchar(xStr))), sep = \"\")\n}\n#Validates and fixes folder name string (Feel free to edit)\nnameFixer &lt;- function(xStr, fixType){\n  #Replaces bad characters with ''\n  xStr &lt;- stri_replace_all_regex(xStr, \n                         pattern=c('/', ':', '\\\\*', '\"', '&lt;', '&gt;', '\\\\|'),\n                         replacement=c('-', '', '', '', '', '', ''),\n                         vectorize=FALSE)\n  if(xStr == \"\"){\n    if(capFirst(fixType) == \"Folder\"){\n      print(\"Ops the folder name has all bad characters lets fix that\")\n      xStr &lt;- \"New_Data_Folder\"\n    }\n    else{\n      print(\"Ops the file name has all bad characters lets fix that\")\n      xStr &lt;- \"New_Data_file\"\n    }\n  }\n  return(xStr)\n}\n#For bulk downloading tables\nACSListToCSV &lt;- function(bulkArray, folder){\n  if(length(bulkArray) %% 2){\n    #Scream if array is odd length. We don't need any mistakes!\n    print(\"Somethings Missing. List format should be like c( Data Name, DataCode, ... )\")\n    return(\"ERROR\")\n  }\n  #Validates folder name\n  folder &lt;- nameFixer(folder, \"Folder\")\n  #Makes a folder if needed\n  if(!file.exists(folder)){\n    dir.create(folder)\n    print(\"New Folder Made Adding Files\")\n  }\n  index &lt;- 1\n  #Adds files\n  while(index &lt; length(bulkArray)){\n    fileImplications(nameFixer(bulkArray[index], \"File\"), bulkArray[index+1]) # (title , code)\n    index &lt;- index + 2\n  }\n  print(\"All Files have been downloaded\")\n}\n#ACS Data frame to CSV file\ndataToCSV &lt;- function(data, fileName){\n  #This adds the file to the folder\n  path &lt;- paste(\".\\\\\",folder,\"\\\\\", fileName, sep='')\n  #Makes the CSV\n  write.csv(data, path)\n  print(paste(\"Added File:\",fileName))\n}"
  },
  {
    "objectID": "posts/Aaron_C_Week2/TidyCensusOverview.html",
    "href": "posts/Aaron_C_Week2/TidyCensusOverview.html",
    "title": "Census Visual",
    "section": "",
    "text": "Both a bar plot, and chorpleth map were made from the American Community Survey (ASC) data using Tidycensus library in R.\n\n#Imports for both Graph and Map\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(plotly)\nlibrary(ggiraph) \n\n#Grabing median_income for bar blot\nmedian_income &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  state = \"IA\", \n  year = 2021\n)\n\n#View the mid range of countys mean income\nmedian_income_data &lt;- median_income %&gt;%\n  #reducing the number of countys the graph can display\n  slice(floor(99 * 0.25):floor(99 * 0.75)+1) %&gt;%\n  #ordering from estimate highest to estimate lowest\n  arrange(desc(estimate))\n\n#The Bar plot\nmd_bar_plot &lt;- ggplot(median_income_data, aes(x = estimate, \n                                    y = reorder(NAME, estimate),\n                                    tooltip = estimate,\n                                    data_id = GEOID)) +\n  #Generating the error bars\n  geom_errorbar(aes(xmin = estimate - moe, \n                    xmax = estimate + moe),\n                    width = 0.5, \n                    size = 1) + \n  #Coloring the estimate dot \n  geom_point_interactive(color = \"darkblue\", size = 1.5) +\n  #Bottom Label range\n  scale_x_continuous(labels = label_dollar()) + \n  #County names and removing the Unnecessary words\n  scale_y_discrete(labels = function(x) str_remove(x, \" County, Iowa|, Iowa\")) +\n  #Graph labeling for views convince \n  labs(title = \"Median Income 2021 ACS\",\n       #subtitle = \"Counties in Iowa\",\n       caption = \"Data acquired with R and tidycensus. \\nError bars represent margin of error around estimates of Median income.\",\n       x = \"ACS Estimate Mean Income\",\n       y = \"Counties in Iowa\") + \n  #Text Sizing\n  theme_minimal(base_size = 8)\n#Making the graph interactive\nmd_bar_plot_interactive &lt;- girafe(ggobj = md_bar_plot) %&gt;% girafe_options(opts_hover(css = \"fill:purple;\"))\n\n#The Map\nmedian_income_map &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  state = \"IA\",\n  year = 2021,\n  geometry = TRUE\n)\n# The Map\nmd_chorpleth_map &lt;- ggplot(median_income_map, aes(fill = estimate)) + \n  #The map display\n  geom_sf() + \n  #Empty theme of the map\n  theme_void() + \n  #Colors the map \n  scale_fill_viridis_c(option = \"G\", n.breaks = 10) + \n  #Information\n  labs(title = \"Median Income by Census track\",\n       subtitle = \"\",\n       fill = \"ACS estimates\",\n       caption = \"Median Income by ACS tidycensus R package in 2021\")\n\n\n#This renders the bar plot \nmd_bar_plot_interactive\n\n#This renders the chorpleth map\nmd_chorpleth_map\n\n\n\n\nBar Plot\n\n\n\n\n\nChorpleth Map"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Aaron_C_Week3.html",
    "href": "posts/Aaron_C_Week3/Aaron_C_Week3.html",
    "title": "Week Three",
    "section": "",
    "text": "During week three, our focus was primarily on research and data collection for our project. This week was dedicated to gathering relevant information and conducting thorough research to support our project goals. In Week Three Research, I have compiled and summarized the information I gathered while researching bacon, eggs, and heirloom tomatoes. This resource outlines and highlights the key findings that I was able to uncover during my investigation.\nIn addition to our focus on research and data collection, I also prioritized learning during week three. Using Data camps and successfully completed Writing Efficient R Code lecture. Recognizing the crucial role of data collection in our project, I took the initiative in learning how to create web scrapers, also known as spiders. This involved dedicating a large portion of the week as well as most of my free time to acquire the necessary skills and knowledge in this area. My efforts in this yielded some positive results. I was able to successfully created a spider that efficiently extracts data from the provided weblink and organizes it into a pandas dataframe. The reason we store the data in this dataframe is because it makes it much easier to work with, especially when it comes to cleaning and exporting it into a comma-separated values (CSV) file for future use. It is important to note that while this image showcases an output of the extraction process, it is crucial to understand that it is far from being considered a finished product.\n\n\n\nOutput\n\n\nFinally at the end of the week I was tasked to present our weekly wrap up for the Team. This covers what the AI Local food team was able to accomplished for the week and can be view in Week Three Wrap Up."
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html",
    "href": "posts/Aaron_C_Week3/Week3_Research.html",
    "title": "Week Three Research",
    "section": "",
    "text": "Farmers Market Prices:\nAt the 2015 PFI Annual Conference, Kay Jensen collected farmers’ vegetable prices. Recent data shows price ranges for tomatoes, ranging from $2.50 to $4 per pound.\nExpatistan:\nTells the price of 1 kg (2 lb.) of tomatoes in Des Moines IA can be used to find other prices located in IA.\nAgMRC:\nProvides a some detailed and helpful references for tomatoes.\nMarket Maker:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nLocal Harvest:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nPractical farmers:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nInstacart:\nA cite that could be of use to find Items.\nNFMD:\nMarket on Central - Farmers Market in Historic Downtown Fort Dodge\nNational Retail Report - Specialty Crops ( USDA Fruits & Vegetables Market Report ):\nAdvertised Prices for Specialty Crops at Major Retail Supermarket Outlets it contains Heirloom Tomatoes but only specifies the Midwest U.S.\nFRED Economic Data:\nContains the average Price of Tomatoes( Tomatoes, Field Grown (Cost per Pound/453.6 Grams) in U.S. City Average) All fresh field grown and vine ripened round red tomatoes. Includes organic and non-organic.\nFarmers Market Nutrition Program:\nLists farmers markets in Iowa\nIowa Food Cooperative (Iowa Food Coop):\nContains some data that could be useful\nNASS Eggs:\nContains tons of data that could be useful\nList of Egg Farms in Iowa:\ncould be useful to source eggs"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html#some-helpful-data-found",
    "href": "posts/Aaron_C_Week3/Week3_Research.html#some-helpful-data-found",
    "title": "Week Three Research",
    "section": "",
    "text": "Farmers Market Prices:\nAt the 2015 PFI Annual Conference, Kay Jensen collected farmers’ vegetable prices. Recent data shows price ranges for tomatoes, ranging from $2.50 to $4 per pound.\nExpatistan:\nTells the price of 1 kg (2 lb.) of tomatoes in Des Moines IA can be used to find other prices located in IA.\nAgMRC:\nProvides a some detailed and helpful references for tomatoes.\nMarket Maker:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nLocal Harvest:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nPractical farmers:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nInstacart:\nA cite that could be of use to find Items.\nNFMD:\nMarket on Central - Farmers Market in Historic Downtown Fort Dodge\nNational Retail Report - Specialty Crops ( USDA Fruits & Vegetables Market Report ):\nAdvertised Prices for Specialty Crops at Major Retail Supermarket Outlets it contains Heirloom Tomatoes but only specifies the Midwest U.S.\nFRED Economic Data:\nContains the average Price of Tomatoes( Tomatoes, Field Grown (Cost per Pound/453.6 Grams) in U.S. City Average) All fresh field grown and vine ripened round red tomatoes. Includes organic and non-organic.\nFarmers Market Nutrition Program:\nLists farmers markets in Iowa\nIowa Food Cooperative (Iowa Food Coop):\nContains some data that could be useful\nNASS Eggs:\nContains tons of data that could be useful\nList of Egg Farms in Iowa:\ncould be useful to source eggs"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html#heirloom-tomatoes",
    "href": "posts/Aaron_C_Week3/Week3_Research.html#heirloom-tomatoes",
    "title": "Week Three Research",
    "section": "Heirloom Tomatoes",
    "text": "Heirloom Tomatoes\nPlaces that might use Heirloom Tomatoes\n\nJava House\n\nStores that sell Heirloom Tomatoes that we can collect data from\n\nGateway market\nRuss’s Market\nFresh thyme\nHy-Vee\nNew Pioneer coop\n\nInvestigated Stores\n\nDogpatch: They have seeds but no tomatoes for sale\nFairway: They don’t have any heirloom tomatoes for sale\nWalmart: They have seeds but no tomatoes for sale\nTarget: They don’t have any heirloom tomatoes for sale\nCostco: They don’t have any heirloom tomatoes for sale\nWhole Foods Market: They don’t have any heirloom tomatoes for sale in Iowa\nAldi: They don’t have any heirloom tomatoes for sale\nTrader Joe’s: They don’t have any heirloom tomatoes for sale\nCampbell’s Nutrition: They don’t have any heirloom tomatoes for sale\nRamsey’s Market: They don’t have any heirloom tomatoes for sale\nGoPuff: They don’t have any heirloom tomatoes for sale\nOver 150+ Other Websites searched"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html#egg",
    "href": "posts/Aaron_C_Week3/Week3_Research.html#egg",
    "title": "Week Three Research",
    "section": "EGG",
    "text": "EGG\nStores that sell Eggs that we can collect data from\n\nGateway market\nRuss’s Market\nFresh Thyme Market\nHv-Vee\nNew Pioneer coop\ndogpatch\nFairway\nwalmart\nTarget\nWhole Foods Market\nAldi\nTrader Joe’s\nRamsey’s Market\nGoPuff"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html#bacon",
    "href": "posts/Aaron_C_Week3/Week3_Research.html#bacon",
    "title": "Week Three Research",
    "section": "Bacon",
    "text": "Bacon\nStores that sell Bacon that we can collect data from\n\nGateway market\nRuss’s Market\nFresh Thyme Market\nHv-Vee\nNew Pioneer coop\ndogpatch\nFairway\nWalmart\nTarget\nCostco\nWhole Foods Market\nAldi\nTrader Joe’s\nRamsey’s Market\nGoPuff"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_WrapUp.html",
    "href": "posts/Aaron_C_Week3/Week3_WrapUp.html",
    "title": "AI/Local Food Week Three Wrap Up",
    "section": "",
    "text": "The currents project objectives for this week was to\n\nCatch up on any additional training.\nCollect and find data on heirloom tomatoes, eggs, and bacon.\nLearning how to do web scraping in python through Datacamp.\nBuilding programs to do web scraping"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_WrapUp.html#current-project-objectives",
    "href": "posts/Aaron_C_Week3/Week3_WrapUp.html#current-project-objectives",
    "title": "AI/Local Food Week Three Wrap Up",
    "section": "",
    "text": "The currents project objectives for this week was to\n\nCatch up on any additional training.\nCollect and find data on heirloom tomatoes, eggs, and bacon.\nLearning how to do web scraping in python through Datacamp.\nBuilding programs to do web scraping"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_WrapUp.html#works-in-progress",
    "href": "posts/Aaron_C_Week3/Week3_WrapUp.html#works-in-progress",
    "title": "AI/Local Food Week Three Wrap Up",
    "section": "Works in Progress",
    "text": "Works in Progress\nAn excel sheet that contains a list of small businesses of Iowa grocers that we had to go through and find places that had the data we wanted.\n\n\n\n\n\n\n\n\n\n\nThese are some examples of what we were looking for\n\n\n\n\n\n\n\n\n\n\nAlong with some manual data scraping. We started work on some data scraping programs (spiders). However, there are still some improvements that still need to be made. This image shows an example output of data we were able to scrape."
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_WrapUp.html#dspg-questions",
    "href": "posts/Aaron_C_Week3/Week3_WrapUp.html#dspg-questions",
    "title": "AI/Local Food Week Three Wrap Up",
    "section": "DSPG Questions",
    "text": "DSPG Questions\n\nAre there stores or market places that would be helpful for us to look into?\nIs anyone experienced in web scraping and if so there any advice that you have for us?"
  },
  {
    "objectID": "posts/Aaron_C_Week4/Aaron_C_Week4.html",
    "href": "posts/Aaron_C_Week4/Aaron_C_Week4.html",
    "title": "Week Four",
    "section": "",
    "text": "During week four, my main focus was on learning and building spiders. I dedicated a significant amount of time to this task and successfully completed the construction of three separate spiders. Nevertheless, It is important to note that while the spiders are functional, there is still room for further improvements, as the code is not yet fully refined. Additionally, During week four I participated in the collection of housing data in Slater Iowa for the WINVEST project. While this activity was aimed for preparation for whats to come in Week Six it did not detract from the primary focus on learning and building spiders during week four.\nFor a detailed explanation of my endeavors and what I have learned, you can visit this link: Spiders. This link will direct you to an overview of my progress, including updates on what I have learned during the week. Please note that I will be updating this link to include my knowledge from Week Five as well.\nFor those who are interested, I have included some of the outputs generated by the spider below. These outputs serve as examples of the spider’s functionality and demonstrate the successful results achieved during the learning and building process in week four.\nOutput of the Fresh Thyme Market Spider\nOutput of the Hy-Vee Spider \nOutput of the Gateway Market Spider"
  },
  {
    "objectID": "posts/Aaron_C_Week5/Aaron_C_Week5.html",
    "href": "posts/Aaron_C_Week5/Aaron_C_Week5.html",
    "title": "Week Five",
    "section": "",
    "text": "During week five, although it may not hold the utmost significance for the scope of the project, I wanted to mention that I had the opportunity to attend the ITAG VI conference. Throughout the conference, I had the privilege of attending various insightful lectures. While this detail may not directly impact the project, it is worth documenting as it contributes to my overall knowledge and professional growth. This conference provided valuable opportunities to learn from experts in their respective fields and gain new perspectives.\nAs for the majority of the week, my primary focus was on building and optimizing spiders. By the end of the week, I successfully completed the construction of four additional spiders, bringing the total number of spiders built for the project to seven. While progress has been made, there are still several improvements that need to be addressed before considering any of the spiders as finished.\nDue to performance issues and consistent absenteeism within our team, we had to make some changes that resulted in substantial delays to the current progression of our project. As a result, I was assigned the task of cleaning the data alongside spider development.\nIn light of these challenges, I believed it would be best to turn this hindrance into an opportunity by implementing a data cleaning program that runs concurrently within the spider! The purpose of this concurrent operation is to ensure the accuracy of the collected data. By incorporating this data cleaning program within each spider using composition, we can easily monitor the collection and cleaning processes in real time without compromising speed and efficiency.\nThis approach allows us to efficiently address data quality issues, ensuring that the collected data is reliable and ready for analysis. The main objective of this program is to guarantee that the collected data is thoroughly cleaned before integrating it into the pandas dataframe. This ensures that the data is ready to be exported in the CSV file format, providing a seamless transition for further analysis and utilization.\nNot only would this approach solve our current situation, but it would also make our programs more dynamic and easily adaptable. This means that we can easily incorporate more products to be added and cleaned for future development, ensuring that our system remains flexible and scalable!\nExpanding upon what I’ve learned in week four, I would like to direct you to the following link which provides a more in-depth summary of my knowledge of Spiders. This summary not only delves into the concepts and techniques I have learned, but also includes some example code to further illustrate them. Feel free to explore the link and delve deeper into my progress.\nAs for those who have been following our project and reading my previous posts, I would like to provide a recap of the spiders that have been developed so far:\n1. Fresh Thyme\n2. Hy-Vee\n3. Gateway Market\n4. New Pioneer Co-op\n5. Russ’s Market\n6. Iowa Food Hub\n7. Joia Food Farm"
  },
  {
    "objectID": "posts/Aaron_C_Week6/Aaron_C_Week6.html",
    "href": "posts/Aaron_C_Week6/Aaron_C_Week6.html",
    "title": "Week Six",
    "section": "",
    "text": "On Monday of week six, as briefly mentioned in Week Four, DSPG conducted assessments of houses in Grundy Center and New Hampton by taking pictures and tracking their characteristics using the Fulcrum app. We then continued our evaluations in Independence on Tuesday, which marked the conclusion of our WINVEST site assessments.\nDuring the assessments, we meticulously examined the houses’ structural features and recorded detailed notes on the condition of the driveway, foundation, gutters, paint, porch, roof, siding, walls, and window frames, as well as any other noteworthy features that were in poor condition. We also took into account the condition of the lot, noting any junk or debris present on the site. Additionally, we evaluated the sidewalk connecting to the house and made note of its condition. We paid extra close attention to the gutters, roof, siding, and landscape, assessing whether they were in good, fair, or poor condition. As we took and evaluated the pictures, we meticulously noted any obstructions in the photo(s) taken, paying close attention to whether they were caused by overgrown vegetation (such as bushes or weeds), trees, electrical posts, cars, or any other obstructions.\nAlong with evaluation of the housing, we also took note of our general impressions of the block. We evaluated the neighborhood sidewalks, taking note of whether they were partial or only on one side, and if they had curb cuts for easy accessibility at intersections. Additionally, we assessed the condition of the sidewalks, ranking them to determine if they were unsafe and in need of repair or replacement. We also assessed the condition of the street trees and evaluated their overall health, marking any that appeared to be in poor condition. We took note of the presence and condition of street lights, paying special attention to the brightness and coverage of the lights. We also marked the location of any street signs for wayfinding. Additionally, we identified the type of storm drain, whether it was a ditch/swale, curb/gutter, or another type of system. Finally to document our observations, we took pictures of both sides of the street block, capturing any notable features or areas of concern. Additionally, if we identified any flaws or damages in the sidewalk, we took pictures to document the issues.\nHere are the comprehensive maps of our WINVEST site assessments, as well as a photo of a typical house that we would assess.\n \n\n\n\nHouse in Independence Location 1\n\n\n\n\n\nIndependence Location 1\n\n\n\n\n\nIndependence Location 2\n\n\nOn Wednesday, our team had a much-needed meeting with our client. In light of the team changes that occurred last week, we found it necessary to reevaluate the scope of the project. This allowed us to ensure that we are aligned with the client’s expectations and make any necessary adjustments to our plans moving forward. By the end of the meeting, I was assigned to improve all seven spider and finish implementing the data cleaner program that was described in Week Five. Additionally, I was tasked with demonstrating that other products could be implemented into the spiders, all by next Wednesday. This means that I have a deadline to complete these tasks and showcase the flexibility of my system in incorporating new products. Which took up the majority of my time to finish.\nOn Thursday, I delivered a coffee talk on spiders, providing a concise summary of everything I have learned, which you can view here:"
  },
  {
    "objectID": "posts/Aaron_C_Week7/Aaron_C_Week7.html",
    "href": "posts/Aaron_C_Week7/Aaron_C_Week7.html",
    "title": "Week Seven",
    "section": "",
    "text": "From Saturday to Wednesday, my primary focus was on enhancing, optimizing, and implementing the data cleaner program for all seven spiders, as outlined in week five. I devoted the majority of my time during this period to ensure the successful implementation of these improvements across all spiders, meeting the Wednesday deadline.\nIn addition to my main focus, I went extra mile by successfully integrating new products into one of the spiders. While the primary objective of this demonstration wasn’t to showcase the data cleaning process, as it had already been proven to scale alongside the program by cleaning the data in real-time, the main aim was to highlight the system’s dynamic product handling and expansion capabilities, which could be customized to meet the employer’s specific requirements.\nThe rest of the week was spent on the presentation."
  },
  {
    "objectID": "posts/Aaron_C_Week7/Week7_WrapUp.html",
    "href": "posts/Aaron_C_Week7/Week7_WrapUp.html",
    "title": "AI/Local Food Team Week Seven Wrap Up",
    "section": "",
    "text": "A comprehensive map to showcase the prices of eggs and bacon across various counties using the collected data. This map serves as a valuable tool for identifying trends and patterns in pricing, as well as understanding customer preferences towards specific brands. Additionally, the map aids in the selection of suitable selling locations by considering crucial factors such as brand reputation, pricing, and travel distance (cost).\nSeveral web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database. These spiders will automate the process of data scraping, enabling repetitive and efficient collection of data.\nShowcase the capability of the spiders with a specific crop example. The spiders will be utilized to extract data for one or more of the following six products: tomatoes (regardless of the type), carrots, green onions, potatoes, spinach, lettuce. This demonstration will effectively highlight the functionality and effectiveness of the spiders in retrieving the desired data.\nOptimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit. We will explore the factors and methodology to estimate the demand and supply.\n\n\n\n\n\nThis week’s tasks:\n\nConsolidated all scraped data from all different websites into one master file to be used for data analysis.\nData Analysis:\n\nImported necessary packages in Juypter Notebook for data analysis.\nStarted cleaning the data: Missing values, fixing datatypes\nStarted exploring the data using various visualizations.\n\n\n\n\nConsolidated all collected data in three master lists of Eggs, Bacon and Heirloom tomato respectively. This would let work separately on each product’s data set and would give us more insights about the dataset.\n\n\n\nStarted the data analysis by importing the necessary packages, we will keep adding more packages as we go along with analysis part.\n\n\n\n\n\nCleaning:\nWe found there were a lot of missing values in our data set for various columns\n\n\n\nWe dealt with missing values for filling NA like shown in the snapshot below:\n\n\n\n\n\n\n\n\nwe can analyze the relationship between the current price and the original price of the heirloom tomatoes.\nThis analysis can help you understand the pricing trends and calculate potential discounts or price differences.\nVisualize the distribution of current prices using histograms, box plots, or kernel density plots to gain insights into the pricing range and identify any outliers.\n\n\n\n\n\n\n\n\nAnalyze the weight of the heirloom tomatoes by comparing the weight in pounds and the true weight.\nThis analysis can help you determine if there are any variations in weight and assess the accuracy of weight measurements.\nCreating scatter plots or line plots to visualize the relationship between weight in pounds and true weight.\n\n\n\n\n\n\n\n\nAnalyze the different brands of heirloom tomatoes available in the dataset. Calculate the frequency of each brand to determine the most popular ones.\nCreating a bar chart and pie chart to visualize the distribution of brands and identify the market share of each brand.\n\n\n\n\n\n\n\n\nAnalyze the presence of organic and locally sourced heirloom tomatoes. Calculate the percentage of organic and local products in the dataset. Creating a bar chart or pie chart to visualize the proportion of organic and local tomatoes."
  },
  {
    "objectID": "posts/Aaron_C_Week7/Week7_WrapUp.html#ai-and-local-food-team-plans-to-produce-following-outputs-at-the-end-of-this-years-project",
    "href": "posts/Aaron_C_Week7/Week7_WrapUp.html#ai-and-local-food-team-plans-to-produce-following-outputs-at-the-end-of-this-years-project",
    "title": "AI/Local Food Team Week Seven Wrap Up",
    "section": "",
    "text": "A comprehensive map to showcase the prices of eggs and bacon across various counties using the collected data. This map serves as a valuable tool for identifying trends and patterns in pricing, as well as understanding customer preferences towards specific brands. Additionally, the map aids in the selection of suitable selling locations by considering crucial factors such as brand reputation, pricing, and travel distance (cost).\nSeveral web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database. These spiders will automate the process of data scraping, enabling repetitive and efficient collection of data.\nShowcase the capability of the spiders with a specific crop example. The spiders will be utilized to extract data for one or more of the following six products: tomatoes (regardless of the type), carrots, green onions, potatoes, spinach, lettuce. This demonstration will effectively highlight the functionality and effectiveness of the spiders in retrieving the desired data.\nOptimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit. We will explore the factors and methodology to estimate the demand and supply.\n\n\n\n\n\nThis week’s tasks:\n\nConsolidated all scraped data from all different websites into one master file to be used for data analysis.\nData Analysis:\n\nImported necessary packages in Juypter Notebook for data analysis.\nStarted cleaning the data: Missing values, fixing datatypes\nStarted exploring the data using various visualizations.\n\n\n\n\nConsolidated all collected data in three master lists of Eggs, Bacon and Heirloom tomato respectively. This would let work separately on each product’s data set and would give us more insights about the dataset.\n\n\n\nStarted the data analysis by importing the necessary packages, we will keep adding more packages as we go along with analysis part.\n\n\n\n\n\nCleaning:\nWe found there were a lot of missing values in our data set for various columns\n\n\n\nWe dealt with missing values for filling NA like shown in the snapshot below:\n\n\n\n\n\n\n\n\nwe can analyze the relationship between the current price and the original price of the heirloom tomatoes.\nThis analysis can help you understand the pricing trends and calculate potential discounts or price differences.\nVisualize the distribution of current prices using histograms, box plots, or kernel density plots to gain insights into the pricing range and identify any outliers.\n\n\n\n\n\n\n\n\nAnalyze the weight of the heirloom tomatoes by comparing the weight in pounds and the true weight.\nThis analysis can help you determine if there are any variations in weight and assess the accuracy of weight measurements.\nCreating scatter plots or line plots to visualize the relationship between weight in pounds and true weight.\n\n\n\n\n\n\n\n\nAnalyze the different brands of heirloom tomatoes available in the dataset. Calculate the frequency of each brand to determine the most popular ones.\nCreating a bar chart and pie chart to visualize the distribution of brands and identify the market share of each brand.\n\n\n\n\n\n\n\n\nAnalyze the presence of organic and locally sourced heirloom tomatoes. Calculate the percentage of organic and local products in the dataset. Creating a bar chart or pie chart to visualize the proportion of organic and local tomatoes."
  },
  {
    "objectID": "posts/Aaron_C_Week7/Week7_WrapUp.html#several-web-scrapping-spiders-for-selected-websites-to-facilitate-the-creation-of-a-comprehensive-product-database.",
    "href": "posts/Aaron_C_Week7/Week7_WrapUp.html#several-web-scrapping-spiders-for-selected-websites-to-facilitate-the-creation-of-a-comprehensive-product-database.",
    "title": "AI/Local Food Team Week Seven Wrap Up",
    "section": "Several web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database.",
    "text": "Several web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database.\nTo facilitate the creation of a comprehensive product database, we have developed several web-scraping spiders for websites such as\n1. Fresh Thyme\n2. Hy-Vee\n3. Gateway Market\n4. New Pioneer Co-op\n5. Russ’s Market\n6. Iowa Food Hub\n7. Joia Food Farm\nThese spiders automate the data scraping process, eliminating the need for repetitive data collection and significantly increasing our efficiency. It’s important to note that this list is not exhaustive, as our web-scraping spiders can be expanded to include other websites beyond those listed here."
  },
  {
    "objectID": "posts/Aaron_C_Week7/Week7_WrapUp.html#showcase-the-capability-of-the-spiders-with-a-specific-crop-example.",
    "href": "posts/Aaron_C_Week7/Week7_WrapUp.html#showcase-the-capability-of-the-spiders-with-a-specific-crop-example.",
    "title": "AI/Local Food Team Week Seven Wrap Up",
    "section": "Showcase the capability of the spiders with a specific crop example.",
    "text": "Showcase the capability of the spiders with a specific crop example.\nIt’s worth noting that the capability of these spiders is not limited to a specific product type; they can be utilized to extract data from a wide selection of products. As an example, we have successfully demonstrated their functionality and effectiveness in retrieving data for various products, including specific crops such as tomatoes, carrots, green onions, potatoes, spinach, lettuce, and many more.\n\n\n\n\n\nMoreover, these spiders can be further enhanced to automate the data cleaning processes which runs concurrently within the spider. This approach allows us to efficiently address data quality issues, ensuring that the collected data is reliable and ready for analysis. The main objective of this enhancement is to guarantee that the collected data is thoroughly cleaned before integrating it into a file format, providing a seamless transition for further analysis and utilization. This specific example Highlights Heirloom Tomatoes from the listed stores as mention above."
  },
  {
    "objectID": "posts/Aaron_C_Week7/Week7_WrapUp.html#optimization-of-the-crop-flow-from-the-point-of-supply-to-the-point-of-demand-that-maximizes-overall-profit",
    "href": "posts/Aaron_C_Week7/Week7_WrapUp.html#optimization-of-the-crop-flow-from-the-point-of-supply-to-the-point-of-demand-that-maximizes-overall-profit",
    "title": "AI/Local Food Team Week Seven Wrap Up",
    "section": "Optimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit",
    "text": "Optimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit\n\n\n\n\n\n\nThe red points represent the counties.\nThe green lines represent the flow of crops, and the blue arrow shows the direction of the flow.\nMaximizes the revenue by selling the crops.\nMinimizes the cost of distance traveled.\nRelaxed previous assumption “Supply is greater than demand”\n\nThe following might be included in the project:\n\nA separate account of fresh and not fresh products.\nConsideration of each individual farmer’s profit.\nConsideration of the flow of trucks rather than the flow of crops."
  },
  {
    "objectID": "posts/Aaron_C_Week7/Week7_WrapUp.html#demand-and-supply-estimation",
    "href": "posts/Aaron_C_Week7/Week7_WrapUp.html#demand-and-supply-estimation",
    "title": "AI/Local Food Team Week Seven Wrap Up",
    "section": "Demand and supply estimation",
    "text": "Demand and supply estimation\n\n\n\n\n\nDisclaimer: Supply and demand estimation requires further correction, and the methodology needs improvements.\n\n\nDemand Estimation:\n\nThe population of United States is 334.9 million as of June 26, 2023. (https://www.census.gov/popclock/)\nIn 2017, fresh market consumption was 20.3 pounds per capita. (https://www.agmrc.org/commodities-products/vegetables/tomatoes)\nSo, the demand for fresh tomatoes in the United States is (334.9 million x 20.3 pounds) or 3.08 Million Metric Tons.\nThere is around 1% of the US population lives in Iowa.\nThat translates to 30.8 thousand Metric Tons of tomatoes demand in Iowa.\nConsidering the demand for tomatoes stays the same for 52 weeks. The demand for tomatoes in Iowa per week is 592 Metric Tons.\nWe will allocate this demand to each county of Iowa by population.\n\n\n\n\n\n\n\n\nSupply Estimation:\n\nThe growing season of tomatoes is between May to Mid-September in Iowa. (https://www.tomatofest.com/Tomato_Growing_Zone_Maps_s/164.htm)\nWe have 135 days between planting and harvesting tomatoes.\nTomatoes require 100 days to fully mature. However, there are some special varieties of tomatoes that require 50-60 days to mature.  (https://www.gardeningknowhow.com/edible/vegetables/tomato/planting-time-for-tomatoes.htm)\nSo, we are considering on average tomatoes take 80 days to be harvested.\nIn this context, we assumed that being mature represents the time between sowing seed and harvesting full-grown tomatoes.\nWe are only considering single cultivation of tomatoes during a year.\nSo, the tomatoes will be harvested during the timeline day 80-135 or, during late July to mid-September, that is 7.85 or 8 weeks. \nIn our calculations, we will estimate the weekly supply of tomatoes from late July to mid-September.\nIn 2020, approximately 12,619.2 tons of fresh market tomatoes were harvested from approximately 272,900 acres. (https://www.agmrc.org/commodities-products/vegetables/tomatoes)\nFlorida and California account for about two-thirds of the national fresh tomato production (Wu, F., Guan, Z., & Suh, D. H. (2017). The Effects of Tomato Suspension Agreements on Market Price Dynamics.; mentioned in https://edis.ifas.ufl.edu/publication/FE1027)\nThe remaining 48 states are responsible for one-third of the national fresh tomatoes. That is 4206.4 tons.\nTotal farming land in USA is 900.21 million acres as of 2022.\nTotal farm land in Florida is 9.73 million acres as of 2017.\nTotal farm land in California is 24.23 million acres as of 2017.\n (https://www.nass.usda.gov/AgCensus)\nRemaining 48 states have a farm land of 866.25 million acres.\nTotal farmland in Iowa is 30.56 million acres. (https://www.nass.usda.gov/AgCensus)\nConsidering tomatoes are grown uniformly in these farmlands, the proportion of farmland Iowa has compared to all 48 states (without California, Florida) is 3.52%.\nThe production of tomatoes in Iowa is (3.52% x 4206.4 tons) or, 148 tons.\nAs the production of tomatoes is distributed over 8 weeks (as mentioned in no 8), the weekly supply quantity of tomatoes is (148/8) or 18.5 tons or 16.78 Metric Tons.\nConsidering the average farm size in each county of Iowa is the same. The supply of tomatoes can be obtained from the number of farms.\n\n\n\n\n\n\nTeaser video: Video"
  },
  {
    "objectID": "posts/Aaron_C_Week8/Week8_WrapUp.html",
    "href": "posts/Aaron_C_Week8/Week8_WrapUp.html",
    "title": "AI/Local Food Team Week Eight Wrap Up",
    "section": "",
    "text": "A comprehensive map to showcase the prices of eggs and bacon across various counties using the collected data. This map serves as a valuable tool for identifying trends and patterns in pricing, as well as understanding customer preferences towards specific brands. Additionally, the map aids in the selection of suitable selling locations by considering crucial factors such as brand reputation, pricing, and travel distance (cost).\nSeveral web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database. These spiders will automate the process of data scraping, enabling repetitive and efficient collection of data.\nShowcase the capability of the spiders with a specific crop example. The spiders will be utilized to extract data for one or more of the following six products: tomatoes (regardless of the type), carrots, green onions, potatoes, spinach, lettuce. This demonstration will effectively highlight the functionality and effectiveness of the spiders in retrieving the desired data.\nOptimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit. We will explore the factors and methodology to estimate the demand and supply."
  },
  {
    "objectID": "posts/Aaron_C_Week8/Week8_WrapUp.html#ai-and-local-food-team-plans-to-produce-following-outputs-at-the-end-of-this-years-project",
    "href": "posts/Aaron_C_Week8/Week8_WrapUp.html#ai-and-local-food-team-plans-to-produce-following-outputs-at-the-end-of-this-years-project",
    "title": "AI/Local Food Team Week Eight Wrap Up",
    "section": "",
    "text": "A comprehensive map to showcase the prices of eggs and bacon across various counties using the collected data. This map serves as a valuable tool for identifying trends and patterns in pricing, as well as understanding customer preferences towards specific brands. Additionally, the map aids in the selection of suitable selling locations by considering crucial factors such as brand reputation, pricing, and travel distance (cost).\nSeveral web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database. These spiders will automate the process of data scraping, enabling repetitive and efficient collection of data.\nShowcase the capability of the spiders with a specific crop example. The spiders will be utilized to extract data for one or more of the following six products: tomatoes (regardless of the type), carrots, green onions, potatoes, spinach, lettuce. This demonstration will effectively highlight the functionality and effectiveness of the spiders in retrieving the desired data.\nOptimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit. We will explore the factors and methodology to estimate the demand and supply."
  },
  {
    "objectID": "posts/Aaron_C_Week8/Week8_WrapUp.html#final-presentation-flow",
    "href": "posts/Aaron_C_Week8/Week8_WrapUp.html#final-presentation-flow",
    "title": "AI/Local Food Team Week Eight Wrap Up",
    "section": "Final Presentation Flow",
    "text": "Final Presentation Flow\n\n\n\n\n\nSpeaker\n\n\n\n\n\nTopic\n\n\n\n\n\nTime\n\n\n\n\n\n\n\n \nSwati\n\n\n\n\nIntroduction\n\nWhat the project is about\nWhat we plan to achieve\nWhy is this important\n\n\n\n\n\n \n6 - 8 mins\n\n\n\n\n\n\n \nAaron\n\n\n\n\nWeb Scrapping (Spiders)\n\nWith what we started\nHow we did scraping\nWhat we achieved at the end\nAny interesting stuff\n\n\n\n\n\n \n10 - 12 mins\n\n\n\n\n\n\n \nSwati\n\n\n\n\nData Analysis\n\nVisualization (What graph say)\nAnswering research questions\nWhat model we used (if any)\n\n\n\n\n\n \n8 - 10 mins\n\n\n\n\n\n\n \nSadat\n\n\n\n\nCrop flow optimization\n\nOutput\nHow we achieved\n\n\n\n\n\n \n10 - 12 mins\n\n\n\n\n\n\n \nSadat / Swati\n\n\n\n\nConclusion and Future Vision\n\n\n\n\n \n2 - 3 mins"
  }
]