[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A link to the DSPG Blog: DSPG 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aaron Case DSPG Blog",
    "section": "",
    "text": "AI/Local Food Team Week Eight Wrap Up\n\n\n\n\n\n\n\nWeek Eight\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\nAI/Local Food Team\n\n\n\n\n\n\n  \n\n\n\n\nWeek Seven\n\n\n\n\n\n\n\nWeek Seven\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nAI/Local Food Team Week Seven Wrap Up\n\n\n\n\n\n\n\nWeek Seven\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nAI/Local Food Team\n\n\n\n\n\n\n  \n\n\n\n\nWeek Six\n\n\n\n\n\n\n\nWeek Six\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nSpiders\n\n\n\n\n\n\n\nWeek Four\n\n\nWeek Five\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Five\n\n\n\n\n\n\n\nWeek Five\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Four\n\n\n\n\n\n\n\nWeek Four\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nAI/Local Food Week Three Wrap Up\n\n\n\n\n\n\n\nWeek Three\n\n\nWrap Up\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nAI/Local Food Team\n\n\n\n\n\n\n  \n\n\n\n\nWeek Three\n\n\n\n\n\n\n\nWeek Three\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Three Research\n\n\n\n\n\n\n\nWeek Three\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Two\n\n\n\n\n\n\n\nWeek Two\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nImporting Census Data\n\n\n\n\n\n\n\nWeek Two\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek One\n\n\n\n\n\n\n\nWeek One\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nCensus Visual\n\n\n\n\n\n\n\nWeek Two\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAaron Case\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Aaron_C_MultiWeek/Aaron_C_Spiders.html",
    "href": "posts/Aaron_C_MultiWeek/Aaron_C_Spiders.html",
    "title": "Spiders",
    "section": "",
    "text": "Web scraping in essence is the process of extracting data from websites using automated software. As for the process, the web-scraping pipeline goes as follows.\n\nSetup - Understand what we want to do and find sources to help us do it.\nAcquisition\n\nRead in the raw data from online.\nFormat these data to be usable.\nAccessing the data.\nParsing this information.\nExtracting these data into meaningful and useful data structures.\n\nProcessing - There are many options to this but the main goal is to run the downloaded data through whatever analyses or processes needed to achieve the desired goal.\n\nThere are many web scraping tools available, but two of the one’s that I’ve the most success in are Scrapy and Selenium.\nScrapy is a Python package that is primarily used for web scraping. Scrapy provides a framework that offers a simple web crawling tool to extract data from websites. These tools are commonly known as spiders and allow for easy extraction of desired information. A spider program can crawl through websites and extracts data from them (Just like real spiders crawl through there webs). Scrapy works by sending HTTP requests to websites and receiving HTML responses, then the spider parses the HTML to extract the data it needs. There is a Data camp course that is currently available to use and is highly recommended watching. These are some of benefit of using Scrapy that I’ve found:\n\nScrapy is designed to be fast and can handle large volumes of data. It’s built on top of Twisted, an asynchronous networking framework, which in simple terms makes it very fast. It can handle thousands of requests per second without slowing down. This makes it really handy for the scope of this project.\nScrapy is highly customizable and can be configured to work with many different websites and data formats. Allowing us to specify how the spider should act when navigating through websites and extract data that we told it to collect. Scrapy also provides features for handling cookies, redirects, and other HTTP features.\nScrapy comes with built-in support for XPath and CSS selectors. With the right skill set these tools provide a powerful and easy way to extract data from HTML and XML documents.\nScrapy automatically comes with built-in support for data cleaning, Making the the cleaning and normalize scraped data much easier.\n\nAs for the cons of Scrapy:\n\nScrapy does have a relatively steep learning curve to it. Especially if you are new to web scraping, XPath/CSS selectors, or asynchronous programming. Unless you have more than a week to spend learning and doing web scrapping it is best to spend the time doing another method. Not to mention the fact that you need to have some knowledge of Python to use it effectively\nScrapy does not have built-in support for JavaScript, which means that it cannot scrape websites and other dynamic web content that rely heavily on JavaScript for rendering content. Which a vast amount of websites rely on to function.\nThis tool is not perfect nor undetectable. Some websites may block Scrapy from accessing their content, especially if they detect that the traffic is coming from a bot.\nScrapy does help you in cleaning data however it is not perfect. You still need to spend some time cleaning the data that was collected and may require additional libraries, such as Pandas or NumPy, to process and analyze scraped data.\n\nSelenium is another Python package which is used for automating web browsers. It allows you to control a browser programatically, which mimics the actions of a what a user will do. Selenium uses a web driver to control your web browser and can interact with several different browsers (such as Chrome, Firefox, and Safari). Selenium works by opening up a web browser and navigates to the target website. The user can then interact with the website using Selenium commands. Just like Scrapy, Selenium can also extract data from the website using XPath and CSS selectors. As for some of benefit of using Selenium:\n\nSelenium has the ability to handle JavaScript and other dynamic web content. Allowing it to handle websites that Scrapy is unable too.\nSelenium can handle more complex interactions with websites, such as clicking buttons, filling out forms, scrolling, etc. which can simulate user behavior. Making Selenium more diverse and a power tool to use when doing specific tasks that require a more human like approach.\n\nFor the Cons of Selenium:\n\nSelenium can be really slow and very resource-intensive. Especially when automating complex tasks or interacting with multiple pages. It requires your web browser to be opened to run meaning If your internet or computer is slow it has a really hard time doing anything you want it. This time that you have to wait for the program to preform any simple task is not ideal when working through large data sets.\nIt is prone to errors and requires extensive debugging and testing. For example, if say that you close your browser or if it freezes (for long periods of time) the program will not function correctly and throw an error.\nIt can be very challenging to set up and configure. Not to mention that It can be difficult to maintain test scripts when the web application is updated or changed. Meaning if you are not dedicated in learning or maintaining your code it will come with a bunch of headaches down the road.\n\nIdeally for this project I strongly recommend using Scrapy spiders were ever possible. Scrapy’s speed and efficiency and overall better performance makes this part of the process much more tolerable in building spiders. However, when it comes to more complex tasks and websites Selenium is the way to go. I would also like to acknowledge that there are other tools and resources that could have been explored and used which may be better or more efficient for the scope of the project. These two tools are just some that I found and was able to implement in the time frame I was given. For example, there is a python package called Scrapy Splash. This works as an extension for Scrapy which provides a JavaScript rendering service. Allowing Scrapy to handle JavaScript and other dynamic web content, similar to Selenium. I was not able to master this tool, nor could I successfully get it this tool to work. If I had more time to develop a clear comprehension of this utility and fully understood how to get it to work. It would have been the tool of choice to handle such websites. I used Selenium since I had more success in getting it to work in comparison.\nThis code covers the bare minimum of Scrapy and is rather (hopefully) intuitive. This can be edited/modified to fit your needs and is just an example.\n\n# Imports for Scrapy Example\nimport scrapy #Required Import\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.log import configure_logging # For debugging\n\n# The Class name can be anything but you need to call this later\nclass SpiderClassName(scrapy.Spider):\n    name = \"spider name\" \n\n    def start_requests( self ): # This is required to start the spider\n        # For one url\n        Desired_Website = [ 'https://www.example.com' ]\n        yield scrapy.Request( url = Desired_Website, callback = self.Some_Function_Name)\n\n        # For many urls\n        urls = [ 'https://www.examples.com', ... ]\n        for url in urls:\n            yield scrapy.Request( url = url, callback = self.parseAndFollow)\n\n        # When passing in varables from one function to the next you use meta\n        for url in urls:\n            yield scrapy.Request( url = url, callback = self.Meta_Example, meta={'Meta_Name': data, ...})\n    \n    def parseAndFollow( self, response ):\n        # Direct to the links in the xpath\n        Extrated_XPath = '//tag-name(@attribute, \"attrib info\")'\n        XPath_links = Extrated_XPath.xpath( './@href' )\n        # Anoter way\n        XPath_links = Extrated_XPath.xpath('//tag-name(@attribute, \"attrib info\")/@href')\n        # Extract the links (as a list of strings)\n        links_to_follow = XPath_links.extract()\n        # Follow the links to the next parser\n        for url in links_to_follow:\n            yield response.follow( url = url, callback = self.parse )\n    \n    def parse( self, response ):\n        # To get the text of the xpath\n        Title_Example = response.xpath('//h1[contains(@class,\"title\")]/text()') \n        # Extract text title\n        Title_Extracted = Title_Example.extract_first()\n        # Clean the title text\n        Title_Text = Title_Extracted.strip()\n        # Saving the text so that we dont lose what we just did\n        Text_Element_List.append(Title_Text)\n\n    def Meta_Example( self, response ):\n        #This is how pass in your arguments\n        Meta_Data = response.meta.get('Meta_Name')\n    \nText_Element_List = []\n# To see the inner mechanics of the spider helpful to have not required\nconfigure_logging()\n# initiate a CrawlerProcess\nprocess = CrawlerProcess()\n# Tell the process which spider to use\nprocess.crawl(SpiderClassName)\n# Start the crawling process\nprocess.start()\n# This is to stop the spider\nprocess.stop()\n# At this point the spider is finished and now we have everything that was saved\nprint(Text_Element_List)\n\nAs for Selenium this code covers the bare minimum and serves as a deminstration of how to handle JavaScript websites. Once again this can be edited/modified to fit your needs and is just an example.\n\n# Imports for Scraping\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# Since I use Firefox this is how I would typically set it up.\n# Note: This is not the only way to do it. Its just how I do this part \nfrom selenium.webdriver.firefox.service import Service as FirefoxService\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom os import path\ndriver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n\n# Another example of how you can do it \n# Set the path to the driver\ndriver_path = \"/path/to/chromedriver\"\n# Create a new instance of the Chrome driver\ndriver = webdriver.Chrome(driver_path)\n\n# Navigate to the website page\ndriver.get(\"https://example.com/\")\n\n# Setup\nwaitTime = 10\n\n# I like using XPaths because I am most comfortable using them \nTitle_XPath = '//h1(@class,\"title\")'\n# We need to wait for the page to render before we do anything otherwise the data we want wont be present.\nelements = WebDriverWait(driver, waitTime).until(EC.visibility_of_element_located((By.XPATH, Title_XPath)))\n# You can also do it like this for CSS\nelements = WebDriverWait(driver, waitTime).until(EC.visibility_of_element_located((By.CSS_SELECTOR, Title_CSS)))\n# Extracts the text of the element\nText = elements[0].text\n\n# Close the browser\ndriver.quit()\n\n#Data Has been extracted\nprint(Text)"
  },
  {
    "objectID": "posts/Aaron_C_Week1/Aaron_C_Week1.html",
    "href": "posts/Aaron_C_Week1/Aaron_C_Week1.html",
    "title": "Week One",
    "section": "",
    "text": "During the first week, I had the chance to provide a brief introduction of myself to the client. This involved taking the necessary time to clearly communicate my role in the project, thereby establishing a professional connection with the Employer.\nAfter concluding the introduction, we proceeded with the debriefing process of the project with the Employer. We discussed the project’s background, with a specific focus on the progress made during the first year. Additionally, we received a comprehensive overview of the previous work that had been completed. Once we were caught up with this overview, we dived into the project’s scope, addressing the specific tasks and goals that needed to be accomplished. This in-depth discussion provided us with a better understanding of the project and clarified what needed to be done.\nRegarding the goals for week one, my primary objective was to acquire as much knowledge and experience about Python and R in Datacamp.\nLessons completed:\n\nIntroduction to R\nIntermediate R\nCleaning Data in R\nIntroduction to Python\nIntermediate Python\nData Manipulation with pandas\nAI Fundamentals\nGitHub Concepts"
  },
  {
    "objectID": "posts/Aaron_C_Week2/Aaron_C_Week2.html",
    "href": "posts/Aaron_C_Week2/Aaron_C_Week2.html",
    "title": "Week Two",
    "section": "",
    "text": "During week two, the plan in place was to continue learning using Datacamp. I successfully completed two lectures, Writing Efficient Python Code and Web Scraping in Python.\nAlso during week two I was tasked to learn Tidycensus in R for data visualization. In Census Visual provides a demonstration of the code I have written and showcases the knowledge and skills I have acquired using Tidycensus with the American Community Survey Data.\nDuring the week DSPG needed to extract a large amount of data from the American Community Survey. Since this process was lengthy, it became one of the main problems that the DSPG encountered during the week. However, I was able to solve it using the R code provided in Importing Census Data."
  },
  {
    "objectID": "posts/Aaron_C_Week2/Importing Census Data.html",
    "href": "posts/Aaron_C_Week2/Importing Census Data.html",
    "title": "Importing Census Data",
    "section": "",
    "text": "Since I needed to extract a large amount of data from the American Community Survey (ASC) and convert it into a CSV file. In light of this I wrote this code to extract ASC data from the table codes. The Code takes in a list of names that you want the file name to be. Along with the corresponding table code. It also takes in a folder name. It then makes (if needed) and adds the CVS files to the specified folder (For a clean directory). Its important to note that where you run the R file is where a folder is made. I made this with the intentions of it being editable (and hopefully user friendly).\n\n####################\n#  Inserting Data  #\n####################\n\n\nfolder &lt;- \"Folder_Name_Here\" # &lt;---------- Change this value FIRST!\n\nACSList &lt;- c(\n    # \"Data Name\",\"DataCode\", \n    # ...\n  ) \nACSListToCSV(ACSList,folder)\n\n\n##########################################\n#  Global Variables That can be Changed  #\n##########################################\n\n#To change the get_acs() geography variable\ngeographyType &lt;- \"county\"\n\n#To change the get_acs() servay variable\nservayType &lt;- \"ACS5\"\n\n#Change to NULL if no state\nstateType &lt;- \"IA\"\n\n#This will make a geometry file as well if TRUE\nwithGeometry &lt;- FALSE\n\n#checking the year\nyear = NULL\n\n###################################\n#  Functions that make life easy  #\n###################################\n\n#Imports \nlibrary(tidycensus) #For ACS extractions\nlibrary(stringi) #For folderNameFixer()\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\n\n\n#File name changers. This will set the name of the file. Feel free to edit this\nrenameFile &lt;- function(tableTitle, tableCode, isGeometric){\n  fileName &lt;- paste(tableTitle, \" (\", tableCode, sep='')\n  if(isGeometric){\n    fileName = paste(fileName, \", \",capFirst(\"tract\"), sep='')\n  } else {\n    fileName = paste(fileName, \", \",capFirst(geographyType), sep='')\n  }\n  if(is.null(stateType) == FALSE){\n    fileName = paste(fileName, \", \", stateType, sep='')\n  }\n  if(is.null(year) == FALSE){\n    fileName = paste(fileName, \", \", year, sep='')\n  }\n  fileName = paste(fileName, \", \", servayType, sep='')\n  if(isGeometric){\n    fileName = paste(fileName, \", Geometry).csv\", sep='')\n  } else {\n    fileName = paste(fileName, \").csv\", sep='')\n  }\n  return(fileName)\n}\n\n#ACS extractions.\n#Feel free to add to this list.\ngeoACSDataFrame = function(tableCode){\n  get_acs(\n    #Add Changes here\n    geography = \"tract\",\n    table = tableCode,\n    servay = servayType,\n    state = stateType,\n    geometry = TRUE\n  )\n}\n\ndefaultACSDataFrame = function(tableCode){\n  get_acs(\n    #Add Changes here\n    geography = geographyType,\n    table = tableCode,\n    servay = servayType,\n    state = stateType\n  )\n}\n\n#Conditions for files fell free to edit this\nfileImplications &lt;- function(tableTitle, tableCode){\n  #Add a condition and apply both the ACS extraction and the File name changer\n  #Example\n  if(withGeometry){\n    fileName &lt;- renameFile(tableTitle, tableCode, TRUE)\n    #output with Geometry\n    dataToCSV(geoACSDataFrame(tableCode), fileName)\n  }\n  \n  fileName &lt;- renameFile(tableTitle, tableCode, FALSE)\n  #Default\n  #This Makes the CSV File\n  #format dataToCSV(your ACS DataFrame, File name changer() )\n  dataToCSV(defaultACSDataFrame(tableCode), fileName)\n}\n\n\n#For clarity capitalizes the first letter in a string and lowercases the rest (Feel free to Use)\ncapFirst = function(xStr){\n   paste(toupper(substring(xStr, 1, 1)), tolower(substring(xStr, 2, nchar(xStr))), sep = \"\")\n}\n#Validates and fixes folder name string (Feel free to edit)\nnameFixer &lt;- function(xStr, fixType){\n  #Replaces bad characters with ''\n  xStr &lt;- stri_replace_all_regex(xStr, \n                         pattern=c('/', ':', '\\\\*', '\"', '&lt;', '&gt;', '\\\\|'),\n                         replacement=c('-', '', '', '', '', '', ''),\n                         vectorize=FALSE)\n  if(xStr == \"\"){\n    if(capFirst(fixType) == \"Folder\"){\n      print(\"Ops the folder name has all bad characters lets fix that\")\n      xStr &lt;- \"New_Data_Folder\"\n    }\n    else{\n      print(\"Ops the file name has all bad characters lets fix that\")\n      xStr &lt;- \"New_Data_file\"\n    }\n  }\n  return(xStr)\n}\n#For bulk downloading tables\nACSListToCSV &lt;- function(bulkArray, folder){\n  if(length(bulkArray) %% 2){\n    #Scream if array is odd length. We don't need any mistakes!\n    print(\"Somethings Missing. List format should be like c( Data Name, DataCode, ... )\")\n    return(\"ERROR\")\n  }\n  #Validates folder name\n  folder &lt;- nameFixer(folder, \"Folder\")\n  #Makes a folder if needed\n  if(!file.exists(folder)){\n    dir.create(folder)\n    print(\"New Folder Made Adding Files\")\n  }\n  index &lt;- 1\n  #Adds files\n  while(index &lt; length(bulkArray)){\n    fileImplications(nameFixer(bulkArray[index], \"File\"), bulkArray[index+1]) # (title , code)\n    index &lt;- index + 2\n  }\n  print(\"All Files have been downloaded\")\n}\n#ACS Data frame to CSV file\ndataToCSV &lt;- function(data, fileName){\n  #This adds the file to the folder\n  path &lt;- paste(\".\\\\\",folder,\"\\\\\", fileName, sep='')\n  #Makes the CSV\n  write.csv(data, path)\n  print(paste(\"Added File:\",fileName))\n}"
  },
  {
    "objectID": "posts/Aaron_C_Week2/TidyCensusOverview.html",
    "href": "posts/Aaron_C_Week2/TidyCensusOverview.html",
    "title": "Census Visual",
    "section": "",
    "text": "Both a bar plot, and chorpleth map were made from the American Community Survey (ASC) data using Tidycensus library in R.\n\n#Imports for both Graph and Map\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(plotly)\nlibrary(ggiraph) \n\n#Grabing median_income for bar blot\nmedian_income &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  state = \"IA\", \n  year = 2021\n)\n\n#View the mid range of countys mean income\nmedian_income_data &lt;- median_income %&gt;%\n  #reducing the number of countys the graph can display\n  slice(floor(99 * 0.25):floor(99 * 0.75)+1) %&gt;%\n  #ordering from estimate highest to estimate lowest\n  arrange(desc(estimate))\n\n#The Bar plot\nmd_bar_plot &lt;- ggplot(median_income_data, aes(x = estimate, \n                                    y = reorder(NAME, estimate),\n                                    tooltip = estimate,\n                                    data_id = GEOID)) +\n  #Generating the error bars\n  geom_errorbar(aes(xmin = estimate - moe, \n                    xmax = estimate + moe),\n                    width = 0.5, \n                    size = 1) + \n  #Coloring the estimate dot \n  geom_point_interactive(color = \"darkblue\", size = 1.5) +\n  #Bottom Label range\n  scale_x_continuous(labels = label_dollar()) + \n  #County names and removing the Unnecessary words\n  scale_y_discrete(labels = function(x) str_remove(x, \" County, Iowa|, Iowa\")) +\n  #Graph labeling for views convince \n  labs(title = \"Median Income 2021 ACS\",\n       #subtitle = \"Counties in Iowa\",\n       caption = \"Data acquired with R and tidycensus. \\nError bars represent margin of error around estimates of Median income.\",\n       x = \"ACS Estimate Mean Income\",\n       y = \"Counties in Iowa\") + \n  #Text Sizing\n  theme_minimal(base_size = 8)\n#Making the graph interactive\nmd_bar_plot_interactive &lt;- girafe(ggobj = md_bar_plot) %&gt;% girafe_options(opts_hover(css = \"fill:purple;\"))\n\n#The Map\nmedian_income_map &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  state = \"IA\",\n  year = 2021,\n  geometry = TRUE\n)\n# The Map\nmd_chorpleth_map &lt;- ggplot(median_income_map, aes(fill = estimate)) + \n  #The map display\n  geom_sf() + \n  #Empty theme of the map\n  theme_void() + \n  #Colors the map \n  scale_fill_viridis_c(option = \"G\", n.breaks = 10) + \n  #Information\n  labs(title = \"Median Income by Census track\",\n       subtitle = \"\",\n       fill = \"ACS estimates\",\n       caption = \"Median Income by ACS tidycensus R package in 2021\")\n\n\n#This renders the bar plot \nmd_bar_plot_interactive\n\n#This renders the chorpleth map\nmd_chorpleth_map\n\n\n\n\nBar Plot\n\n\n\n\n\nChorpleth Map"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Aaron_C_Week3.html",
    "href": "posts/Aaron_C_Week3/Aaron_C_Week3.html",
    "title": "Week Three",
    "section": "",
    "text": "During week three, our focus was primarily on research and data collection for our project. This week was dedicated to gathering relevant information and conducting thorough research to support our project goals. In Week Three Research, I have compiled and summarized the information I gathered while researching bacon, eggs, and heirloom tomatoes. This resource outlines and highlights the key findings that I was able to uncover during my investigation.\nIn addition to our focus on research and data collection, I also prioritized learning during week three. Using Data camps and successfully completed Writing Efficient R Code lecture. Recognizing the crucial role of data collection in our project, I took the initiative in learning how to create web scrapers, also known as spiders. This involved dedicating a large portion of the week as well as most of my free time to acquire the necessary skills and knowledge in this area. My efforts in this yielded some positive results. I was able to successfully created a spider that efficiently extracts data from the provided weblink and organizes it into a pandas dataframe. The reason we store the data in this dataframe is because it makes it much easier to work with, especially when it comes to cleaning and exporting it into a comma-separated values (CSV) file for future use. It is important to note that while this image showcases an output of the extraction process, it is crucial to understand that it is far from being considered a finished product.\n\n\n\nOutput\n\n\nFinally at the end of the week I was tasked to present our weekly wrap up for the Team. This covers what the AI Local food team was able to accomplished for the week and can be view in Week Three Wrap Up."
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html",
    "href": "posts/Aaron_C_Week3/Week3_Research.html",
    "title": "Week Three Research",
    "section": "",
    "text": "Farmers Market Prices:\nAt the 2015 PFI Annual Conference, Kay Jensen collected farmers’ vegetable prices. Recent data shows price ranges for tomatoes, ranging from $2.50 to $4 per pound.\nExpatistan:\nTells the price of 1 kg (2 lb.) of tomatoes in Des Moines IA can be used to find other prices located in IA.\nAgMRC:\nProvides a some detailed and helpful references for tomatoes.\nMarket Maker:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nLocal Harvest:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nPractical farmers:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nInstacart:\nA cite that could be of use to find Items.\nNFMD:\nMarket on Central - Farmers Market in Historic Downtown Fort Dodge\nNational Retail Report - Specialty Crops ( USDA Fruits & Vegetables Market Report ):\nAdvertised Prices for Specialty Crops at Major Retail Supermarket Outlets it contains Heirloom Tomatoes but only specifies the Midwest U.S.\nFRED Economic Data:\nContains the average Price of Tomatoes( Tomatoes, Field Grown (Cost per Pound/453.6 Grams) in U.S. City Average) All fresh field grown and vine ripened round red tomatoes. Includes organic and non-organic.\nFarmers Market Nutrition Program:\nLists farmers markets in Iowa\nIowa Food Cooperative (Iowa Food Coop):\nContains some data that could be useful\nNASS Eggs:\nContains tons of data that could be useful\nList of Egg Farms in Iowa:\ncould be useful to source eggs"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html#some-helpful-data-found",
    "href": "posts/Aaron_C_Week3/Week3_Research.html#some-helpful-data-found",
    "title": "Week Three Research",
    "section": "",
    "text": "Farmers Market Prices:\nAt the 2015 PFI Annual Conference, Kay Jensen collected farmers’ vegetable prices. Recent data shows price ranges for tomatoes, ranging from $2.50 to $4 per pound.\nExpatistan:\nTells the price of 1 kg (2 lb.) of tomatoes in Des Moines IA can be used to find other prices located in IA.\nAgMRC:\nProvides a some detailed and helpful references for tomatoes.\nMarket Maker:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nLocal Harvest:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nPractical farmers:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nInstacart:\nA cite that could be of use to find Items.\nNFMD:\nMarket on Central - Farmers Market in Historic Downtown Fort Dodge\nNational Retail Report - Specialty Crops ( USDA Fruits & Vegetables Market Report ):\nAdvertised Prices for Specialty Crops at Major Retail Supermarket Outlets it contains Heirloom Tomatoes but only specifies the Midwest U.S.\nFRED Economic Data:\nContains the average Price of Tomatoes( Tomatoes, Field Grown (Cost per Pound/453.6 Grams) in U.S. City Average) All fresh field grown and vine ripened round red tomatoes. Includes organic and non-organic.\nFarmers Market Nutrition Program:\nLists farmers markets in Iowa\nIowa Food Cooperative (Iowa Food Coop):\nContains some data that could be useful\nNASS Eggs:\nContains tons of data that could be useful\nList of Egg Farms in Iowa:\ncould be useful to source eggs"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html#heirloom-tomatoes",
    "href": "posts/Aaron_C_Week3/Week3_Research.html#heirloom-tomatoes",
    "title": "Week Three Research",
    "section": "Heirloom Tomatoes",
    "text": "Heirloom Tomatoes\nPlaces that might use Heirloom Tomatoes\n\nJava House\n\nStores that sell Heirloom Tomatoes that we can collect data from\n\nGateway market\nRuss’s Market\nFresh thyme\nHy-Vee\nNew Pioneer coop\n\nInvestigated Stores\n\nDogpatch: They have seeds but no tomatoes for sale\nFairway: They don’t have any heirloom tomatoes for sale\nWalmart: They have seeds but no tomatoes for sale\nTarget: They don’t have any heirloom tomatoes for sale\nCostco: They don’t have any heirloom tomatoes for sale\nWhole Foods Market: They don’t have any heirloom tomatoes for sale in Iowa\nAldi: They don’t have any heirloom tomatoes for sale\nTrader Joe’s: They don’t have any heirloom tomatoes for sale\nCampbell’s Nutrition: They don’t have any heirloom tomatoes for sale\nRamsey’s Market: They don’t have any heirloom tomatoes for sale\nGoPuff: They don’t have any heirloom tomatoes for sale\nOver 150+ Other Websites searched"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html#egg",
    "href": "posts/Aaron_C_Week3/Week3_Research.html#egg",
    "title": "Week Three Research",
    "section": "EGG",
    "text": "EGG\nStores that sell Eggs that we can collect data from\n\nGateway market\nRuss’s Market\nFresh Thyme Market\nHv-Vee\nNew Pioneer coop\ndogpatch\nFairway\nwalmart\nTarget\nWhole Foods Market\nAldi\nTrader Joe’s\nRamsey’s Market\nGoPuff"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html#bacon",
    "href": "posts/Aaron_C_Week3/Week3_Research.html#bacon",
    "title": "Week Three Research",
    "section": "Bacon",
    "text": "Bacon\nStores that sell Bacon that we can collect data from\n\nGateway market\nRuss’s Market\nFresh Thyme Market\nHv-Vee\nNew Pioneer coop\ndogpatch\nFairway\nWalmart\nTarget\nCostco\nWhole Foods Market\nAldi\nTrader Joe’s\nRamsey’s Market\nGoPuff"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_WrapUp.html",
    "href": "posts/Aaron_C_Week3/Week3_WrapUp.html",
    "title": "AI/Local Food Week Three Wrap Up",
    "section": "",
    "text": "The currents project objectives for this week was to\n\nCatch up on any additional training.\nCollect and find data on heirloom tomatoes, eggs, and bacon.\nLearning how to do web scraping in python through Datacamp.\nBuilding programs to do web scraping"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_WrapUp.html#current-project-objectives",
    "href": "posts/Aaron_C_Week3/Week3_WrapUp.html#current-project-objectives",
    "title": "AI/Local Food Week Three Wrap Up",
    "section": "",
    "text": "The currents project objectives for this week was to\n\nCatch up on any additional training.\nCollect and find data on heirloom tomatoes, eggs, and bacon.\nLearning how to do web scraping in python through Datacamp.\nBuilding programs to do web scraping"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_WrapUp.html#works-in-progress",
    "href": "posts/Aaron_C_Week3/Week3_WrapUp.html#works-in-progress",
    "title": "AI/Local Food Week Three Wrap Up",
    "section": "Works in Progress",
    "text": "Works in Progress\nAn excel sheet that contains a list of small businesses of Iowa grocers that we had to go through and find places that had the data we wanted.\n\n\n\n\n\n\n\n\n\n\nThese are some examples of what we were looking for\n\n\n\n\n\n\n\n\n\n\nAlong with some manual data scraping. We started work on some data scraping programs (spiders). However, there are still some improvements that still need to be made. This image shows an example output of data we were able to scrape."
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_WrapUp.html#dspg-questions",
    "href": "posts/Aaron_C_Week3/Week3_WrapUp.html#dspg-questions",
    "title": "AI/Local Food Week Three Wrap Up",
    "section": "DSPG Questions",
    "text": "DSPG Questions\n\nAre there stores or market places that would be helpful for us to look into?\nIs anyone experienced in web scraping and if so there any advice that you have for us?"
  },
  {
    "objectID": "posts/Aaron_C_Week4/Aaron_C_Week4.html",
    "href": "posts/Aaron_C_Week4/Aaron_C_Week4.html",
    "title": "Week Four",
    "section": "",
    "text": "During week four, my main focus was on learning and building spiders. I dedicated a significant amount of time to this task and successfully completed the construction of three separate spiders. Nevertheless, It is important to note that while the spiders are functional, there is still room for further improvements, as the code is not yet fully refined. Additionally, During week four I participated in the collection of housing data in Slater Iowa for the WINVEST project. While this activity was aimed for preparation for whats to come in Week Six it did not detract from the primary focus on learning and building spiders during week four.\nFor a detailed explanation of my endeavors and what I have learned, you can visit this link: Spiders. This link will direct you to an overview of my progress, including updates on what I have learned during the week. Please note that I will be updating this link to include my knowledge from Week Five as well.\nFor those who are interested, I have included some of the outputs generated by the spider below. These outputs serve as examples of the spider’s functionality and demonstrate the successful results achieved during the learning and building process in week four.\nOutput of the Fresh Thyme Market Spider\nOutput of the Hy-Vee Spider \nOutput of the Gateway Market Spider"
  },
  {
    "objectID": "posts/Aaron_C_Week5/Aaron_C_Week5.html",
    "href": "posts/Aaron_C_Week5/Aaron_C_Week5.html",
    "title": "Week Five",
    "section": "",
    "text": "During week five, although it may not hold the utmost significance for the scope of the project, I wanted to mention that I had the opportunity to attend the ITAG VI conference. Throughout the conference, I had the privilege of attending various insightful lectures. While this detail may not directly impact the project, it is worth documenting as it contributes to my overall knowledge and professional growth. This conference provided valuable opportunities to learn from experts in their respective fields and gain new perspectives.\nAs for the majority of the week, my primary focus was on building and optimizing spiders. By the end of the week, I successfully completed the construction of four additional spiders, bringing the total number of spiders built for the project to seven. While progress has been made, there are still several improvements that need to be addressed before considering any of the spiders as finished.\nDue to performance issues and consistent absenteeism within our team, we had to make some changes that resulted in substantial delays to the current progression of our project. As a result, I was assigned the task of cleaning the data alongside spider development.\nIn light of these challenges, I believed it would be best to turn this hindrance into an opportunity by implementing a data cleaning program that runs concurrently within the spider! The purpose of this concurrent operation is to ensure the accuracy of the collected data. By incorporating this data cleaning program within each spider using composition, we can easily monitor the collection and cleaning processes in real time without compromising speed and efficiency.\nThis approach allows us to efficiently address data quality issues, ensuring that the collected data is reliable and ready for analysis. The main objective of this program is to guarantee that the collected data is thoroughly cleaned before integrating it into the pandas dataframe. This ensures that the data is ready to be exported in the CSV file format, providing a seamless transition for further analysis and utilization.\nNot only would this approach solve our current situation, but it would also make our programs more dynamic and easily adaptable. This means that we can easily incorporate more products to be added and cleaned for future development, ensuring that our system remains flexible and scalable!\nExpanding upon what I’ve learned in week four, I would like to direct you to the following link which provides a more in-depth summary of my knowledge of Spiders. This summary not only delves into the concepts and techniques I have learned, but also includes some example code to further illustrate them. Feel free to explore the link and delve deeper into my progress.\nAs for those who have been following our project and reading my previous posts, I would like to provide a recap of the spiders that have been developed so far:\n1. Fresh Thyme\n2. Hy-Vee\n3. Gateway Market\n4. New Pioneer Co-op\n5. Russ’s Market\n6. Iowa Food Hub\n7. Joia Food Farm"
  },
  {
    "objectID": "posts/Aaron_C_Week6/Aaron_C_Week6.html",
    "href": "posts/Aaron_C_Week6/Aaron_C_Week6.html",
    "title": "Week Six",
    "section": "",
    "text": "On Monday of week six, as briefly mentioned in Week Four, DSPG conducted assessments of houses in Grundy Center and New Hampton by taking pictures and tracking their characteristics using the Fulcrum app. We then continued our evaluations in Independence on Tuesday, which marked the conclusion of our WINVEST site assessments.\nDuring the assessments, we meticulously examined the houses’ structural features and recorded detailed notes on the condition of the driveway, foundation, gutters, paint, porch, roof, siding, walls, and window frames, as well as any other noteworthy features that were in poor condition. We also took into account the condition of the lot, noting any junk or debris present on the site. Additionally, we evaluated the sidewalk connecting to the house and made note of its condition. We paid extra close attention to the gutters, roof, siding, and landscape, assessing whether they were in good, fair, or poor condition. As we took and evaluated the pictures, we meticulously noted any obstructions in the photo(s) taken, paying close attention to whether they were caused by overgrown vegetation (such as bushes or weeds), trees, electrical posts, cars, or any other obstructions.\nAlong with evaluation of the housing, we also took note of our general impressions of the block. We evaluated the neighborhood sidewalks, taking note of whether they were partial or only on one side, and if they had curb cuts for easy accessibility at intersections. Additionally, we assessed the condition of the sidewalks, ranking them to determine if they were unsafe and in need of repair or replacement. We also assessed the condition of the street trees and evaluated their overall health, marking any that appeared to be in poor condition. We took note of the presence and condition of street lights, paying special attention to the brightness and coverage of the lights. We also marked the location of any street signs for wayfinding. Additionally, we identified the type of storm drain, whether it was a ditch/swale, curb/gutter, or another type of system. Finally to document our observations, we took pictures of both sides of the street block, capturing any notable features or areas of concern. Additionally, if we identified any flaws or damages in the sidewalk, we took pictures to document the issues.\nHere are the comprehensive maps of our WINVEST site assessments, as well as a photo of a typical house that we would assess.\n \n\n\n\nHouse in Independence Location 1\n\n\n\n\n\nIndependence Location 1\n\n\n\n\n\nIndependence Location 2\n\n\nOn Wednesday, our team had a much-needed meeting with our client. In light of the team changes that occurred last week, we found it necessary to reevaluate the scope of the project. This allowed us to ensure that we are aligned with the client’s expectations and make any necessary adjustments to our plans moving forward. By the end of the meeting, I was assigned to improve all seven spider and finish implementing the data cleaner program that was described in Week Five. Additionally, I was tasked with demonstrating that other products could be implemented into the spiders, all by next Wednesday. This means that I have a deadline to complete these tasks and showcase the flexibility of my system in incorporating new products. Which took up the majority of my time to finish.\nOn Thursday, I delivered a coffee talk on spiders, providing a concise summary of everything I have learned, which you can view here:"
  },
  {
    "objectID": "posts/Aaron_C_Week7/Aaron_C_Week7.html",
    "href": "posts/Aaron_C_Week7/Aaron_C_Week7.html",
    "title": "Week Seven",
    "section": "",
    "text": "From Saturday to Wednesday, my primary focus was on enhancing, optimizing, and implementing the data cleaner program for all seven spiders, as outlined in week five. I devoted the majority of my time during this period to ensure the successful implementation of these improvements across all spiders, meeting the Wednesday deadline.\nIn addition to my main focus, I went extra mile by successfully integrating new products into one of the spiders. While the primary objective of this demonstration wasn’t to showcase the data cleaning process, as it had already been proven to scale alongside the program by cleaning the data in real-time, the main aim was to highlight the system’s dynamic product handling and expansion capabilities, which could be customized to meet the employer’s specific requirements.\nThe rest of the week was spent on the presentation."
  },
  {
    "objectID": "posts/Aaron_C_Week7/Week7_WrapUp.html",
    "href": "posts/Aaron_C_Week7/Week7_WrapUp.html",
    "title": "AI/Local Food Team Week Seven Wrap Up",
    "section": "",
    "text": "A comprehensive map to showcase the prices of eggs and bacon across various counties using the collected data. This map serves as a valuable tool for identifying trends and patterns in pricing, as well as understanding customer preferences towards specific brands. Additionally, the map aids in the selection of suitable selling locations by considering crucial factors such as brand reputation, pricing, and travel distance (cost).\nSeveral web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database. These spiders will automate the process of data scraping, enabling repetitive and efficient collection of data.\nShowcase the capability of the spiders with a specific crop example. The spiders will be utilized to extract data for one or more of the following six products: tomatoes (regardless of the type), carrots, green onions, potatoes, spinach, lettuce. This demonstration will effectively highlight the functionality and effectiveness of the spiders in retrieving the desired data.\nOptimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit. We will explore the factors and methodology to estimate the demand and supply.\n\n\n\n\n\nThis week’s tasks:\n\nConsolidated all scraped data from all different websites into one master file to be used for data analysis.\nData Analysis:\n\nImported necessary packages in Juypter Notebook for data analysis.\nStarted cleaning the data: Missing values, fixing datatypes\nStarted exploring the data using various visualizations.\n\n\n\n\nConsolidated all collected data in three master lists of Eggs, Bacon and Heirloom tomato respectively. This would let work separately on each product’s data set and would give us more insights about the dataset.\n\n\n\nStarted the data analysis by importing the necessary packages, we will keep adding more packages as we go along with analysis part.\n\n\n\n\n\nCleaning:\nWe found there were a lot of missing values in our data set for various columns\n\n\n\nWe dealt with missing values for filling NA like shown in the snapshot below:\n\n\n\n\n\n\n\n\nwe can analyze the relationship between the current price and the original price of the heirloom tomatoes.\nThis analysis can help you understand the pricing trends and calculate potential discounts or price differences.\nVisualize the distribution of current prices using histograms, box plots, or kernel density plots to gain insights into the pricing range and identify any outliers.\n\n\n\n\n\n\n\n\nAnalyze the weight of the heirloom tomatoes by comparing the weight in pounds and the true weight.\nThis analysis can help you determine if there are any variations in weight and assess the accuracy of weight measurements.\nCreating scatter plots or line plots to visualize the relationship between weight in pounds and true weight.\n\n\n\n\n\n\n\n\nAnalyze the different brands of heirloom tomatoes available in the dataset. Calculate the frequency of each brand to determine the most popular ones.\nCreating a bar chart and pie chart to visualize the distribution of brands and identify the market share of each brand.\n\n\n\n\n\n\n\n\nAnalyze the presence of organic and locally sourced heirloom tomatoes. Calculate the percentage of organic and local products in the dataset. Creating a bar chart or pie chart to visualize the proportion of organic and local tomatoes."
  },
  {
    "objectID": "posts/Aaron_C_Week7/Week7_WrapUp.html#ai-and-local-food-team-plans-to-produce-following-outputs-at-the-end-of-this-years-project",
    "href": "posts/Aaron_C_Week7/Week7_WrapUp.html#ai-and-local-food-team-plans-to-produce-following-outputs-at-the-end-of-this-years-project",
    "title": "AI/Local Food Team Week Seven Wrap Up",
    "section": "",
    "text": "A comprehensive map to showcase the prices of eggs and bacon across various counties using the collected data. This map serves as a valuable tool for identifying trends and patterns in pricing, as well as understanding customer preferences towards specific brands. Additionally, the map aids in the selection of suitable selling locations by considering crucial factors such as brand reputation, pricing, and travel distance (cost).\nSeveral web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database. These spiders will automate the process of data scraping, enabling repetitive and efficient collection of data.\nShowcase the capability of the spiders with a specific crop example. The spiders will be utilized to extract data for one or more of the following six products: tomatoes (regardless of the type), carrots, green onions, potatoes, spinach, lettuce. This demonstration will effectively highlight the functionality and effectiveness of the spiders in retrieving the desired data.\nOptimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit. We will explore the factors and methodology to estimate the demand and supply.\n\n\n\n\n\nThis week’s tasks:\n\nConsolidated all scraped data from all different websites into one master file to be used for data analysis.\nData Analysis:\n\nImported necessary packages in Juypter Notebook for data analysis.\nStarted cleaning the data: Missing values, fixing datatypes\nStarted exploring the data using various visualizations.\n\n\n\n\nConsolidated all collected data in three master lists of Eggs, Bacon and Heirloom tomato respectively. This would let work separately on each product’s data set and would give us more insights about the dataset.\n\n\n\nStarted the data analysis by importing the necessary packages, we will keep adding more packages as we go along with analysis part.\n\n\n\n\n\nCleaning:\nWe found there were a lot of missing values in our data set for various columns\n\n\n\nWe dealt with missing values for filling NA like shown in the snapshot below:\n\n\n\n\n\n\n\n\nwe can analyze the relationship between the current price and the original price of the heirloom tomatoes.\nThis analysis can help you understand the pricing trends and calculate potential discounts or price differences.\nVisualize the distribution of current prices using histograms, box plots, or kernel density plots to gain insights into the pricing range and identify any outliers.\n\n\n\n\n\n\n\n\nAnalyze the weight of the heirloom tomatoes by comparing the weight in pounds and the true weight.\nThis analysis can help you determine if there are any variations in weight and assess the accuracy of weight measurements.\nCreating scatter plots or line plots to visualize the relationship between weight in pounds and true weight.\n\n\n\n\n\n\n\n\nAnalyze the different brands of heirloom tomatoes available in the dataset. Calculate the frequency of each brand to determine the most popular ones.\nCreating a bar chart and pie chart to visualize the distribution of brands and identify the market share of each brand.\n\n\n\n\n\n\n\n\nAnalyze the presence of organic and locally sourced heirloom tomatoes. Calculate the percentage of organic and local products in the dataset. Creating a bar chart or pie chart to visualize the proportion of organic and local tomatoes."
  },
  {
    "objectID": "posts/Aaron_C_Week7/Week7_WrapUp.html#several-web-scrapping-spiders-for-selected-websites-to-facilitate-the-creation-of-a-comprehensive-product-database.",
    "href": "posts/Aaron_C_Week7/Week7_WrapUp.html#several-web-scrapping-spiders-for-selected-websites-to-facilitate-the-creation-of-a-comprehensive-product-database.",
    "title": "AI/Local Food Team Week Seven Wrap Up",
    "section": "Several web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database.",
    "text": "Several web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database.\nTo facilitate the creation of a comprehensive product database, we have developed several web-scraping spiders for websites such as\n1. Fresh Thyme\n2. Hy-Vee\n3. Gateway Market\n4. New Pioneer Co-op\n5. Russ’s Market\n6. Iowa Food Hub\n7. Joia Food Farm\nThese spiders automate the data scraping process, eliminating the need for repetitive data collection and significantly increasing our efficiency. It’s important to note that this list is not exhaustive, as our web-scraping spiders can be expanded to include other websites beyond those listed here."
  },
  {
    "objectID": "posts/Aaron_C_Week7/Week7_WrapUp.html#showcase-the-capability-of-the-spiders-with-a-specific-crop-example.",
    "href": "posts/Aaron_C_Week7/Week7_WrapUp.html#showcase-the-capability-of-the-spiders-with-a-specific-crop-example.",
    "title": "AI/Local Food Team Week Seven Wrap Up",
    "section": "Showcase the capability of the spiders with a specific crop example.",
    "text": "Showcase the capability of the spiders with a specific crop example.\nIt’s worth noting that the capability of these spiders is not limited to a specific product type; they can be utilized to extract data from a wide selection of products. As an example, we have successfully demonstrated their functionality and effectiveness in retrieving data for various products, including specific crops such as tomatoes, carrots, green onions, potatoes, spinach, lettuce, and many more.\n\n\n\n\n\nMoreover, these spiders can be further enhanced to automate the data cleaning processes which runs concurrently within the spider. This approach allows us to efficiently address data quality issues, ensuring that the collected data is reliable and ready for analysis. The main objective of this enhancement is to guarantee that the collected data is thoroughly cleaned before integrating it into a file format, providing a seamless transition for further analysis and utilization. This specific example Highlights Heirloom Tomatoes from the listed stores as mention above."
  },
  {
    "objectID": "posts/Aaron_C_Week7/Week7_WrapUp.html#optimization-of-the-crop-flow-from-the-point-of-supply-to-the-point-of-demand-that-maximizes-overall-profit",
    "href": "posts/Aaron_C_Week7/Week7_WrapUp.html#optimization-of-the-crop-flow-from-the-point-of-supply-to-the-point-of-demand-that-maximizes-overall-profit",
    "title": "AI/Local Food Team Week Seven Wrap Up",
    "section": "Optimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit",
    "text": "Optimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit\n\n\n\n\n\n\nThe red points represent the counties.\nThe green lines represent the flow of crops, and the blue arrow shows the direction of the flow.\nMaximizes the revenue by selling the crops.\nMinimizes the cost of distance traveled.\nRelaxed previous assumption “Supply is greater than demand”\n\nThe following might be included in the project:\n\nA separate account of fresh and not fresh products.\nConsideration of each individual farmer’s profit.\nConsideration of the flow of trucks rather than the flow of crops."
  },
  {
    "objectID": "posts/Aaron_C_Week7/Week7_WrapUp.html#demand-and-supply-estimation",
    "href": "posts/Aaron_C_Week7/Week7_WrapUp.html#demand-and-supply-estimation",
    "title": "AI/Local Food Team Week Seven Wrap Up",
    "section": "Demand and supply estimation",
    "text": "Demand and supply estimation\n\n\n\n\n\nDisclaimer: Supply and demand estimation requires further correction, and the methodology needs improvements.\n\n\nDemand Estimation:\n\nThe population of United States is 334.9 million as of June 26, 2023. (https://www.census.gov/popclock/)\nIn 2017, fresh market consumption was 20.3 pounds per capita. (https://www.agmrc.org/commodities-products/vegetables/tomatoes)\nSo, the demand for fresh tomatoes in the United States is (334.9 million x 20.3 pounds) or 3.08 Million Metric Tons.\nThere is around 1% of the US population lives in Iowa.\nThat translates to 30.8 thousand Metric Tons of tomatoes demand in Iowa.\nConsidering the demand for tomatoes stays the same for 52 weeks. The demand for tomatoes in Iowa per week is 592 Metric Tons.\nWe will allocate this demand to each county of Iowa by population.\n\n\n\n\n\n\n\n\nSupply Estimation:\n\nThe growing season of tomatoes is between May to Mid-September in Iowa. (https://www.tomatofest.com/Tomato_Growing_Zone_Maps_s/164.htm)\nWe have 135 days between planting and harvesting tomatoes.\nTomatoes require 100 days to fully mature. However, there are some special varieties of tomatoes that require 50-60 days to mature.  (https://www.gardeningknowhow.com/edible/vegetables/tomato/planting-time-for-tomatoes.htm)\nSo, we are considering on average tomatoes take 80 days to be harvested.\nIn this context, we assumed that being mature represents the time between sowing seed and harvesting full-grown tomatoes.\nWe are only considering single cultivation of tomatoes during a year.\nSo, the tomatoes will be harvested during the timeline day 80-135 or, during late July to mid-September, that is 7.85 or 8 weeks. \nIn our calculations, we will estimate the weekly supply of tomatoes from late July to mid-September.\nIn 2020, approximately 12,619.2 tons of fresh market tomatoes were harvested from approximately 272,900 acres. (https://www.agmrc.org/commodities-products/vegetables/tomatoes)\nFlorida and California account for about two-thirds of the national fresh tomato production (Wu, F., Guan, Z., & Suh, D. H. (2017). The Effects of Tomato Suspension Agreements on Market Price Dynamics.; mentioned in https://edis.ifas.ufl.edu/publication/FE1027)\nThe remaining 48 states are responsible for one-third of the national fresh tomatoes. That is 4206.4 tons.\nTotal farming land in USA is 900.21 million acres as of 2022.\nTotal farm land in Florida is 9.73 million acres as of 2017.\nTotal farm land in California is 24.23 million acres as of 2017.\n (https://www.nass.usda.gov/AgCensus)\nRemaining 48 states have a farm land of 866.25 million acres.\nTotal farmland in Iowa is 30.56 million acres. (https://www.nass.usda.gov/AgCensus)\nConsidering tomatoes are grown uniformly in these farmlands, the proportion of farmland Iowa has compared to all 48 states (without California, Florida) is 3.52%.\nThe production of tomatoes in Iowa is (3.52% x 4206.4 tons) or, 148 tons.\nAs the production of tomatoes is distributed over 8 weeks (as mentioned in no 8), the weekly supply quantity of tomatoes is (148/8) or 18.5 tons or 16.78 Metric Tons.\nConsidering the average farm size in each county of Iowa is the same. The supply of tomatoes can be obtained from the number of farms.\n\n\n\n\n\n\nTeaser video: Video"
  },
  {
    "objectID": "posts/Aaron_C_Week8/Week8_WrapUp.html",
    "href": "posts/Aaron_C_Week8/Week8_WrapUp.html",
    "title": "AI/Local Food Team Week Eight Wrap Up",
    "section": "",
    "text": "A comprehensive map to showcase the prices of eggs and bacon across various counties using the collected data. This map serves as a valuable tool for identifying trends and patterns in pricing, as well as understanding customer preferences towards specific brands. Additionally, the map aids in the selection of suitable selling locations by considering crucial factors such as brand reputation, pricing, and travel distance (cost).\nSeveral web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database. These spiders will automate the process of data scraping, enabling repetitive and efficient collection of data.\nShowcase the capability of the spiders with a specific crop example. The spiders will be utilized to extract data for one or more of the following six products: tomatoes (regardless of the type), carrots, green onions, potatoes, spinach, lettuce. This demonstration will effectively highlight the functionality and effectiveness of the spiders in retrieving the desired data.\nOptimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit. We will explore the factors and methodology to estimate the demand and supply."
  },
  {
    "objectID": "posts/Aaron_C_Week8/Week8_WrapUp.html#first-column",
    "href": "posts/Aaron_C_Week8/Week8_WrapUp.html#first-column",
    "title": "AI/Local Food Team Week Eight Wrap Up",
    "section": "First column",
    "text": "First column\nI would like to have text here\nSentence becomes longer, it should automatically stay in their column"
  },
  {
    "objectID": "posts/Aaron_C_Week8/Week8_WrapUp.html#second-column",
    "href": "posts/Aaron_C_Week8/Week8_WrapUp.html#second-column",
    "title": "AI/Local Food Team Week Eight Wrap Up",
    "section": "Second column",
    "text": "Second column\nand here\nMore text"
  },
  {
    "objectID": "posts/Aaron_C_Week8/Week8_WrapUp.html#ai-and-local-food-team-plans-to-produce-following-outputs-at-the-end-of-this-years-project",
    "href": "posts/Aaron_C_Week8/Week8_WrapUp.html#ai-and-local-food-team-plans-to-produce-following-outputs-at-the-end-of-this-years-project",
    "title": "AI/Local Food Team Week Eight Wrap Up",
    "section": "",
    "text": "A comprehensive map to showcase the prices of eggs and bacon across various counties using the collected data. This map serves as a valuable tool for identifying trends and patterns in pricing, as well as understanding customer preferences towards specific brands. Additionally, the map aids in the selection of suitable selling locations by considering crucial factors such as brand reputation, pricing, and travel distance (cost).\nSeveral web-scrapping spiders for selected websites to facilitate the creation of a comprehensive product database. These spiders will automate the process of data scraping, enabling repetitive and efficient collection of data.\nShowcase the capability of the spiders with a specific crop example. The spiders will be utilized to extract data for one or more of the following six products: tomatoes (regardless of the type), carrots, green onions, potatoes, spinach, lettuce. This demonstration will effectively highlight the functionality and effectiveness of the spiders in retrieving the desired data.\nOptimization of the crop flow, from the point of supply to the point of demand that maximizes overall profit. We will explore the factors and methodology to estimate the demand and supply."
  },
  {
    "objectID": "posts/Aaron_C_Week8/Week8_WrapUp.html#third-column",
    "href": "posts/Aaron_C_Week8/Week8_WrapUp.html#third-column",
    "title": "AI/Local Food Team Week Eight Wrap Up",
    "section": "Third column",
    "text": "Third column\nand here\nMore text"
  },
  {
    "objectID": "posts/Aaron_C_Week8/Week8_WrapUp.html#final-presentation-flow",
    "href": "posts/Aaron_C_Week8/Week8_WrapUp.html#final-presentation-flow",
    "title": "AI/Local Food Team Week Eight Wrap Up",
    "section": "Final Presentation Flow",
    "text": "Final Presentation Flow\n\n\n\n\n\nPerson\n\n\n\n\n\nTopic\n\n\n\n\n\nTime\n\n\n\n\n\n\n\nSwati\n\n\n\n\nIntroduction\n\nWhat\nWhat to plan to achieve\nWhat we plan to achieve\nWhy is this important\n\n\n\n\n\n6 - 8\n\n\n\n\n\n\nAaron\n\n\n\n\nWeb Scrapping (Spiders)\n\nWith what we started\nHow we did scraping\nWhat we achieved at the end\nAny interesting stuff\n\n\n\n\n\n10 - 12\n\n\n\n\n\n\nSwati\n\n\n\n\nData Analysis\n\nVisualization (What graph say)\nAny predictions\nWhat model we used (if any)\n\n\n\n\n\n8 - 10\n\n\n\n\n\n\nSadat\n\n\n\n\nSupply chain opti.\n\nOutput\nHow we achieved\n\n\n\n\n\n???\n\n\n\n\n\n\nSadat / Swati\n\n\n\n\nConclusion\n\n\n\n\n2 - 3"
  }
]