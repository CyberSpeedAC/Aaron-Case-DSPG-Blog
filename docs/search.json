[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aaron Case DSPG Blog",
    "section": "",
    "text": "Spiders\n\n\n\n\n\n\n\nWeek Four\n\n\nWeek Five\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Five Spider Code\n\n\n\n\n\n\n\nWeek Five\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Four\n\n\n\n\n\n\n\nWeek Four\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Five\n\n\n\n\n\n\n\nWeek Five\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nAI/Local Food Week 3 Wrap Up\n\n\n\n\n\n\n\nWeek Three\n\n\nWrap Up\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nAaron C\n\n\n\n\n\n\n  \n\n\n\n\nWeek Three\n\n\n\n\n\n\n\nWeek Three\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Three Research\n\n\n\n\n\n\n\nWeek Three\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Three Spiders\n\n\n\n\n\n\n\nWeek Three\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek Two\n\n\n\n\n\n\n\nWeek Two\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nImporting Census Data\n\n\n\n\n\n\n\nWeek Two\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nWeek One\n\n\n\n\n\n\n\nWeek One\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAaron Case\n\n\n\n\n\n\n  \n\n\n\n\nCensus Visual\n\n\n\n\n\n\n\nWeek Two\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAaron Case\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Aaron_C_MultiWeek/Aaron_C_Spiders.html",
    "href": "posts/Aaron_C_MultiWeek/Aaron_C_Spiders.html",
    "title": "Spiders",
    "section": "",
    "text": "Web scraping in essence is the process of extracting data from websites using automated software. As for the process, the web-scraping pipeline goes as follows.\n\nSetup - Understand what we want to do and find sources to help us do it.\nAcquisition\n\nRead in the raw data from online.\nFormat these data to be usable.\nAccessing the data.\nParsing this information.\nExtracting these data into meaningful and useful data structures.\n\nProcessing - There are many options to this but the main goal is to run the downloaded data through whatever analyses or processes needed to achieve the desired goal.\n\nThere are many web scraping tools available, but two of the one’s that I’ve the most success in are Scrapy and Selenium.\nScrapy is a Python package that is primarily used for web scraping. Scrapy provides a framework that offers a simple web crawling tool to extract data from websites. These tools are commonly known as spiders and allow for easy extraction of desired information. A spider program can crawl through websites and extracts data from them (Just like real spiders crawl through there webs). Scrapy works by sending HTTP requests to websites and receiving HTML responses, then the spider parses the HTML to extract the data it needs. There is a Data camp course that is currently available to use and is highly recommended watching. These are some of benefit of using Scrapy that I’ve found:\n\nScrapy is designed to be fast and can handle large volumes of data. It’s built on top of Twisted, an asynchronous networking framework, which in simple terms makes it very fast. It can handle thousands of requests per second without slowing down. This makes it really handy for the scope of this project.\nScrapy is highly customizable and can be configured to work with many different websites and data formats. Allowing us to specify how the spider should act when navigating through websites and extract data that we told it to collect. Scrapy also provides features for handling cookies, redirects, and other HTTP features.\nScrapy comes with built-in support for XPath and CSS selectors. With the right skill set these tools provide a powerful and easy way to extract data from HTML and XML documents.\nScrapy automatically comes with built-in support for data cleaning, Making the the cleaning and normalize scraped data much easier.\n\nAs for the cons of Scrapy:\n\nScrapy does have a relatively steep learning curve to it. Especially if you are new to web scraping, XPath/CSS selectors, or asynchronous programming. Unless you have more than a week to spend learning and doing web scrapping it is best to spend the time doing another method. Not to mention the fact that you need to have some knowledge of Python to use it effectively\nScrapy does not have built-in support for JavaScript, which means that it cannot scrape websites and other dynamic web content that rely heavily on JavaScript for rendering content. Which a vast amount of websites rely on to function.\nThis tool is not perfect nor undetectable. Some websites may block Scrapy from accessing their content, especially if they detect that the traffic is coming from a bot.\nScrapy does help you in cleaning data however it is not perfect. You still need to spend some time cleaning the data that was collected and may require additional libraries, such as Pandas or NumPy, to process and analyze scraped data.\n\nSelenium is another Python package which is used for automating web browsers. It allows you to control a browser programatically, which mimics the actions of a what a user will do. Selenium uses a web driver to control your web browser and can interact with several different browsers (such as Chrome, Firefox, and Safari). Selenium works by opening up a web browser and navigates to the target website. The user can then interact with the website using Selenium commands. Just like Scrapy, Selenium can also extract data from the website using XPath and CSS selectors. As for some of benefit of using Selenium:\n\nSelenium has the ability to handle JavaScript and other dynamic web content. Allowing it to handle websites that Scrapy is unable too.\nSelenium can handle more complex interactions with websites, such as clicking buttons, filling out forms, scrolling, etc. which can simulate user behavior. Making Selenium more diverse and a power tool to use when doing specific tasks that require a more human like approach.\n\nFor the Cons of Selenium:\n\nSelenium can be really slow and very resource-intensive. Especially when automating complex tasks or interacting with multiple pages. It requires your web browser to be opened to run meaning If your internet or computer is slow it has a really hard time doing anything you want it. This time that you have to wait for the program to preform any simple task is not ideal when working through large data sets.\nIt is prone to errors and requires extensive debugging and testing. For example, if say that you close your browser or if it freezes (for long periods of time) the program will not function correctly and throw an error.\nIt can be very challenging to set up and configure. Not to mention that It can be difficult to maintain test scripts when the web application is updated or changed. Meaning if you are not dedicated in learning or maintaining your code it will come with a bunch of headaches down the road.\n\nIdeally for this project I strongly recommend using Scrapy spiders were ever possible. Scrapy’s speed and efficiency and overall better performance makes this part of the process much more tolerable in building spiders. However when it comes to more complex tasks and websites Selenium is the way to go. I would also like to acknowledge that there are other tools and resources that could be explored and used which may be better or more effenect for the scope of the project. Theses two tools are just some that I found and was able to implement in the time frame I was given. For example, there is a python package called Scrapy Splash. This works as an extension for Scrapy which provides a JavaScript rendering service. Allowing Scrapy to handle JavaScript and other dynamic web content, similar to Selenium. I wasn’t able to master this tool nor could I successfully get it this tool to work. If I had more time to develop a clear comprehension of this utility and fully understood how to get it to work. It would have be the tool of choice to handle such websites. I used Selenium since I had more success in getting it to work in comparison.\nThis code covers the bare minimum of Scrapy and is rather (hopefully) intuitive. This can be edited/modified to fit your needs and is just an example.\n# python\n# Imports for Scrapy Example\nimport scrapy #Required Import\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.log import configure_logging # For debugging\n\n# The Class name can be anything but you need to call this later\nclass SpiderClassName(scrapy.Spider):\n    name = \"spider name\" \n\n    def start_requests( self ): # This is required to start the spider\n        # For one url\n        Desired_Website = [ 'https://www.example.com' ]\n        yield scrapy.Request( url = Desired_Website, callback = self.Some_Function_Name)\n\n        # For many urls\n        urls = [ 'https://www.examples.com', ... ]\n        for url in urls:\n            yield scrapy.Request( url = url, callback = self.parseAndFollow)\n\n        # When passing in varables from one function to the next you use meta\n        for url in urls:\n            yield scrapy.Request( url = url, callback = self.Meta_Example, meta={'Meta_Name': data, ...})\n    \n    def parseAndFollow( self, response ):\n        # Direct to the links in the xpath\n        Extrated_XPath = '//tag-name(@attribute, \"attrib info\")'\n        XPath_links = Extrated_XPath.xpath( './@href' )\n        # Anoter way\n        XPath_links = Extrated_XPath.xpath('//tag-name(@attribute, \"attrib info\")/@href')\n        # Extract the links (as a list of strings)\n        links_to_follow = XPath_links.extract()\n        # Follow the links to the next parser\n        for url in links_to_follow:\n            yield response.follow( url = url, callback = self.parse )\n    \n    def parse( self, response ):\n        # To get the text of the xpath\n        Title_Example = response.xpath('//h1[contains(@class,\"title\")]/text()') \n        # Extract text title\n        Title_Extracted = Title_Example.extract_first()\n        # Clean the title text\n        Title_Text = Title_Extracted.strip()\n        # Saving the text so that we dont lose what we just did\n        Text_Element_List.append(Title_Text)\n\n    def Meta_Example( self, response ):\n        #This is how pass in your arguments\n        Meta_Data = response.meta.get('Meta_Name')\n    \nText_Element_List = []\n# To see the inner mechanics of the spider helpful to have not required\nconfigure_logging()\n# initiate a CrawlerProcess\nprocess = CrawlerProcess()\n# Tell the process which spider to use\nprocess.crawl(SpiderClassName)\n# Start the crawling process\nprocess.start()\n# This is to stop the spider\nprocess.stop()\n# At this point the spider is finished and now we have everything that was saved\nprint(Text_Element_List)\n\nAs for Selenium this code covers the bare minimum and serves as a deminstration of how to handle JavaScript websites. Once again this can be edited/modified to fit your needs and is just an example.\n# python\n# Imports for Scraping\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# Since I use Firefox this is how I would typically set it up.\n# Note: This is not the only way to do it. Its just how I do this part \nfrom selenium.webdriver.firefox.service import Service as FirefoxService\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom os import path\ndriver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n\n# Another example of how you can do it \n# Set the path to the driver\ndriver_path = \"/path/to/chromedriver\"\n# Create a new instance of the Chrome driver\ndriver = webdriver.Chrome(driver_path)\n\n# Navigate to the website page\ndriver.get(\"https://example.com/\")\n\n# Setup\nwaitTime = 10\n\n# I like using XPaths because I am most comfortable using them \nTitle_XPath = '//h1(@class,\"title\")'\n# We need to wait for the page to render before we do anything otherwise the data we want wont be present.\nelements = WebDriverWait(driver, waitTime).until(EC.visibility_of_element_located((By.XPATH, Title_XPath)))\n# You can also do it like this for CSS\nelements = WebDriverWait(driver, waitTime).until(EC.visibility_of_element_located((By.CSS_SELECTOR, Title_CSS)))\n# Extracts the text of the element\nText = elements[0].text\n\n# Close the browser\ndriver.quit()\n\n#Data Has been extracted\nprint(Text)"
  },
  {
    "objectID": "posts/Aaron_C_MultiWeek/Aaron_C_Week5Code.html",
    "href": "posts/Aaron_C_MultiWeek/Aaron_C_Week5Code.html",
    "title": "Week Five Spider Code",
    "section": "",
    "text": "This post show all the code that I’ve written for week 5. It contains 2 examples of how I find xpaths as well as seven spider/scrapers that I have made and modified.\nAn example of how I find data that the xpaths have. This example is for Russ Market Spider.\n\n# Imports for Scraping\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.service import Service as FirefoxService\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import StaleElementReferenceException\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom os import path\nimport time\n\n#Use this and edit this to extract data from the xpaths \n#Creators Note: if any major changes that are made you need to update the spiders javascriptXpath function since \n#this is the exact function that the spider uses to collect data from the url\ndef javascriptXpath (driver, xpath):\n    try: \n        # Waits for page to load \n        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n        elements = WebDriverWait(driver, waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))\n        # A set of all the outputs that we want to avoid\n        invalidOutputs = {\"error\", 'skip' \"$nan\", ''}\n        loopCount = 0\n        # Runs the javascript and collects the text data from the inputed xpath\n        # We want to keep repeating if we get '' becasue the page is still loading\n        while loopCount &lt; loopRepeat:\n            #Running the JavaScript\n            text = driver.execute_script(\"\"\"\n                const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;\n                if (!element) {\n                    return 'skip';\n                }\n                return element.textContent.trim();\n            \"\"\", xpath)\n            checkText = text.replace(\" \", \"\").lower()\n            if checkText in invalidOutputs:\n                loopCount+=1\n            else:\n                print(loopCount, \"xpath attempts for (\", text, \")\")\n                break\n        if loopCount &lt; loopRepeat:\n            print(\"Success for \" ,xpath)         \n        else: \n            return \"xpath value not found\"\n        return text\n    except TimeoutException:\n        # This means the xpath wasn't found in the page\n        print('Could not find xpath for: ', xpath)\n        return 'Empty'\n\n#Add test and fix xpaths here. Use multiple urls for testing  \ndef testXpaths(url):\n    driver.get(url)\n    nameXpath = '//*[@id=\"page-title\"]//h1[contains(@class,\"fp-page-header fp-page-title\")]'\n    print(javascriptXpath(driver, nameXpath))\n    priceXpath = '//*[@id=\"page-title\"]//*[contains(@class,\"fp-item-price\")]/span[contains(@class,\"fp-item-base-price\")]'\n    print(javascriptXpath(driver, priceXpath))\n    weightXpath = '//*[@id=\"page-title\"]//*[contains(@class,\"fp-item-price\")]/span[contains(@class,\"fp-item-size\")]' \n    print(javascriptXpath(driver, weightXpath))\n    saleXpath = '//*[@id=\"page-title\"]//*[contains(@class,\"fp-item-sale\")]/span[contains(@class,\"fp-item-sale-date\")]/strong' #optional\n    print(javascriptXpath(driver, saleXpath))\n\n# setup\nwaitTime = 10\ndriver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\nstoreLocationUrl = 'https://www.russmarket.com/shop#!/?store_id=6158'\ndriver.get(storeLocationUrl)\ntime.sleep(5)\nprint(\"Store location set\")\nloopRepeat = 100\n\n# Successful tests\nurl = \"https://www.russmarket.com/shop/produce/fresh_vegetables/tomatoes/heirloom_tomatoes/p/12412\"\ntestXpaths(url)\n# Successful tests\nurl = \"https://www.russmarket.com/shop/meat/bacon/prairie_fresh_signature_applewood_smoked_bacon_pork_loin_filet_27_2_oz/p/6828650\"\ntestXpaths(url)\n\ndriver.quit()\n\nThis is example of how I find data that the xpaths have. This example is for New Pioneer Co-op Spider.\n\n# Imports for Scraping\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.service import Service as FirefoxService\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import StaleElementReferenceException\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom os import path\nimport time\n\n#Use this and edit this to extract data from the xpaths \n#Creators Note: if any major changes that are made you need to update the spiders javascriptXpath function since \n#this is the exact function that the spider uses to collect data from the url\ndef javascriptXpath (driver, xpath):\n    try: \n        # Waits for page to load \n        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n        elements = WebDriverWait(driver, waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))\n        # A set of all the outputs that we want to avoid\n        invalidOutputs = {\"error\", 'skip' \"$nan\", ''}\n        loopCount = 0\n        # Runs the javascript and collects the text data from the inputed xpath\n        # We want to keep repeating if we get '' becasue the page is still loading\n        while loopCount &lt; loopRepeat:\n            #Running the JavaScript\n            text = driver.execute_script(\"\"\"\n                const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;\n                if (!element) {\n                    return 'skip';\n                }\n                return element.textContent.trim();\n            \"\"\", xpath)\n            checkText = text.replace(\" \", \"\").lower()\n            if checkText in invalidOutputs:\n                loopCount+=1\n            else:\n                print(loopCount, \"xpath attempts for (\", text, \")\")\n                break\n        if loopCount &lt; loopRepeat:\n            print(\"Success for \" ,xpath)         \n        else: \n            return \"xpath value not found\"\n        return text\n    except TimeoutException:\n        # This means the xpath wasn't found in the page\n        print('Could not find xpath for: ', xpath)\n        return 'Empty'\n\n#Add test and fix xpaths here. Use multiple urls for testing  \ndef testXpaths(url):\n    driver.get(url)\n    nameXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-name\")]'\n    print(javascriptXpath(driver, nameXpath))\n    priceXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-price\")]//span[contains(@class,\"fp-item-base-price\")]'\n    print(javascriptXpath(driver, priceXpath))\n    weightXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-price\")]//span[contains(@class,\"fp-item-size\")]'\n    print(javascriptXpath(driver, weightXpath))\n    saleXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-sale\")]//span[contains(@class,\"fp-item-sale-price\")]' # optional\n    print(javascriptXpath(driver, saleXpath))\n\ndriver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\nurl = \"https://shop.newpi.coop/shop/meat/bacon/sliced/applegate_natural_hickory_smoked_uncured_sunday_bacon_8_oz/p/19959#!/?department_id=1322093\"\ntestXpaths(url)\n\nurl = 'https://shop.newpi.coop/shop/just_ice_tea_green_tea_original_unsweetened_16_fl_oz/p/1564405684713463723'\ntestXpaths(url)\n\nThis is the code for the Fresh Thyme Spider.\n\nfrom datetime import datetime\nimport pandas as pd\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.log import configure_logging\n\nclass FreshThymeSpider(scrapy.Spider):\n    name = 'Fresh Thyme Market Spider'\n\n    def start_requests( self ):\n        #Bacon Scraper part\n        bacon_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage',\n                      'https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage']\n        for url in bacon_urls:\n            yield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'bacon', 'url': url})\n\n        #Egg Scraper part\n        egg_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Eggs&take=48&f=Category%3AEggs',\n                      'https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Eggs&take=48&f=Category%3AEggs']\n        for url in egg_urls:\n            yield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'egg', 'url': url})\n\n        #Heirloom Tomatoes part\n        tomato_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=heirloom%20tomatoes',\n                       'https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=heirloom%20tomatoes']\n\n        for url in tomato_urls:\n            yield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'tomato', 'url': url})\n\n    def cardsParse(self, response):\n        #Failsafe for links\n        try:\n            #grabs the store location\n            storeXpath = '//*[contains(@class,\"HeaderSubtitle\")]/text()'\n            store = response.xpath(storeXpath).extract_first()\n            #grabs all cards from list and saves the link to follow\n            xpath = '//*[contains(@class,\"Listing\")]/div/a/@href'\n            listCards = response.xpath(xpath)\n            for url in listCards:\n                yield response.follow( url = url, callback = self.itemParse, meta={'store': store, 'type': response.meta.get('type'), 'url': response.meta.get('url')} )\n        except AttributeError:\n           pass\n    \n    def itemParse(self, response):\n        #xpaths to extract \n        nameXpath = '//*[contains(@class, \"PdpInfoTitle\")]/text()'\n        priceXpath = '//*[contains(@class, \"PdpMainPrice\")]/text()'\n        prevPriceXpath = '//*[contains(@class, \"PdpPreviousPrice\")]/text()'\n        unitPriceXpath = '//*[contains(@class, \"PdpUnitPrice\")]/text()'\n        #Adding the data to data frame\n        itemType = response.meta.get('type')\n        if(itemType == \"bacon\"):\n            baconFrame.loc[len(baconFrame)] = [response.xpath(nameXpath).extract_first(),\n                                               response.xpath(priceXpath).extract_first(), \n                                               response.xpath(unitPriceXpath).extract_first(), \n                                               response.xpath(prevPriceXpath).extract_first(), \n                                               response.meta.get('store'),\n                                               response.meta.get('url')]\n        elif(itemType == \"egg\"):\n            eggFrame.loc[len(eggFrame)] = [response.xpath(nameXpath).extract_first(),\n                                           response.xpath(priceXpath).extract_first(), \n                                           response.xpath(prevPriceXpath).extract_first(), \n                                           response.meta.get('store'),\n                                           response.meta.get('url')]\n        elif(itemType == \"tomato\"):\n            tomatoFrame.loc[len(tomatoFrame)] = [response.xpath(nameXpath).extract_first(),\n                                                 response.xpath(priceXpath).extract_first(), \n                                                 response.xpath(prevPriceXpath).extract_first(), \n                                                 response.meta.get('store'),\n                                                 response.meta.get('url')]\n\n# Start\n#DEBUG Switch\nDEBUG = 0\n\n#Data frames\nbaconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Unit Price', 'Sale', 'Store Location', 'Url'])\neggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Sale', 'Store Location', 'Url'])\ntomatoFrame = pd.DataFrame(columns=['Heirloom Tomato', 'Current Price', 'Sale', 'Store Location', 'Url'])\n\nif(DEBUG):\n    #To see the inner mechanics of the spider\n    configure_logging()\n\n#This is to start the spider\nprocess = CrawlerProcess()\nprocess.crawl(FreshThymeSpider)\nprocess.start()\nprocess.stop()\n\nif(DEBUG):\n    #To see the outputs\n    print(baconFrame)\n    print(eggFrame)\n    print(tomatoFrame)\n\n#Adds the date that the data was scraped\ncurrentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n#To CSV files\nbaconFrame.to_csv(currentDate + \"Fresh Thyme Bacon.csv\")\neggFrame.to_csv(currentDate + \"Fresh Thyme Egg.csv\")\ntomatoFrame.to_csv(currentDate + \"Fresh Thyme Heirloom Tomatoes.csv\")\n\nThis is the code for the Iowa Food Hub Spider.\n\nfrom datetime import datetime\nimport pandas as pd\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.log import configure_logging\n\nclass IowaFoodHubSpider(scrapy.Spider):\n    name = 'Iowa Food Hub'\n    currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n    def start_requests( self ):\n\n        iowaFoodHubBaconUrl = 'https://iowa-food-hub.myshopify.com/search?q=bacon'\n        yield scrapy.Request( url = iowaFoodHubBaconUrl, callback = self.iowaFoodHubSearch, meta={'url': iowaFoodHubBaconUrl, 'type': 'bacon'})\n\n        iowaFoodHubEggsUrl = 'https://iowa-food-hub.myshopify.com/search?q=Egg'\n        yield scrapy.Request( url = iowaFoodHubEggsUrl, callback = self.iowaFoodHubSearch, meta={'url': iowaFoodHubEggsUrl, 'type': 'eggs'})\n\n    def iowaFoodHubSearch(self, response):\n        #Failsafe for links\n        try:\n            #grabs all cards from list and saves the link to follow\n            xpath = '//*[@id=\"MainContent\"]//a[contains(@class,\"list-view-item\")]/@href'\n            linkList = response.xpath(xpath)\n            productType = response.meta.get('type')\n            if productType == 'bacon':\n                for url in linkList:\n                    yield response.follow( url = url, callback = self.iowaFoodHubBacon, meta={'url': response.meta.get('url')}, dont_filter=True )\n            elif productType == 'eggs':\n                for url in linkList:\n                    yield response.follow( url = url, callback = self.iowaFoodHubEggs, meta={'url': response.meta.get('url')}, dont_filter=True )\n        except AttributeError:\n           pass\n\n    def iowaFoodHubBacon(self, response):\n        #validating the name\n        nameXpath = '//*[@id=\"ProductSection-product-template\"]//*[contains(@class, \"product-single__title\")]/text()'\n        name = response.xpath(nameXpath).extract_first()\n        desiredNames = {\"bacon\"}\n        if not self.containsWord(name, desiredNames):\n            return \n        #The other areas we are interested in\n        venderXpath = '//*[@id=\"ProductSection-product-template\"]//*[contains(@class, \"product-single__vendor\")]/text()'\n        priceXpath = '//*[@id=\"ProductPrice-product-template\"]/text()'\n        \n        #getting the product discription\n        discXpath = '//*[@id=\"ProductSection-product-template\"]//*[contains(@class, \"product-single__description\") and @itemprop=\"description\"]/*'\n        description = response.xpath(discXpath)\n        descriptionText = ''\n        for text in description:\n            descriptionText += \"\".join(text.xpath('.//text()').extract_first().strip())\n        \n        #Adding product to data frame    \n        IowaFoodHubBaconDataFrame.loc[len(IowaFoodHubBaconDataFrame)] = [name,\n                                                                         response.xpath(venderXpath).extract_first(), \n                                                                         response.xpath(priceXpath).extract_first(), \n                                                                         descriptionText,                                       \n                                                                         self.currentDate,\n                                                                         response.meta.get('url')\n                                                                        ]\n                \n    def iowaFoodHubEggs(self, response):\n        #validating the name\n        nameXpath = '//*[@id=\"ProductSection-product-template\"]//*[contains(@class, \"product-single__title\") and @itemprop=\"name\"]/text()'       \n        name = response.xpath(nameXpath).extract_first()\n        desiredNames = {\"egg\"}\n        if not self.containsWord(name, desiredNames):\n            return \n        #The other areas we are interested in\n        venderXpath = '//*[@id=\"ProductSection-product-template\"]//*[contains(@class, \"product-single__vendor\") and @itemprop=\"brand\"]/text()'\n        priceXpath = '//*[@id=\"ProductPrice-product-template\" and @itemprop=\"price\"]/text()'\n\n        #getting the product discription\n        discXpath = '//*[@id=\"ProductSection-product-template\"]//*[contains(@class, \"product-single__description\") and @itemprop=\"description\"]/*'\n        description = response.xpath(discXpath)\n        descriptionText = ''\n        for text in description:\n            descriptionText += \"\".join(text.xpath('.//text()').extract_first()).strip()\n        \n        #Adding product to data frame\n        IowaFoodHubEggDataFrame.loc[len(IowaFoodHubEggDataFrame)] = [name,\n                                                                     response.xpath(venderXpath).extract_first(), \n                                                                     response.xpath(priceXpath).extract_first(), \n                                                                     descriptionText,                                       \n                                                                     self.currentDate,\n                                                                     response.meta.get('url')\n                                                                    ]\n    def containsWord(self, string, validWords):\n        checkText = string.replace(\" \", \"\").lower()\n        for word in validWords:\n            if word in checkText:\n                return True\n        return False\n\nDEBUG = False\n#Data frames\nIowaFoodHubBaconDataFrame = pd.DataFrame(columns=['Bacon', 'Vender', 'Price', 'Weight', 'Extraction Date', 'Url'])\nIowaFoodHubEggDataFrame = pd.DataFrame(columns=['Eggs', 'Vender', 'Price', 'Amount', 'Extraction Date', 'Url'])\nif(DEBUG):\n    #To see the inner mechanics of the spider\n    configure_logging()\n\n#This is to start the spider\nprocess = CrawlerProcess()\nprocess.crawl(IowaFoodHubSpider)\nprocess.start()\nprocess.stop()\ncurrentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n\nIowaFoodHubBaconDataFrame.to_csv(currentDate + \"Iowa Food Hub Bacon.csv\")\nIowaFoodHubEggDataFrame.to_csv(currentDate + \"Iowa Food Hub Eggs.csv\")\n\nif(DEBUG):\n    #To see the outputs\n    print(IowaFoodHubBaconDataFrame)\n    print(IowaFoodHubEggDataFrame)\n\nThis is the code for the Joia Food Farm Spider.\n\nfrom datetime import datetime\nimport pandas as pd\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.log import configure_logging\n\n# Note this spider does not go through all the products only the provided links\n\nclass JoiaFoodFarmSpider(scrapy.Spider):\n    name = 'Joia Food Farm'\n    currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n    def start_requests( self ):\n        #Bacon Scraper part\n         \n        JoiaFoodFarmBaconUrls = ['https://www.joiafoodfarm.com/farmstore/bacon',\n                                 'https://www.joiafoodfarm.com/farmstore/jowl-bacon',\n                                 'https://www.joiafoodfarm.com/farmstore/danish-bacon',\n                                 'https://www.joiafoodfarm.com/farmstore/cottage-bacon'\n                                ]\n        for url in JoiaFoodFarmBaconUrls:\n            yield scrapy.Request( url = url, callback = self.JoiaFoodFarmBacon, meta={'url': url})\n        \n        JoiaFoodFarmEggsUrls = 'https://www.joiafoodfarm.com/farmstore/1mln8u54udaby8zgy3me268ekeos07'\n        yield scrapy.Request( url = JoiaFoodFarmEggsUrls, callback = self.JoiaFoodFarmEggs, meta={'url': JoiaFoodFarmEggsUrls})\n\n\n    def JoiaFoodFarmBacon(self, response):\n        nameXpath = '//*[contains(@class, \"ProductItem-summary\")]//h1[contains(@class, \"ProductItem-details-title\")]/text()'\n        priceXpath = '//*[contains(@class, \"ProductItem-summary\")]//*[contains(@class, \"product-price\")]/text()'        \n        Xlist = ['//*[contains(@class, \"ProductItem-summary\")]//*[contains(@class, \"ProductItem-details-excerpt\")]/p[2]/em/text()', #p2\n                 '//*[contains(@class, \"ProductItem-summary\")]//*[contains(@class, \"ProductItem-details-excerpt\")]/p[3]/em/text()', #p3\n                 '//*[contains(@class, \"ProductItem-summary\")]//*[contains(@class, \"ProductItem-details-excerpt\")]/p[4]/em/text()'  #p4\n                ]\n        data = [response.xpath(xpath).extract_first() for xpath in Xlist if response.xpath(xpath).extract_first() is not None]\n        data = self.fill_list(data, 2)        \n        JoiaFoodFarmBaconDataFrame.loc[len(JoiaFoodFarmBaconDataFrame)] = [response.xpath(nameXpath).extract_first(),\n                                                                           response.xpath(priceXpath).extract_first(), \n                                                                           data[0], \n                                                                           data[1], \n                                                                           \"2038 March Avenue, Charles City, IA, 50616\",\n                                                                           self.currentDate,\n                                                                           response.meta.get('url')]\n\n    def JoiaFoodFarmEggs(self, response):\n        nameXpath = '//*[contains(@class, \"ProductItem-summary\")]//h1[contains(@class, \"ProductItem-details-title\")]/text()'\n        priceXpath = '//*[contains(@class, \"ProductItem-summary\")]//*[contains(@class, \"product-price\")]/text()'\n        JoiaFoodFarmEggsDataFrame.loc[len(JoiaFoodFarmEggsDataFrame)] = [response.xpath(nameXpath).extract_first(),\n                                                                         response.xpath(priceXpath).extract_first(), \n                                                                         \"2038 March Avenue, Charles City, IA, 50616\",\n                                                                         self.currentDate,\n                                                                         response.meta.get('url')]\n\n    def fill_list(self, lst, length):\n        if len(lst) &lt; length:\n            lst += [None] * (length - len(lst))\n        return lst\n\n\nDEBUG = False\n#Data frames\nJoiaFoodFarmBaconDataFrame = pd.DataFrame(columns=['Bacon', 'Price', 'Unit Price', 'weight', 'Store Location', 'Url', 'extraction date'])\nJoiaFoodFarmEggsDataFrame = pd.DataFrame(columns=['Eggs', 'Price', 'Store Location', 'Url', 'extraction date'])\ncurrentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n\nif(DEBUG):\n    #To see the inner mechanics of the spider\n    configure_logging()\n\n#This is to start the spider\nprocess = CrawlerProcess()\nprocess.crawl(JoiaFoodFarmSpider)\nprocess.start()\nprocess.stop()\ncurrentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n\nif(DEBUG):\n    #To see the inner mechanics of the spider\n    configure_logging()\n\nJoiaFoodFarmBaconDataFrame.to_csv(currentDate + \"Joia Food Farm Bacon.csv\")\nJoiaFoodFarmEggsDataFrame.to_csv(currentDate + \"Joia Food Farm Eggs.csv\")\n\nif(DEBUG):\n    #To see the outputs\n    print(JoiaFoodFarmBaconDataFrame)\n    print(JoiaFoodFarmEggsDataFrame)\n\nThis is the code for the Gateway Market Scraper\n\n#Imports\nfrom datetime import datetime\nimport pandas as pd\nfrom enum import Enum\n#Imports for Scraping\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.service import Service as FirefoxService\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import StaleElementReferenceException\nfrom selenium.common.exceptions import WebDriverException\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom os import path\nimport time\nimport sys\n\n\n#Creator's Note: Products(Enum) and ProductsLoader is probably the only classes you need to edit \n#unless you need to change the way the data is cleaned. Which handled in the DataCleaner class\n\n#These class is here so that we can expand to differnet products easier making the spider more dynamic and expandable\nclass Products(Enum):\n    #Add products like this ProductName = index iteration, [], [] \n    #the 2 empty list will be filled in using the ProductsLoader class\n    Bacon = 0, [], []\n    Eggs = 1, [], []\n    HeirloomTomatoes = 2, [], []\n\n    # Helper method to reduce code for adding to the products and weed out duplicate inputs\n    # if you type something in really wrong code will stop the setup is important \n    # correct index inputs are correct index number, url, urls, xpath, xpaths\n    def addToProduct(self, items, index):\n        product = None\n        if isinstance(index, int):\n            product = self.value[index]\n        elif isinstance(index, str):\n            if index.lower() in ['urls', 'url']:\n                product = self.value[1]\n            elif index.lower() in ['xpaths', 'xpath']:\n                product = self.value[2]\n        if product == None:\n            raise ValueError(f\"Invalid index input for ({index}) for input: {items}\")\n        #Sets are fast at finding dups so we use them for speed\n        product_set = set(product)\n        for item in items:\n            if item not in product_set:\n                product.append(item)\n                product_set.add(item)\n\n#this class loads the xpaths and urls to the Products Enum and adds dataframes to the spider\nclass ProductsLoader():\n    DataFrames = []\n    def __init__(self):\n        self.dataFrameAdder()\n        self.urlsAdder()\n        self.xpathMaker()\n\n    #This adds the dataframe to the spider on load\n    def dataFrameAdder(self):\n        #Dataframes (You can add more here)\n\n\n        baconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Orignal Price', 'Weight in lbs', 'True Weight', 'Brand', 'Local', 'Address', 'State', 'City', 'Zip Code', 'Date Collected', 'Url'])\n        eggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Orignal Price', 'Amount in dz', 'True Amount', 'Brand', 'Local', 'Address', 'State', 'City', 'Zip Code', 'Date Collected', 'Url'])\n        tomatoFrame = pd.DataFrame(columns=['Heirloom Tomatoes', 'Current Price', 'Orignal Price', 'Weight in lbs', 'True Weight', 'Brand', 'Organic', 'Local', 'Address', 'State', 'City', 'Zip Code', 'Date Collected', 'Url'])\n\n        self.DataFrames = [baconFrame,\n                           eggFrame,\n                           tomatoFrame\n                          ]\n\n    #Adding Urls to products\n    def urlsAdder(self):\n        BaconUrls = ['https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18483',\n                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18485',\n                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-24190',\n                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18553',\n                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-33732',\n                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18521',\n                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18548',\n                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18469',\n                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-33734',\n                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-33736',\n                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-33731',\n                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-29349',\n                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18524'\n                    ]\n        EggUrls = ['https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-22775',\n                   'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-22776',\n                   'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-12603',\n                  ]\n        HeirloomTomatoesUrls = ['https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-11820',\n                                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-22455',\n                                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-11896',\n                                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-11973',\n                                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-22343',\n                               ]\n        Products.Bacon.addToProduct(BaconUrls,'urls')\n        Products.Eggs.addToProduct(EggUrls,'urls')\n        Products.HeirloomTomatoes.addToProduct(HeirloomTomatoesUrls,'urls')\n\n    #This handles the xpaths by adding to the Products class\n    #most websites have simular xpaths for each item. You might need to make differnet xpaths for each item \n    #if that is the case\n    #For assigning xpaths mark them if they are optional meaning it could or could not be present on the page \n    #we do this for speed up if you mark it as non optional and its not pressent it will skip the value \n    #and hurt the preformence\n    #best practice is to render the optional last so it reduces the chances of skipping \n    #Note spiecal cases do happen but they are extremely rare a good indiaction of finding one \n    #is by using skipHandler method and tracking/watching the logs  \n    #IMPORTANT &lt; -!- NOT ALL XPATHS ARE THE SAME FOR EACH PRODUCT -!-&gt;\n    def xpathMaker(self):\n        #Add the xpaths here and mark if they are optional\n        #Format [xpath, optional, speical]\n        nameXpath = '//*[@id=\"item-details\"]/h1[contains(@class, \"name\")]'\n        priceXpath = '//*[@id=\"item-details\"]//*[contains(@class, \"wc-pricing\")]//*[contains(@aria-describedby, \"priceDesc\")]'\n        prevPriceXpath = '//*[@id=\"item-details\"]//*[contains(@class, \"wc-pricing\")]/div[contains(@class, \"text-muted\")]/s' # optional\n        brandXpath = '//*[@id=\"item-details\"]/div[1]' # optional \n        weightXpath = '//*[@id=\"item-details\"]//*[contains(@class, \"wc-sold-by-avg-weight\")]'# optional\n        sizeXpath = '//*[@id=\"item-details\"]//*[contains(@class, \"card-body\")]//*[@class=\"size\"]'\n        #xpath, Optional\n        xpathList = [(nameXpath, False),\n                     (priceXpath, False),\n                     (prevPriceXpath, True),\n                     (brandXpath, True)\n                     (sizeXpath,True)]\n        Products.Bacon.addToProduct(xpathList,'xpath')\n        Products.Eggs.addToProduct(xpathList,'xpath')\n        xpathList = [(nameXpath, False),\n                    (priceXpath, False),\n                    (prevPriceXpath, True),\n                    (brandXpath, True, True),\n                    (weightXpath, True),\n                    (sizeXpath,True)]\n        Products.HeirloomTomatoes.addToProduct(xpathList,'xpath')\n\n\nclass DataCleaner():\n    DataArray = []\n\n\n    def cleanUp(self, item, productName, url):\n        self.DataArray = item\n        self.DataArray.append(\"2002 Woodland Avenue Des Moines, IA 50312\")\n        self.DataArray.append(url)\n        return self.DataArray\n    \n\nclass GatewaySpider():\n    name = \"Gateway Market\" #The store name \n    spiderLogs = []         #The logs \n    skipped = []            #Skipped data \n\n    #These are methods that are available for your convences\n    def log(self, *args):\n        self.spiderLogs.append(('Logger:', args))\n        if self.LOGGER:\n            print('Logger:', *args)\n\n    def debug(self, *args):\n        self.spiderLogs.append(('Debug:', args))\n        if self.DEBUGGER:\n            print('Debug:', *args)\n    \n    def printer(self, *args):\n        self.spiderLogs.append(('Printer:', args))\n        print(*args)\n    \n    def printLogs(self):\n        print(\"\\n&lt; --- Printing Logs --- &gt;\\n\")\n        for entry in self.spiderLogs:\n            print(*entry)\n\n    def Logs_to_file(self, filename):\n        with open(filename, 'w') as file:\n            for log_entry in self.spiderLogs:\n                file.write('{} {}\\n'.format(log_entry[0], log_entry[1]))\n    \n    def __init__(self):\n        self.DEBUGGER = False #The debugger switch to see whats going on. The Default is False\n        self.LOGGER = False #When you need to see everything that happends. The Default is False\n        self.attempts = 3 #The number of attempts the spider can retry if an error occurs. Default is 3\n        self.waitTime = 10 #The number of seconds WebDriver will wait. Default is 10\n        self.count = 0 #This saves the location of the url we are going through\n        self.runTime = 0 #Total time of extractions\n        self.totalRecoveries = 0 #Number of recoveries made while running\n        self.maxRetryCount = 100 #Number of retrys the javascript can make Defualt is 100\n        self.cleaner = DataCleaner() #Loads the cleaner\n        #Selenium needs a webdriver to work. I chose Firefox however you can do another if you need too\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n        self.log(\"Driver started\")\n    \n    #This handles the restart in case we run into an error\n    def restart(self):\n        self.driver.quit()\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n        self.log(\"Driver restarted\")\n    \n    #This starts the spider\n    def start_requests( self ):\n        self.runTime = time.time()\n        self.log(\"Loading from ProductsLoader Class\")\n        load = ProductsLoader() #Loads all products\n        self.dataFrames = load.DataFrames #Adds all dataframes\n        self.debug(\"Products Loaded and Data Frames Added\")\n        self.debug('\\n &lt; --- Setup runtime is %s seconds --- &gt;' % (time.time() - self.runTime))\n        self.totalRecoveries = 0 \n        #Sweeps through all products\n        for product in (Products):\n            result = self.requestExtraction(product)\n        #Adds the date that the data was scraped\n        currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n        self.log(\"Exporting files\")\n        #Dataframes to CSV files\n        for df, product in zip(self.dataFrames, (Products)):\n            df.to_csv(currentDate + self.name +\" \" + product.name + \".csv\")\n            self.log('\\n', df.to_string())\n        self.debug('\\n &lt; --- Total runtime took %s seconds with %d recoveries --- &gt;' % (time.time() - self.runTime, self.totalRecoveries))\n        if len(self.skipped) != 0:\n            self.debug('\\n &lt; -!- WARNING SKIPPED (' + str(len(self.skipped)) + ') DATA FOUND ---&gt;')\n        self.Logs_to_file(currentDate + self.name + ' Spider Logs.txt')\n        if len(self.skipped) &gt; 0:\n            self.debug(self.skipped)\n            self.skipHandler(currentDate)      \n        self.driver.quit()\n\n    #This handles the extraction request for the inputed product \n    def requestExtraction(self, product):\n        self.count = 0\n        errors = 0\n        start = time.time()\n        self.debug(\"Starting \"+ product.name)    \n        for trying in range(self.attempts):\n            try:\n                self.makeRequest(product)\n                self.debug(product.name + \" Finished\")    \n                self.log('\\n&lt; --- ' + product.name + ' scrape took %s seconds with %d recoveries --- &gt;\\n' % ((time.time() - start), errors))\n                self.totalRecoveries += errors\n                return self.totalRecoveries\n            except Exception as e:\n                #Note sometimes the browser will closed unexpectedly and theres not we can do but restart the driver\n                errors += 1\n                self.debug(\"An error occurred:\", e)\n                self.debug(\"Recovering extraction and continueing\")\n                self.restart() \n        self.debug(product.name + \" Did not Finished after \" + str(self.attempts) + \" Time wasted: %s seconds\" % (time.time() - start))\n        self.totalRecoveries += errors\n        return self.totalRecoveries\n\n    #This handles the reqests for each url and adds the data to the dataframe\n    def makeRequest(self, product):\n        productUrls = product.value[1]\n        total = len(productUrls)\n        while self.count &lt; total:\n            url = productUrls[self.count]\n            self.driver.get(url)\n            self.log(\"Making a request for: \", url)\n            item = []\n            time.sleep(1) # marionette Error Fix\n            for xpath in product.value[2]:\n                #Retrying the xpath given the number of attempts\n                for attempt in range(self.attempts):\n                    data = self.javascriptXpath(xpath[0])\n                    if data in {'empty', 'skip'}:\n                        #speical case in case you need it\n                        if len(xpath) == 3:\n                            if xpath[2]:\n                                #example would be when there is actually is a '' in the xpath\n                                self.debug(\"xpath marked as speical\")\n                                item.append(None)\n                                data = 'speical'\n                                break\n                        if xpath[1] and data == 'empty':    \n                            #this is where setting the xpath to optional comes in\n                            self.debug(\"xpath wasnt avaliable\")\n                            item.append(None)\n                            break\n                        self.debug(\"Missing item retrying\")\n                    else:  #Data found\n                        item.append(data)\n                        self.log(data + ' was added to the list for: ', url)\n                        break\n                if attempt == self.attempts:\n                    data = 'skip'\n                if data == 'skip':  #To help clean the data we skip the item with gaps of data \n                    self.debug(\"An Item has been skipped for: \", url)  \n                    item = ['SKIPPED']\n                    #Taking the product name  dataframe number and index added as well as the url \n                    #to retry for later \n                    #This could take time to do so we do this at the very end after we made the cvs files\n                    self.skipped.append([product, self.count, url])\n                    break\n            if 'SKIPPED' in item:\n                #No point in cleaning skipped items\n                items = ['SKIPPED']*(self.dataFrames[product.value[0]].shape[1] - 1)\n                items.append(url)\n            else:\n                #We call the DataCleaner class to handle the cleaning of the data\n                #Its best to clean the data before we add it to the data frame\n                self.debug('Data cleaning started: ', item)\n                items = self.cleaner.cleanUp(item, product.name, url)\n                self.debug('Data cleaning finished: ', items)\n                if items == None:\n                        self.printer(\"Data cleaner not configured to \", product.name)\n            self.debug('Extracted: ', items)\n            self.dataFrames[product.value[0]].loc[len(self.dataFrames[product.value[0]])] = items                    \n            self.count += 1\n            self.printer(product.name + \" item added \", self.count, \" of \", total, \":  \", items)\n\n    #Collecting the data from the xpath in JavaScript is faster and results in fewer errors than doing it in python\n    #This is where selenium shines because we can both use JavaScript and render JavaScript websites\n    #and is the only reason why we use it instead of scrapy\n    def javascriptXpath(self, xpath):\n        # if the time expires it assumes xpath wasnt found in the page\n        try: \n            #Waits for page to load \n            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n            elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))\n\n            # Runs the javascript and collects the text data from the inputed xpath\n            # We want to keep repeating if we get any of these outputs becasue the page is still \n            # loading and we dont want to skip or waste time. (for fast computers)\n            retrycount = 0\n            invalidOutputs = {\"error\", 'skip' \"$nan\", ''}\n            while retrycount &lt; self.maxRetryCount :\n                text = self.driver.execute_script(\"\"\"\n                    const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;\n                    if (!element) {\n                        return 'skip';\n                    }\n                    return element.textContent.trim();\n                \"\"\", \n                xpath)\n                checkText = text.replace(\" \", \"\").lower()\n                if checkText in invalidOutputs:\n                    retrycount+=1\n                else:\n                    self.log(retrycount, \"xpath attempts for (\", text, \")\")\n                    return text\n            self.log(\"xpath attempts count met. Problematic text (\" + text + \") for \", xpath)\n            return 'skip'\n        except TimeoutException:\n            self.log('Could not find xpath for: ', xpath)\n            return 'empty'\n\n           \n\n    #This is here to hopefully fix skipped data\n    #Best case sinarios this will never be used\n    def skipHandler(self, currentDate):\n        corrections = 0\n        # skipped format\n        # [product name, DataFrame number, DataFrame index, url]\n        while len(self.skipped) != 0:\n            #each skip \n            for index, dataSkip in enumerate(self.skipped):\n                product = dataSkip[0]\n                #Limiting the Attempts to fix while avoiding bottlenecking the problem\n                for attempt in range(self.attempts*2):\n                    product = dataSkip[0]\n                    url = dataSkip[2]\n                    self.driver.get(url)\n                    self.log(\"Making a request for: \", url)\n                    item = []\n                    for xpath in product.value[2]:\n                        for attemptIn in range(self.attempts*2):\n                            data = self.javascriptXpath(xpath[0])\n                            if data in {'empty', 'skip'}:   \n                                if xpath[1] and data == 'empty':    \n                                    #this is where setting the xpath to optional comes in\n                                    self.debug(\"xpath wasnt avaliable\")\n                                    item.append(None)\n                                    break\n                                self.debug(\"Missing item retrying\")\n                            else:  #Data found\n                                item.append(data)\n                                self.log(data + ' was added to the list for: ', url)\n                                break\n                        if attemptIn == self.attempts*2:\n                            data = 'skip'\n                            break\n                if data == 'skip':  #To help clean the data we skip the item with gaps of data \n                    self.debug(\"Item still missing attempting other skipped for now\") \n                else:\n                    items = self.cleaner.cleanUp(item, product.name, url)\n                    if items == None:\n                        self.printer(\"Data cleaner not configured to \", product.name)\n                    self.dataFrames[dataSkip[1]].loc[dataSkip[2]] = items                    \n                    self.printer(\"Fixed \" + product.name + \" item: \", items)\n                    #To avoid infinite loops and never saving our data we save the file now\n                    self.dataFrames[product.value[0]].to_csv(currentDate + \"REPAIRED Gateway Market \" + product.name + \".csv\")\n                    self.debug('\\n &lt; --- Total runtime with saving of repairs took %s seconds --- &gt;' % (time.time() - self.runTime))\n                    self.Logs_to_file(currentDate + self.name + ' Spider REPAIR Logs.txt')\n                    #To avoid fixing fixed items we pop, mark, and break\n                    self.skipped.pop(index)\n                    corrections += 1\n                    break\n        self.debug('\\n &lt; --- Total runtime with all repairs took %s seconds --- &gt;' % (time.time() - self.runTime))\n        self.Logs_to_file(currentDate + self.name + ' spider COMPLETED REPAIR Logs.txt')\n        \n# Start\n#DEBUG Switch\nSHOW = True\n\n#Spider setup\nspider = GatewaySpider()\nspider.LOGGER = True\nspider.DEBUGGER = True\n\n#Running the spider\nspider.start_requests()\n\nif(SHOW):\n    [print(dataFrame) for dataFrame in spider.dataFrames]\n    spider.printLogs()\n\nThis is the code for the HyVee Scraper\n\n#Imports\nfrom datetime import datetime\nimport pandas as pd\nfrom enum import Enum\n#Imports for Scraping\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.service import Service as FirefoxService\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import StaleElementReferenceException\nfrom selenium.common.exceptions import WebDriverException\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom os import path\nimport time\nimport sys\n\n\n#Creator's Note: Products(Enum) and ProductsLoader is probably the only classes you need to edit \n#unless you need to change the way the data is cleaned. Which handled in the DataCleaner class\n\n#These class is here so that we can expand to differnet products easier making the spider more dynamic and expandable\nclass Products(Enum):\n    #Add products like this ProductName = index iteration, [], [] \n    #the 2 empty list will be filled in using the ProductsLoader class\n    Bacon = 0, [], []\n    Eggs = 1, [], []\n    HeirloomTomatoes = 2, [], []\n\n    # Helper method to reduce code for adding to the products and weed out duplicate inputs\n    # if you type something in really wrong code will stop the setup is important \n    # correct index inputs are correct index number, url, urls, xpath, xpaths\n    def addToProduct(self, items, index):\n        product = None\n        if isinstance(index, int):\n            product = self.value[index]\n        elif isinstance(index, str):\n            if index.lower() in ['urls', 'url']:\n                product = self.value[1]\n            elif index.lower() in ['xpaths', 'xpath']:\n                product = self.value[2]\n        if product == None:\n            raise ValueError(f\"Invalid index input for ({index}) for input: {items}\")\n        #Sets are fast at finding dups so we use them for speed\n        product_set = set(product)\n        for item in items:\n            if item not in product_set:\n                product.append(item)\n                product_set.add(item)\n\n#This class loads the xpaths and urls to the Products Enum and adds dataframes to the spider\nclass ProductsLoader():\n    DataFrames = []\n    def __init__(self):\n        self.dataFrameAdder()\n        self.urlsAdder()\n        self.xpathMaker()\n\n    #This adds the dataframe to the spider on load\n    def dataFrameAdder(self):\n        #Dataframes (You can add more here)\n        baconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Sale', 'Weight', 'Url'])\n        eggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Sale', 'Amount', 'Url'])\n        tomatoFrame = pd.DataFrame(columns=['Heirloom Tomato', 'Current Price', 'Sale', 'Weight', 'Url'])\n        self.DataFrames = [baconFrame,\n                           eggFrame,\n                           tomatoFrame\n                          ]\n\n    #Adding Urls to products\n    def urlsAdder(self):\n        BaconUrls = ['https://www.hy-vee.com/aisles-online/p/11315/Hormel-Black-Label-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/47128/Hormel-Black-Label-Fully-Cooked-Original-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/41626/Applegate-Naturals-Uncured-Sunday-Bacon-Hickory-Smoked',\n                     'https://www.hy-vee.com/aisles-online/p/57278/HyVee-Double-Smoked-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2405550/Applegate-Naturals-No-Sugar-Uncured-Bacon-Hickory-Smoked',\n                     'https://www.hy-vee.com/aisles-online/p/57279/HyVee-Sweet-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/11366/Hormel-Black-Label-Original-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2455081/Jimmy-Dean-Premium-Hickory-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/3595492/Farmland-Bacon-Double-Smoked-Double-Thick-Cut',\n                     'https://www.hy-vee.com/aisles-online/p/47117/Hormel-Black-Label-Center-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/57277/HyVee-Center-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2199424/Country-Smokehouse-Thick-Applewood-Slab-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/77228/Hormel-Black-Label-Original-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21239/Farmland-Naturally-Hickory-Smoked-Classic-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2456254/Jimmy-Dean-Premium-Applewood-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21240/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/47159/Hormel-Black-Label-Original-Bacon-4Pk',\n                     'https://www.hy-vee.com/aisles-online/p/50315/Oscar-Mayer-Naturally-Hardwood-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/50321/Oscar-Mayer-Center-Cut-Original-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/50316/Oscar-Mayer-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2199421/Country-Smokehouse-Thick-Hickory-Smoked-Slab-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/104466/Hickory-Country-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/23975/HyVee-Hickory-House-Applewood-Naturally-Smoked-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/23949/HyVee-Sweet-Smoked-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/23963/HyVee-Fully-Cooked-Hickory-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/11173/Hormel-Black-Label-Applewood-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21317/Farmland-Naturally-Applewood-Smoked-Classic-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21238/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon-Package',\n                     'https://www.hy-vee.com/aisles-online/p/23948/HyVee-Lower-Sodium-Sweet-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/458259/Wright-Naturally-Hickory-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/11384/Hormel-Natural-Choice-Uncured-Original-Bacon-12-oz',\n                     'https://www.hy-vee.com/aisles-online/p/2476490/Jimmy-Dean-FC-Hickory-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/1646677/Smithfield-Hometown-Original-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/53849/Farmland-Naturally-Hickory-Smoked-Lower-Sodium-Classic-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/47121/Hormel-Black-Label-Maple-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/164627/Oscar-Mayer-Fully-Cooked-Original-Bacon-252-oz-Box',\n                     'https://www.hy-vee.com/aisles-online/p/23974/HyVee-Hickory-House-Hickory-Smoked-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/50319/Oscar-Mayer-Selects-Smoked-Uncured-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2471760/Jimmy-Dean-FC-Applewood-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/16239/Oscar-Mayer-Center-Cut-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2214511/Hormel-Black-Label-Original-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/1008152/Wright-Naturally-Smoked-Applewood-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/1813260/Smithfield-Naturally-Hickory-Smoked-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/23976/HyVee-Hickory-House-Peppered-Naturally-Smoked-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21320/Farmland-Naturally-Applewood-Smoked-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21253/Farmland-Naturally-Hickory-Smoked-Extra-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/1255920/Hormel-Black-Label-Cherrywood-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/57304/HyVee-Blue-Ribbon-Maple-Naturally-Smoked-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21252/Farmland-Naturally-Hickory-Smoked-30-Less-Fat-Center-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2501872/Bourbon-And-Brown-Sugar-Slab-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2516586/Hormel-Natural-ChoiceOriginal-Thick-Cut-Uncured-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21319/Farmland-Naturally-Hickory-Smoked-Double-Smoked-Classic-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/317829/Des-Moines-Bacon-And-Meat-Company-Hardwood-Smoked-Uncured-Country-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/1255919/Hormel-Black-Label-Jalapeno-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/3538865/Oscar-Mayer-Bacon-Thick-Cut-Applewood',\n                     'https://www.hy-vee.com/aisles-online/p/317830/Des-Moines-Bacon-And-Meat-Company-Applewood-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/3308731/Oscar-Mayer-Natural-Fully-Cooked-Uncured-Bacon'\n                    ]\n        EggUrls = ['https://www.hy-vee.com/aisles-online/p/57236/HyVee-Grade-A-Large-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/23899/HyVee-Grade-A-Large-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/715446/Farmers-Hen-House-Free-Range-Organic-Large-Brown-Grade-A-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/2849570/Thats-Smart-Large-Shell-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/31351/Farmers-Hen-House-Free-Range-Grade-A-Large-Brown-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/23900/HyVee-Grade-A-Extra-Large-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/71297/Egglands-Best-Farm-Fresh-Grade-A-Large-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/36345/Egglands-Best-Grade-A-Large-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/3192325/HyVee-Free-Range-Large-Brown-Egg-Grade-A',\n                   'https://www.hy-vee.com/aisles-online/p/23903/HyVee-Grade-A-Jumbo-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/3192323/HyVee-Cage-Free-Large-Brown-Egg-Grade-A',\n                   'https://www.hy-vee.com/aisles-online/p/36346/Egglands-Best-Cage-Free-Brown-Grade-A-Large-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/3192322/HyVee-Cage-Free-Large-Brown-Egg-Grade-A',\n                   'https://www.hy-vee.com/aisles-online/p/858343/HyVee-Cage-Free-Omega3-Grade-A-Large-Brown-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/1901565/Farmers-Hen-House-Pasture-Raised-Organic-Grade-A-Large-Brown-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/60364/HyVee-HealthMarket-Organic-Grade-A-Large-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/71298/Egglands-Best-Extra-Large-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/23902/HyVee-Grade-A-Extra-Large-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/453006/Egglands-Best-XL-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/2668550/HyVee-One-Step-Pasture-Raised-Large-Brown-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/66622/Farmers-Hen-House-Jumbo-Brown-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/3274825/Nellies-Eggs-Brown-Free-Range-Large',\n                   'https://www.hy-vee.com/aisles-online/p/57235/HyVee-Grade-A-Medium-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/2437128/Pete-And-Gerrys-Eggs-Organic-Brown-Free-Range-Large',\n                   'https://www.hy-vee.com/aisles-online/p/36347/Egglands-Best-Organic-Cage-Free-Grade-A-Large-Brown-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/2698224/Nellies-Free-Range-Eggs-Large-Fresh-Brown-Grade-A',\n                   'https://www.hy-vee.com/aisles-online/p/57237/HyVee-Grade-A-Large-Eggs',\n                   'https://www.hy-vee.com/aisles-online/p/190508/Farmers-Hen-House-Organic-Large-Brown-Eggs'\n                  ]\n        HeirloomTomatoesUrls = ['https://www.hy-vee.com/aisles-online/p/37174/']\n\n        Products.Bacon.addToProduct(BaconUrls,'urls')\n        Products.Eggs.addToProduct(EggUrls,'urls')\n        Products.HeirloomTomatoes.addToProduct(HeirloomTomatoesUrls,'urls')\n\n    #This handles the xpaths by adding to the Products class\n    #most websites have simular xpaths for each item. You might need to make differnet xpaths for each item \n    #if that is the case\n    #For assigning xpaths mark them if they are optional meaning it could or could not be present on the page \n    #we do this for speed up if you mark it as non optional and its not pressent it will skip the value \n    #and hurt the preformence\n    #best practice is to render the optional last so it reduces the chances of skipping \n    #Note spiecal cases do happen but they are extremely rare a good indiaction of finding one \n    #is by using skipHandler method and tracking/watching the logs  \n    #IMPORTANT &lt; -!- NOT ALL XPATHS ARE THE SAME FOR EACH PRODUCT -!-&gt;\n    def xpathMaker(self):\n        #Add the xpaths here and mark if they are optional\n        nameXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/h1'\n        priceXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[1]'\n        prevPriceXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[2]'\n        weightXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[3]' # optional\n\n        #xpath, Optional\n        xpathList = [(nameXpath, False),\n                     (priceXpath, False),\n                     (prevPriceXpath, False),\n                     (weightXpath, True)]\n\n        Products.Bacon.addToProduct(xpathList,'xpath')\n        Products.Eggs.addToProduct(xpathList,'xpath')\n        Products.HeirloomTomatoes.addToProduct(xpathList,'xpath')\n\n\nclass DataCleaner():\n    DataArray = []\n    def cleanUp(self, item, url):\n        self.DataArray = item\n        if self.DataArray[3] == None:\n            self.swap_elements(2, 3)\n        self.DataArray.append(url)\n        return self.DataArray\n    \n    def swap_elements(self, idx1, idx2):\n        # Make a copy of the input list to avoid modifying it\n        new_lst = self.DataArray.copy()\n        # Swap the elements at the two indices\n        new_lst[idx1], new_lst[idx2] = new_lst[idx2], new_lst[idx1]\n        self.DataArray = new_lst\n\nclass HyveeSpider():\n    name = \"Hyvee\"  #The store name \n    spiderLogs = []         #The logs \n    skipped = []            #Skipped data \n\n    #These are methods that are available for your convences\n    def log(self, *args):\n        self.spiderLogs.append(('Logger:', args))\n        if self.LOGGER:\n            print('Logger:', *args)\n\n    def debug(self, *args):\n        self.spiderLogs.append(('Debug:', args))\n        if self.DEBUGGER:\n            print('Debug:', *args)\n    \n    def printer(self, *args):\n        self.spiderLogs.append(('Printer:', args))\n        print(*args)\n    \n    def printLogs(self):\n        print(\"\\n&lt; --- Printing Logs --- &gt;\\n\")\n        for entry in self.spiderLogs:\n            print(*entry)\n\n    def Logs_to_file(self, filename):\n        with open(filename, 'w') as file:\n            for log_entry in self.spiderLogs:\n                file.write('{} {}\\n'.format(log_entry[0], log_entry[1]))\n    \n    def __init__(self):\n        self.DEBUGGER = False #The debugger switch to see whats going on. The Default is False\n        self.LOGGER = False #When you need to see everything that happends. The Default is False\n        self.attempts = 3 #The number of attempts the spider can retry if an error occurs. Default is 3\n        self.waitTime = 10 #The number of seconds WebDriver will wait. Default is 10\n        self.count = 0 #This saves the location of the url we are going through\n        self.runTime = 0 #Total time of extractions\n        self.totalRecoveries = 0 #Number of recoveries made while running\n        self.maxRetryCount = 100 #Number of retrys the javascript can make Defualt is 100\n        self.cleaner = DataCleaner() #Loads the cleaner\n        #Selenium needs a webdriver to work. I chose Firefox however you can do another if you need too\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n        self.log(\"Driver started\")\n    \n    #This handles the restart in case we run into an error\n    def restart(self):\n        self.driver.quit()\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n        self.log(\"Driver restarted\")\n    \n    #This starts the spider\n    def start_requests( self ):\n        self.runTime = time.time()\n        self.log(\"Loading from ProductsLoader Class\")\n        load = ProductsLoader() #Loads all products\n        self.dataFrames = load.DataFrames #Adds all dataframes\n        self.debug(\"Products Loaded and Data Frames Added\")\n        self.debug('\\n &lt; --- Setup runtime is %s seconds --- &gt;' % (time.time() - self.runTime))\n        self.totalRecoveries = 0 \n        #Sweeps through all products\n        for product in (Products):\n            result = self.requestExtraction(product)\n        #Adds the date that the data was scraped\n        currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n        self.log(\"Exporting files\")\n        #Dataframes to CSV files\n        for df, product in zip(self.dataFrames, (Products)):\n            df.to_csv(currentDate + self.name +\" \" + product.name + \".csv\")\n            self.log('\\n', df.to_string())\n        self.debug('\\n &lt; --- Total runtime took %s seconds with %d recoveries --- &gt;' % (time.time() - self.runTime, self.totalRecoveries))\n        if len(self.skipped) != 0:\n            self.debug('\\n &lt; -!- WARNING SKIPPED (' + str(len(self.skipped)) + ') DATA FOUND ---&gt;')\n        self.Logs_to_file(currentDate + self.name + ' Spider Logs.txt')\n        if len(self.skipped) &gt; 0:\n            self.debug(self.skipped)\n            self.skipHandler(currentDate)      \n        self.driver.quit()\n\n    #This handles the extraction request for the inputed product \n    def requestExtraction(self, product):\n        self.count = 0\n        errors = 0\n        start = time.time()\n        self.debug(\"Starting \"+ product.name)    \n        for trying in range(self.attempts):\n            try:\n                self.makeRequest(product)\n                self.debug(product.name + \" Finished\")    \n                self.log('\\n&lt; --- ' + product.name + ' scrape took %s seconds with %d recoveries --- &gt;\\n' % ((time.time() - start), errors))\n                self.totalRecoveries += errors\n                return self.totalRecoveries\n            except Exception as e:\n                #Note sometimes the browser will closed unexpectedly and theres not we can do but restart the driver\n                errors += 1\n                self.debug(\"An error occurred:\", e)\n                self.debug(\"Recovering extraction and continueing\")\n                self.restart() \n        self.debug(product.name + \" Did not Finished after \" + str(self.attempts) + \" Time wasted: %s seconds\" % (time.time() - start))\n        self.totalRecoveries += errors\n        return self.totalRecoveries\n\n    #This handles the reqests for each url and adds the data to the dataframe\n    def makeRequest(self, product):\n        productUrls = product.value[1]\n        total = len(productUrls)\n        while self.count &lt; total:\n            url = productUrls[self.count]\n            self.driver.get(url)\n            self.log(\"Making a request for: \", url)\n            item = []\n            time.sleep(1) # marionette Error Fix\n            for xpath in product.value[2]:\n                #Retrying the xpath given the number of attempts\n                for attempt in range(self.attempts):\n                    data = self.javascriptXpath(xpath[0])\n                    if data in {'empty', 'skip'}:\n                        #speical case in case you need it\n                        if len(xpath) == 3:\n                            if xpath[2]:\n                                #example would be when there is actually is a '' in the xpath\n                                self.debug(\"xpath marked as speical\")\n                                item.append(None)\n                                data = 'speical'\n                                break\n                        if xpath[1] and data == 'empty':    \n                            #this is where setting the xpath to optional comes in\n                            self.debug(\"xpath wasnt avaliable\")\n                            item.append(None)\n                            break\n                        self.debug(\"Missing item retrying\")\n                    else:  #Data found\n                        item.append(data)\n                        self.log(data + ' was added to the list for: ', url)\n                        break\n                if attempt == self.attempts:\n                    data = 'skip'\n                if data == 'skip':  #To help clean the data we skip the item with gaps of data \n                    self.debug(\"An Item has been skipped for: \", url)  \n                    item = ['SKIPPED']\n                    #Taking the product name  dataframe number and index added as well as the url \n                    #to retry for later \n                    #This could take time to do so we do this at the very end after we made the cvs files\n                    self.skipped.append([product, self.count, url])\n                    break\n            if 'SKIPPED' in item:\n                #No point in cleaning skipped items\n                items = ['SKIPPED']*(self.dataFrames[product.value[0]].shape[1] - 1)\n                items.append(url)\n            else:\n                #We call the DataCleaner class to handle the cleaning of the data\n                #Its best to clean the data before we add it to the data frame\n                self.debug('Data cleaning started: ', item)\n                items = self.cleaner.cleanUp(item, url)\n                self.debug('Data cleaning finished: ', item)\n            self.debug('Extracted: ', items)\n            self.dataFrames[product.value[0]].loc[len(self.dataFrames[product.value[0]])] = items                    \n            self.count += 1\n            self.printer(product.name + \" item added \", self.count, \" of \", total, \":  \", items)\n\n    #Collecting the data from the xpath in JavaScript is faster and results in fewer errors than doing it in python\n    #This is where selenium shines because we can both use JavaScript and render JavaScript websites\n    #and is the only reason why we use it instead of scrapy\n    def javascriptXpath(self, xpath):\n        # if the time expires it assumes xpath wasnt found in the page\n        try: \n            #Waits for page to load \n            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n            elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))\n\n            # Runs the javascript and collects the text data from the inputed xpath\n            # We want to keep repeating if we get any of these outputs becasue the page is still \n            # loading and we dont want to skip or waste time. (for fast computers)\n            retrycount = 0\n            invalidOutputs = {\"error\", 'skip' \"$nan\", ''}\n            while retrycount &lt; self.maxRetryCount :\n                text = self.driver.execute_script(\"\"\"\n                    const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;\n                    if (!element) {\n                        return 'skip';\n                    }\n                    return element.textContent.trim();\n                \"\"\", \n                xpath)\n                checkText = text.replace(\" \", \"\").lower()\n                if checkText in invalidOutputs:\n                    retrycount+=1\n                else:\n                    self.log(retrycount, \"xpath attempts for (\", text, \")\")\n                    return text\n            self.log(\"xpath attempts count met. Problematic text (\" + text + \") for \", xpath)\n            return 'skip'\n        except TimeoutException:\n            self.log('Could not find xpath for: ', xpath)\n            return 'empty'\n\n           \n\n    #This is here to hopefully fix skipped data\n    #Best case sinarios this will never be used\n    def skipHandler(self, currentDate):\n        corrections = 0\n        # skipped format\n        # [product name, DataFrame number, DataFrame index, url]\n        while len(self.skipped) != 0:\n            #each skip \n            for index, dataSkip in enumerate(self.skipped):\n                product = dataSkip[0]\n                #Limiting the Attempts to fix while avoiding bottlenecking the problem\n                for attempt in range(self.attempts*2):\n                    product = dataSkip[0]\n                    url = dataSkip[2]\n                    self.driver.get(url)\n                    self.log(\"Making a request for: \", url)\n                    item = []\n                    for xpath in product.value[2]:\n                        for attemptIn in range(self.attempts*2):\n                            data = self.javascriptXpath(xpath[0])\n                            if data in {'empty', 'skip'}:   \n                                if xpath[1] and data == 'empty':    \n                                    #this is where setting the xpath to optional comes in\n                                    self.debug(\"xpath wasnt avaliable\")\n                                    item.append(None)\n                                    break\n                                self.debug(\"Missing item retrying\")\n                            else:  #Data found\n                                item.append(data)\n                                self.log(data + ' was added to the list for: ', url)\n                                break\n                        if attemptIn == self.attempts*2:\n                            data = 'skip'\n                            break\n                if data == 'skip':  #To help clean the data we skip the item with gaps of data \n                    self.debug(\"Item still missing attempting other skipped for now\") \n                else:\n                    items = self.cleaner.cleanUp(item, url)\n                    self.dataFrames[dataSkip[1]].loc[dataSkip[2]] = items                    \n                    self.printer(\"Fixed \" + product.name + \" item: \", items)\n                    #To avoid infinite loops and never saving our data we save the file now\n                    self.dataFrames[product.value[0]].to_csv(currentDate + \"REPAIRED Gateway Market \" + product.name + \".csv\")\n                    self.debug('\\n &lt; --- Total runtime with saving of repairs took %s seconds --- &gt;' % (time.time() - self.runTime))\n                    self.Logs_to_file(currentDate + self.name + ' Spider REPAIR Logs.txt')\n                    #To avoid fixing fixed items we pop, mark, and break\n                    self.skipped.pop(index)\n                    corrections += 1\n                    break\n        self.debug('\\n &lt; --- Total runtime with all repairs took %s seconds --- &gt;' % (time.time() - self.runTime))\n        self.Logs_to_file(currentDate + self.name + ' spider COMPLETED REPAIR Logs.txt')\n        \n# Start\n#DEBUG Switch\nSHOW = True\n\n#Spider setup\nspider = HyveeSpider()\nspider.LOGGER = True\nspider.DEBUGGER = True\n\n#Running the spider\nspider.start_requests()\n\nif(SHOW):\n    [print(dataFrame) for dataFrame in spider.dataFrames]\n    spider.printLogs()\n\nThis is the code for the New Pioneer Co-op Scraper\n\n#Imports\nfrom datetime import datetime\nimport pandas as pd\nfrom enum import Enum\n#Imports for Scraping\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.service import Service as FirefoxService\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import StaleElementReferenceException\nfrom selenium.common.exceptions import WebDriverException\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom os import path\nimport time\nimport sys\n\n\n#Creator's Note: Products(Enum) and ProductsLoader is probably the only classes you need to edit \n#unless you need to change the way the data is cleaned. Which handled in the DataCleaner class\n\n#These class is here so that we can expand to differnet products easier making the spider more dynamic and expandable\nclass Products(Enum):\n    #Add products like this ProductName = index iteration, [], [] \n    #the 2 empty list will be filled in using the ProductsLoader class\n    Bacon = 0, [], []\n    Eggs = 1, [], []\n    HeirloomTomatoes = 2, [], []\n\n    # Helper method to reduce code for adding to the products and weed out duplicate inputs\n    # if you type something in really wrong code will stop the setup is important \n    # correct index inputs are correct index number, url, urls, xpath, xpaths\n    def addToProduct(self, items, index):\n        product = None\n        if isinstance(index, int):\n            product = self.value[index]\n        elif isinstance(index, str):\n            if index.lower() in ['urls', 'url']:\n                product = self.value[1]\n            elif index.lower() in ['xpaths', 'xpath']:\n                product = self.value[2]\n        if product == None:\n            raise ValueError(f\"Invalid index input for ({index}) for input: {items}\")\n        #Sets are fast at finding dups so we use them for speed\n        product_set = set(product)\n        for item in items:\n            if item not in product_set:\n                product.append(item)\n                product_set.add(item)\n\n#This class loads the xpaths and urls to the Products Enum and adds dataframes to the spider\nclass ProductsLoader():\n    DataFrames = []\n    storeXpaths = []\n    def __init__(self):\n        self.dataFrameAdder()\n        self.setStoreXpaths()\n        self.urlsAdder()\n        self.xpathMaker()\n\n    #This adds the dataframe to the spider on load\n    def dataFrameAdder(self):\n        #Dataframes (You can add more here)\n        baconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Weight' ,'Sale', 'Store Location', 'Url'])\n        eggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Weight' ,'Sale', 'Store Location', 'Url'])\n        tomatoFrame = pd.DataFrame(columns=['Heirloom Tomato', 'Current Price', 'Weight' ,'Sale', 'Store Location', 'Url'])\n        self.DataFrames = [baconFrame,\n                           eggFrame,\n                           tomatoFrame\n                          ]\n\n    def setStoreXpaths(self):\n        CoralvilleButtonXpath = '//*[@id=\"store-locator\"]//*[contains(@data-store-id,\"2843\") and contains(@class,\"fp-btn fp-btn-default fp-btn-mystore \") and contains(@role,\"button\")]'\n        IowaCityButtonXpath = '//*[@id=\"store-locator\"]//*[contains(@data-store-id,\"2844\") and contains(@class,\"fp-btn fp-btn-default fp-btn-mystore \") and contains(@role,\"button\")]'\n        CedarRapidsButtonXpath = '//*[@id=\"store-locator\"]//*[contains(@data-store-id,\"2845\") and contains(@class,\"fp-btn fp-btn-default fp-btn-mystore \") and contains(@role,\"button\")]'\n        self.storeXpaths = [CedarRapidsButtonXpath,\n                            IowaCityButtonXpath,\n                            CoralvilleButtonXpath\n                            ]\n\n    #Adding Urls to products\n    def urlsAdder(self):\n        BaconUrls = ['https://shop.newpi.coop/shop/meat/bacon/sliced/applegate_natural_hickory_smoked_uncured_sunday_bacon_8_oz/p/19959#!/?department_id=1322093',\n                     'https://shop.newpi.coop/shop/meat/bacon/beeler_hickory_smoked_bacon/p/7703726#!/?department_id=1322093',\n                     'https://shop.newpi.coop/shop/meat/bacon/beeler_bacon_ends_and_pieces/p/1564405684703446698#!/?department_id=1322093',\n                     'https://shop.newpi.coop/shop/meat/bacon/beeler_hickory_smoked_bacon/p/7791059#!/?department_id=1322093',\n                     'https://shop.newpi.coop/shop/meat/bacon/garrett_valley_pork_bacon_classic_dry_rubbed_uncured/p/7703238#!/?department_id=1322093',\n                     'https://shop.newpi.coop/shop/meat/bacon/turkey/garrett_valley_turkey_bacon_sugar_free_paleo/p/7703237#!/?department_id=1322093',\n                     'https://shop.newpi.coop/shop/meat/bacon/beeler_pepper_bacon/p/1564405684702577823#!/?department_id=1322093',\n                     'https://shop.newpi.coop/shop/meat/bacon/turkey/plainville_farms_turkey_bacon_uncured/p/4750634#!/?department_id=1322093',\n                     'https://shop.newpi.coop/shop/meat/bacon/garrett_valley_pork_bacon_8_oz/p/6572556#!/?department_id=1322093',\n                     'https://shop.newpi.coop/shop/refrigerated/meat_alternatives/herbivorous_butcher_hickory_maple_bacon/p/1564405684704334152#!/?department_id=1322093',\n                     'https://shop.newpi.coop/shop/meat/pork/new_pi_bulk_bacon/p/1564405684704337543#!/?department_id=1322171',\n                     'https://shop.newpi.coop/shop/meat/pork/niman_ranch_uncured_bacon_12_oz/p/7276#!/?department_id=1322171'\n                    ]\n        EggUrls = ['https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/farmers_hen_house_eggs_grade_a_free_range_large_brown_12_ea/p/7110637',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/farmers_hen_house_eggs_white_cage_free_large/p/7110638',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/farmers_hen_house_eggs_large_brown_free_range/p/7613595',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/vital_farms_eggs_organic_pasture_raised_large_12_ea/p/5637123',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/hotz_eggs_dozen_xtra_large/p/1564405684714084840',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/farmers_hen_house_eggs_jumbo_brown/p/7613596',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/cedar_ridge_farm_grade_a_large_eggs/p/1564405684704338616',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/farmers_hen_house_eggs_og_pasture_lrg_brwn/p/7613597',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/farmers_hen_house_eggs_large_brown_free_range/p/1564405684703497142',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/vital_farms_large_pasture_raised_eggs_12_ea/p/5323128',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/vital_farms_eggs_pasture_raised_large_18_ea/p/1564405684690018196',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/organic_valley_free_range_brown_large_eggs_12_ea/p/48765',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/cedar_ridge_farm_grade_a_extra_large_eggs/p/1564405684704338617',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/steinecke_family_farm_duck_eggs/p/1564405684711593802',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/cosgrove_rd_farm_eggs_pasture_raised/p/1564405684710338102',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/cedar_ridge_farm_grade_a_jumbo_eggs/p/1564405684704338619',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/hotz_eggs_large/p/1564405684704684702',\n                   'https://shop.newpi.coop/shop/refrigerated/eggs/hotz_eggs_medium/p/1564405684713746940',\n                  ]\n        HeirloomTomatoesUrls = ['https://shop.newpi.coop/shop/produce/fresh_vegetables/tomatoes/heirloom_tomatoes/p/2311736']\n\n        Products.Bacon.addToProduct(BaconUrls,'urls')\n        Products.Eggs.addToProduct(EggUrls,'urls')\n        Products.HeirloomTomatoes.addToProduct(HeirloomTomatoesUrls,'urls')\n\n    #This handles the xpaths by adding to the Products class\n    #most websites have simular xpaths for each item. You might need to make differnet xpaths for each item \n    #if that is the case\n    #For assigning xpaths mark them if they are optional meaning it could or could not be present on the page \n    #we do this for speed up if you mark it as non optional and its not pressent it will skip the value \n    #and hurt the preformence\n    #best practice is to render the optional last so it reduces the chances of skipping \n    #Note spiecal cases do happen but they are extremely rare a good indiaction of finding one \n    #is by using skipHandler method and tracking/watching the logs  \n    #IMPORTANT &lt; -!- NOT ALL XPATHS ARE THE SAME FOR EACH PRODUCT -!-&gt;\n    def xpathMaker(self):\n        #Add the xpaths here and mark if they are optional\n        nameXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-name\")]' #special because the store can be not carrying the product\n        priceXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-price\")]//span[contains(@class,\"fp-item-base-price\")]'\n        weightXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-price\")]//span[contains(@class,\"fp-item-size\")]'\n        saleXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-item-detail\")]//*[contains(@class,\"fp-item-sale\")]//span[contains(@class,\"fp-item-sale-price\")]' # optional\n        #xpath, Optional, special\n        xpathList = [(nameXpath, False, True),\n                     (priceXpath, False),\n                     (weightXpath, False),\n                     (saleXpath, True)]\n\n        Products.Bacon.addToProduct(xpathList,'xpath')\n        Products.Eggs.addToProduct(xpathList,'xpath')\n        Products.HeirloomTomatoes.addToProduct(xpathList,'xpath')\n\n\nclass DataCleaner():\n    DataArray = []\n    storeLocation = ''\n    def cleanUp(self, item, url):\n        self.DataArray = item\n        self.DataArray.append(self.storeLocation)\n        self.DataArray.append(url)\n        return self.DataArray\n    \n    def setStore(self, storeIndex):\n        cases = {\n            0: \"3338 Center Point Road Northeast Cedar Rapids, IA 52402\",\n            1: \"22 South Van Buren Street Iowa City, IA 52240\",\n            2: \"1101 2nd St Coralville, IA 52241\"\n        }\n        self.storeLocation = cases.get(storeIndex)\n\n    \n\nclass NewPioneerSpider():\n    name = \"New Pioneer Co-op\"  #The store name \n    spiderLogs = []         #The logs \n    skipped = []            #Skipped data \n\n    #These are methods that are available for your convences\n    def log(self, *args):\n        self.spiderLogs.append(('Logger:', args))\n        if self.LOGGER:\n            print('Logger:', *args)\n\n    def debug(self, *args):\n        self.spiderLogs.append(('Debug:', args))\n        if self.DEBUGGER:\n            print('Debug:', *args)\n    \n    def printer(self, *args):\n        self.spiderLogs.append(('Printer:', args))\n        print(*args)\n    \n    def printLogs(self):\n        print(\"\\n&lt; --- Printing Logs --- &gt;\\n\")\n        for entry in self.spiderLogs:\n            print(*entry)\n\n    def Logs_to_file(self, filename):\n        with open(filename, 'w') as file:\n            for log_entry in self.spiderLogs:\n                file.write('{} {}\\n'.format(log_entry[0], log_entry[1]))\n    \n    def __init__(self):\n        self.DEBUGGER = False #The debugger switch to see whats going on. The Default is False\n        self.LOGGER = False #When you need to see everything that happends. The Default is False\n        self.attempts = 3 #The number of attempts the spider can retry if an error occurs. Default is 3\n        self.waitTime = 10 #The number of seconds WebDriver will wait. Default is 10\n        self.count = 0 #This saves the location of the url we are going through\n        self.runTime = 0 #Total time of extractions\n        self.totalRecoveries = 0 #Number of recoveries made while running\n        self.maxRetryCount = 100 #Number of retrys the javascript can make Defualt is 100\n        self.cleaner = DataCleaner() #Loads the cleaner\n        self.load = ProductsLoader() #Loads all products\n        #Selenium needs a webdriver to work. I chose Firefox however you can do another if you need too\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n        self.log(\"Driver started\")\n    \n    #This handles the restart in case we run into an error\n    def restart(self):\n        self.driver.quit()\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n        self.log(\"Driver restarted\")\n        self.setStoreLocation()\n    \n    #Some stores need to have a location set\n    def setStoreLocation(self):\n        storeLocationUrl = 'https://shop.newpi.coop/my-store/store-locator'\n        self.driver.get(storeLocationUrl)\n        time.sleep(5) #Wait for the page to set\n        xpath = self.load.storeXpaths[self.storeIndex]\n        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n        elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))\n        elements[0].click()\n        time.sleep(5) #Wait for the page to set\n        self.log(\"Store location set\")\n\n\n    #This starts the spider\n    def start_requests( self ):\n        self.runTime = time.time()\n        self.log(\"Loading from ProductsLoader Class\")\n        self.dataFrames = self.load.DataFrames #Adds all dataframes\n        self.debug(\"Products Loaded and Data Frames Added\")\n        self.debug('\\n &lt; --- Setup runtime is %s seconds --- &gt;' % (time.time() - self.runTime))\n        self.totalRecoveries = 0 \n        #This sweeps through every inputed store\n        for index in range(len(self.load.storeXpaths)):\n            self.storeIndex = index\n            self.cleaner.setStore(self.storeIndex)\n            self.setStoreLocation()\n            #Sweeps through all products\n            for product in (Products):\n                result = self.requestExtraction(product)\n            self.debug(\"New store location data added\")\n\n        #Adds the date that the data was scraped\n        currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n        self.log(\"Exporting files\")\n        #Dataframes to CSV files\n        for df, product in zip(self.dataFrames, (Products)):\n            df.to_csv(currentDate + self.name +\" \" + product.name + \".csv\")\n            self.log('\\n', df.to_string())\n        self.debug('\\n &lt; --- Total runtime took %s seconds with %d recoveries --- &gt;' % (time.time() - self.runTime, self.totalRecoveries))\n        if len(self.skipped) != 0:\n            self.debug('\\n &lt; -!- WARNING SKIPPED (' + str(len(self.skipped)) + ') DATA FOUND ---&gt;')\n        self.Logs_to_file(currentDate + self.name + ' Spider Logs.txt')\n        if len(self.skipped) &gt; 0:\n            self.debug(self.skipped)\n            self.skipHandler(currentDate)      \n        self.driver.quit()\n\n    #This handles the extraction request for the inputed product \n    def requestExtraction(self, product):\n        self.count = 0\n        errors = 0\n        start = time.time()\n        self.debug(\"Starting \"+ product.name)    \n        for trying in range(self.attempts):\n            try:\n                self.makeRequest(product)\n                self.debug(product.name + \" Finished\")    \n                self.log('\\n&lt; --- ' + product.name + ' scrape took %s seconds with %d recoveries --- &gt;\\n' % ((time.time() - start), errors))\n                self.totalRecoveries += errors\n                return self.totalRecoveries\n            except Exception as e:\n                #Note sometimes the browser will closed unexpectedly and theres not we can do but restart the driver\n                errors += 1\n                self.debug(\"An error occurred:\", e)\n                self.debug(\"Recovering extraction and continueing\")\n                self.restart() \n        self.debug(product.name + \" Did not Finished after \" + str(self.attempts) + \" Time wasted: %s seconds\" % (time.time() - start))\n        self.totalRecoveries += errors\n        return self.totalRecoveries\n\n    #This handles the reqests for each url and adds the data to the dataframe\n    def makeRequest(self, product):\n        productUrls = product.value[1]\n        total = len(productUrls)\n        while self.count &lt; total:\n            url = productUrls[self.count]\n            self.driver.get(url)\n            self.log(\"Making a request for: \", url)\n            item = []\n            time.sleep(1) # marionette Error Fix\n            breakout = False\n            for xpath in product.value[2]:\n                #Retrying the xpath given the number of attempts\n                for attempt in range(self.attempts):\n                    data = self.javascriptXpath(xpath[0])\n                    if data in {'empty', 'skip'}:\n                        #speical case\n                        if len(xpath) == 3:\n                            #the first attempt shouldnt go through in case it when through to fast\n                            #this will slow the fuction down however Accuracy &gt; Speed \n                            if xpath[2]:\n                                if attempt == 0:\n                                    self.debug(\"Found a missing item in store. Double checking\")\n                                    continue\n                                #example would be when there is actually is a '' in the xpath\n                                self.debug(\"xpath marked as speical\")\n                                notFoundXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-text-center fp-not-found\")]//*[contains(@class,\"fp-text-center\")]'\n                                data = self.javascriptXpath(notFoundXpath)\n                                if data in {'empty', 'skip'}:\n                                    self.debug(\"Missing item retrying\")\n                                else:\n                                    self.debug(\"An Item not in stock for: \", url) \n                                    item.append(data)\n                                    df = self.dataFrames[product.value[0]]\n                                    num = len(df.columns) - len(item) % len(df.columns)\n                                    item += [\"None in stock\"] * (num - 2)\n                                    breakout = True\n                                    break\n                        if xpath[1] and data == 'empty':    \n                            #this is where setting the xpath to optional comes in\n                            self.debug(\"xpath wasnt avaliable\")\n                            item.append(None)\n                            break\n                        self.debug(\"Missing item retrying\")\n                    else:  #Data found\n                        item.append(data)\n                        self.log(data + ' was added to the list for: ', url)\n                        break\n                if breakout:\n                    break\n                if attempt == self.attempts:\n                    data = 'skip'\n                if data == 'skip':  #To help clean the data we skip the item with gaps of data \n                    self.debug(\"An Item has been skipped for: \", url)  \n                    item = ['SKIPPED']\n                    #Taking the product name  dataframe number and index added as well as the url \n                    #to retry for later \n                    #This could take time to do so we do this at the very end after we made the cvs files\n                    self.skipped.append([product, self.count, url])\n                    break\n            if 'SKIPPED' in item:\n                #No point in cleaning skipped items\n                items = ['SKIPPED']*(self.dataFrames[product.value[0]].shape[1] - 1)\n                items.append(url)\n            else:\n                #We call the DataCleaner class to handle the cleaning of the data\n                #Its best to clean the data before we add it to the data frame\n                self.debug('Data cleaning started: ', item)\n                items = self.cleaner.cleanUp(item, url)\n                self.debug('Data cleaning finished: ', item)\n            self.debug('Extracted: ', items)\n            self.dataFrames[product.value[0]].loc[len(self.dataFrames[product.value[0]])] = items                    \n            self.count += 1\n            self.printer(product.name + \" item added \", self.count, \" of \", total, \":  \", items)\n\n    #Collecting the data from the xpath in JavaScript is faster and results in fewer errors than doing it in python\n    #This is where selenium shines because we can both use JavaScript and render JavaScript websites\n    #and is the only reason why we use it instead of scrapy\n    def javascriptXpath(self, xpath):\n        # if the time expires it assumes xpath wasnt found in the page\n        try: \n            #Waits for page to load \n            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n            elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))\n\n            # Runs the javascript and collects the text data from the inputed xpath\n            # We want to keep repeating if we get any of these outputs becasue the page is still \n            # loading and we dont want to skip or waste time. (for fast computers)\n            retrycount = 0\n            invalidOutputs = {\"error\", 'skip' \"$nan\", ''}\n            while retrycount &lt; self.maxRetryCount :\n                text = self.driver.execute_script(\"\"\"\n                    const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;\n                    if (!element) {\n                        return 'skip';\n                    }\n                    return element.textContent.trim();\n                \"\"\", \n                xpath)\n                checkText = text.replace(\" \", \"\").lower()\n                if checkText in invalidOutputs:\n                    retrycount+=1\n                else:\n                    self.log(retrycount, \"xpath attempts for (\", text, \")\")\n                    return text\n            self.log(\"xpath attempts count met. Problematic text (\" + text + \") for \", xpath)\n            return 'skip'\n        except TimeoutException:\n            self.log('Could not find xpath for: ', xpath)\n            return 'empty'\n\n           \n\n    #This is here to hopefully fix skipped data\n    #Best case sinarios this will never be used\n    def skipHandler(self, currentDate):\n        corrections = 0\n        # skipped format\n        # [product name, DataFrame number, DataFrame index, url]\n        while len(self.skipped) != 0:\n            #each skip \n            for index, dataSkip in enumerate(self.skipped):\n                product = dataSkip[0]\n                #Limiting the Attempts to fix while avoiding bottlenecking the problem\n                for attempt in range(self.attempts*2):\n                    product = dataSkip[0]\n                    url = dataSkip[2]\n                    self.driver.get(url)\n                    self.log(\"Making a request for: \", url)\n                    item = []\n                    breakout = False\n                    for xpath in product.value[2]:\n                        for attemptIn in range(self.attempts*2):\n                            if data in {'empty', 'skip'}:\n                                #speical case\n                                if len(xpath) == 3:\n                                    #the first attempt shouldnt go through in case it when through to fast\n                                    #this will slow the fuction down however Accuracy &gt; Speed \n                                    if xpath[2]:\n                                        if attempt == 0:\n                                            self.debug(\"Found a missing item in store. Double checking\")\n                                            continue\n                                        #example would be when there is actually is a '' in the xpath\n                                        self.debug(\"xpath marked as speical\")\n                                        notFoundXpath = '//*[@id=\"products\"]//*[contains(@class,\"fp-text-center fp-not-found\")]//*[contains(@class,\"fp-text-center\")]'\n                                        data = self.javascriptXpath(notFoundXpath)\n                                        if data in {'empty', 'skip'}:\n                                            self.debug(\"Missing item retrying\")\n                                        else:\n                                            self.debug(\"An Item not in stock for: \", url) \n                                            item.append(data)\n                                            df = self.dataFrames[product.value[0]]\n                                            num = len(df.columns) - len(item) % len(df.columns)\n                                            item += [\"None in stock\"] * (num - 2)\n                                            breakout = True\n                                            break\n                                if xpath[1] and data == 'empty':    \n                                    #this is where setting the xpath to optional comes in\n                                    self.debug(\"xpath wasnt avaliable\")\n                                    item.append(None)\n                                    break\n                                self.debug(\"Missing item retrying\")\n                            else:  #Data found\n                                item.append(data)\n                                self.log(data + ' was added to the list for: ', url)\n                                break\n                        if breakout:\n                            break\n                    if breakout:\n                        break\n                    if attemptIn == self.attempts*2:\n                        data = 'skip'\n                        break\n                if data == 'skip':  #To help clean the data we skip the item with gaps of data \n                    self.debug(\"Item still missing attempting other skipped for now\") \n                else:\n                    items = self.cleaner.cleanUp(item, url)\n                    self.dataFrames[dataSkip[1]].loc[dataSkip[2]] = items                    \n                    self.printer(\"Fixed \" + product.name + \" item: \", items)\n                    #To avoid infinite loops and never saving our data we save the file now\n                    self.dataFrames[product.value[0]].to_csv(currentDate + \"REPAIRED Gateway Market \" + product.name + \".csv\")\n                    self.debug('\\n &lt; --- Total runtime with saving of repairs took %s seconds --- &gt;' % (time.time() - self.runTime))\n                    self.Logs_to_file(currentDate + self.name + ' Spider REPAIR Logs.txt')\n                    #To avoid fixing fixed items we pop, mark, and break\n                    self.skipped.pop(index)\n                    corrections += 1\n                    break\n        self.debug('\\n &lt; --- Total runtime with all repairs took %s seconds --- &gt;' % (time.time() - self.runTime))\n        self.Logs_to_file(currentDate + self.name + ' spider COMPLETED REPAIR Logs.txt')\n\n# Start\n#DEBUG Switch\nSHOW = True\n\n#Spider setup\nspider = NewPioneerSpider()\nspider.LOGGER = True\nspider.DEBUGGER = True\n\n#Running the spider\nspider.start_requests()\n\nif(SHOW):\n    [print(dataFrame) for dataFrame in spider.dataFrames]\n    spider.printLogs()\n\nThis is the code for the Russ’s Market Scraper\n\n#Imports\nfrom datetime import datetime\nimport pandas as pd\nfrom enum import Enum\n#Imports for Scraping\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.service import Service as FirefoxService\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import StaleElementReferenceException\nfrom selenium.common.exceptions import WebDriverException\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom os import path\nimport time\nimport sys\n\n\n#Creator's Note: Products(Enum) and ProductsLoader is probably the only classes you need to edit \n#unless you need to change the way the data is cleaned. Which handled in the DataCleaner class\n\n#These class is here so that we can expand to differnet products easier making the spider more dynamic and expandable\nclass Products(Enum):\n    #Add products like this ProductName = index iteration, [], [] \n    #the 2 empty list will be filled in using the ProductsLoader class\n    Bacon = 0, [], []\n    Eggs = 1, [], []\n    HeirloomTomatoes = 2, [], []\n\n    # Helper method to reduce code for adding to the products and weed out duplicate inputs\n    # if you type something in really wrong code will stop the setup is important \n    # correct index inputs are correct index number, url, urls, xpath, xpaths\n    def addToProduct(self, items, index):\n        product = None\n        if isinstance(index, int):\n            product = self.value[index]\n        elif isinstance(index, str):\n            if index.lower() in ['urls', 'url']:\n                product = self.value[1]\n            elif index.lower() in ['xpaths', 'xpath']:\n                product = self.value[2]\n        if product == None:\n            raise ValueError(f\"Invalid index input for ({index}) for input: {items}\")\n        #Sets are fast at finding dups so we use them for speed\n        product_set = set(product)\n        for item in items:\n            if item not in product_set:\n                product.append(item)\n                product_set.add(item)\n\n#this class loads the xpaths and urls to the Products Enum and adds dataframes to the spider\nclass ProductsLoader():\n    DataFrames = []\n    def __init__(self):\n        self.dataFrameAdder()\n        self.urlsAdder()\n        self.xpathMaker()\n\n    #This adds the dataframe to the spider on load\n    def dataFrameAdder(self):\n        #Dataframes (You can add more here)\n        baconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Weight', 'Sale Price', 'Location', 'Url'])\n        eggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Weight', 'Sale Price', 'Location', 'Url'])\n        tomatoFrame = pd.DataFrame(columns=['Heirloom Tomato', 'Current Price', 'Weight', 'Sale Price', 'Location', 'Url'])\n        self.DataFrames = [baconFrame,\n                           eggFrame,\n                           tomatoFrame\n                          ]\n\n    #Adding Urls to products\n    def urlsAdder(self):\n        BaconUrls = ['https://www.russmarket.com/shop/meat/bacon/sliced/hormel_black_label_thick_cut_bacon_1_lb/p/229335',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/hormel_black_label_original_bacon_16_ounce/p/2349369',\n                     'https://www.russmarket.com/shop/meat/bacon/mullan_road_bacon/p/5220311',\n                     'https://www.russmarket.com/shop/meat/bacon/prairie_fresh_signature_applewood_smoked_bacon_pork_loin_filet_27_2_oz/p/6828650',\n                     'https://www.russmarket.com/shop/meat/bacon/farmland_bacon_thick_cut_naturally_hickory_smoked/p/571658',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced_slab_bacon/p/1564405684712590572',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/smithfield_naturally_hickory_smoked_hometown_original_bacon/p/3142755',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/jamestown_economy_sliced_bacon_16_oz/p/180026',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/smithfield_naturally_hickory_smoked_thick_cut_bacon_12_oz_pack/p/3142757',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/smithfield_bacon_thick_cut_12_oz/p/2101085',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/farmland_naturally_hickory_smoked_classic_cut_bacon_16_oz/p/2376191',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/farmland_naturally_hickory_smoked_thick_cut_bacon_16_oz/p/95721',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/farmland_naturally_applewood_smoked_bacon_16_oz/p/585815',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/hormel_black_label_microwave_ready_bacon/p/26151',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/oscar_mayer_original_bacon_16_oz/p/32303',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/big_buy_hardwood_smoked_bacon/p/2101073',\n                     'https://www.russmarket.com/shop/meat/bacon/turkey/oscar_mayer_turkey_bacon_original_12_oz/p/39809',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/farmland_bacon_low_sodium_hickory_smoked_16_oz/p/2376192',\n                     'https://www.russmarket.com/shop/meat/bacon/canadian/farmland_bacon_double_smoked_double_thick_cut_12_oz/p/1564405684713224952',\n                     'https://www.russmarket.com/shop/meat/bacon/canadian/hormel_black_label_naturally_hardwood_smoked_97_fat_free_canadian_bacon_6_oz_zip_pak/p/26168',\n                     'https://www.russmarket.com/shop/meat/bacon/sliced/oscar_mayer_bacon_original_8_oz/p/32302',\n                    ]\n\n        EggUrls = [\n                   'https://www.russmarket.com/shop/dairy/eggs/everyday/best_choice_grade_a_large_eggs/p/3139172',\n                   'https://www.russmarket.com/shop/dairy/eggs/everyday/best_choice_large_eggs/p/3139176',\n                   'https://www.russmarket.com/shop/dairy/eggs/everyday/best_choice_jumbo_eggs/p/3139173',\n                   'https://www.russmarket.com/shop/dairy/eggs/everyday/best_choice_extra_large_eggs/p/3139174',\n                   'https://www.russmarket.com/shop/dairy/eggs/everyday/best_choice_large_eggs/p/3139192',\n                   'https://www.russmarket.com/shop/dairy/eggs/everyday/eggland_s_best_large_eggs_12_ea/p/54283',\n                   'https://www.russmarket.com/shop/dairy/eggs/everyday/eggland_s_best_large_eggs_18_ea/p/54279',\n                   'https://www.russmarket.com/shop/dairy/cheese/artisan_and_specialty/best_choice_hard_cook_eggs_6_ct/p/1564405684702358019',\n                  ]\n        HeirloomTomatoesUrls = ['https://www.russmarket.com/shop/produce/fresh_vegetables/tomatoes/heirloom_tomatoes/p/12412']\n        \n        Products.Bacon.addToProduct(BaconUrls,'urls')\n        Products.Eggs.addToProduct(EggUrls,'urls')\n        Products.HeirloomTomatoes.addToProduct(HeirloomTomatoesUrls,'urls')\n\n    #This handles the xpaths by adding to the Products class\n    #most websites have simular xpaths for each item. You might need to make differnet xpaths for each item \n    #if that is the case\n    #For assigning xpaths mark them if they are optional meaning it could or could not be present on the page \n    #we do this for speed up if you mark it as non optional and its not pressent it will skip the value \n    #and hurt the preformence\n    #best practice is to render the optional last so it reduces the chances of skipping \n    #Note spiecal cases do happen but they are extremely rare a good indiaction of finding one \n    #is by using skipHandler method and tracking/watching the logs  \n    #IMPORTANT &lt; -!- NOT ALL XPATHS ARE THE SAME FOR EACH PRODUCT -!-&gt;\n    def xpathMaker(self):\n        #Add the xpaths here and mark if they are optional\n        #Format [xpath, optional, speical]\n        #xpath, Optional\n        \n        nameXpath = '//*[@id=\"page-title\"]//h1[contains(@class,\"fp-page-header fp-page-title\")]'\n        priceXpath = '//*[@id=\"page-title\"]//*[contains(@class,\"fp-item-price\")]/span[contains(@class,\"fp-item-base-price\")]'\n        weightXpath = '//*[@id=\"page-title\"]//*[contains(@class,\"fp-item-price\")]/span[contains(@class,\"fp-item-size\")]' \n        saleXpath = '//*[@id=\"page-title\"]//*[contains(@class,\"fp-item-sale\")]/span[contains(@class,\"fp-item-sale-date\")]/strong' #optional\n\n        xpathList = [(nameXpath, False),\n                     (priceXpath, False),\n                     (weightXpath, False),\n                     (saleXpath, True)]\n\n        Products.Bacon.addToProduct(xpathList,'xpath')\n        Products.Eggs.addToProduct(xpathList,'xpath')\n        Products.HeirloomTomatoes.addToProduct(xpathList,'xpath')\n\nclass DataCleaner():\n    DataArray = []\n    def cleanUp(self, item, url):\n        self.DataArray = item\n        self.DataArray.append(\"900 South Locust Street Glenwood, IA 51534 \")\n        self.DataArray.append(url)\n        return self.DataArray\n\n\nclass RussMarketSpider():\n    name = \"Russ Market\"    #The store name \n    spiderLogs = []         #The logs \n    skipped = []            #Skipped data \n\n    #These are methods that are available for your convences\n    def log(self, *args):\n        self.spiderLogs.append(('Logger:', args))\n        if self.LOGGER:\n            print('Logger:', *args)\n\n    def debug(self, *args):\n        self.spiderLogs.append(('Debug:', args))\n        if self.DEBUGGER:\n            print('Debug:', *args)\n    \n    def printer(self, *args):\n        self.spiderLogs.append(('Printer:', args))\n        print(*args)\n    \n    def printLogs(self):\n        print(\"\\n&lt; --- Printing Logs --- &gt;\\n\")\n        for entry in self.spiderLogs:\n            print(*entry)\n\n    def Logs_to_file(self, filename):\n        with open(filename, 'w') as file:\n            for log_entry in self.spiderLogs:\n                file.write('{} {}\\n'.format(log_entry[0], log_entry[1]))\n    \n    def __init__(self):\n        self.DEBUGGER = False #The debugger switch to see whats going on. The Default is False\n        self.LOGGER = False #When you need to see everything that happends. The Default is False\n        self.attempts = 3 #The number of attempts the spider can retry if an error occurs. Default is 3\n        self.waitTime = 10 #The number of seconds WebDriver will wait. Default is 10\n        self.count = 0 #This saves the location of the url we are going through\n        self.runTime = 0 #Total time of extractions\n        self.totalRecoveries = 0 #Number of recoveries made while running\n        self.maxRetryCount = 100 #Number of retrys the javascript can make Defualt is 100\n        self.cleaner = DataCleaner() #Loads the cleaner\n        #Selenium needs a webdriver to work. I chose Firefox however you can do another if you need too\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n        self.log(\"Driver started\")\n    \n    #This handles the restart in case we run into an error\n    def restart(self):\n        self.driver.quit()\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n        self.log(\"Driver restarted\")\n        self.setStoreLocation()\n\n    #Some stores need to have a location set\n    def setStoreLocation(self):\n        storeLocationUrl = 'https://www.russmarket.com/shop#!/?store_id=6158'\n        self.driver.get(storeLocationUrl)\n        time.sleep(5) #Wait for the page to set\n        self.log(\"Store location set\")\n\n    #This starts the spider\n    def start_requests( self ):\n        self.runTime = time.time()\n        self.setStoreLocation()\n        self.log(\"Loading from ProductsLoader Class\")\n        load = ProductsLoader() #Loads all products\n        self.dataFrames = load.DataFrames #Adds all dataframes\n        self.debug(\"Products Loaded and Data Frames Added\")\n        self.debug('\\n &lt; --- Setup runtime is %s seconds --- &gt;' % (time.time() - self.runTime))\n        self.totalRecoveries = 0 \n        #Sweeps through all products\n        for product in (Products):\n            result = self.requestExtraction(product)\n        #Adds the date that the data was scraped\n        currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n        self.log(\"Exporting files\")\n        #Dataframes to CSV files\n        for df, product in zip(self.dataFrames, (Products)):\n            df.to_csv(currentDate + self.name +\" \" + product.name + \".csv\")\n            self.log('\\n', df.to_string())\n        self.debug('\\n &lt; --- Total runtime took %s seconds with %d recoveries --- &gt;' % (time.time() - self.runTime, self.totalRecoveries))\n        if len(self.skipped) != 0:\n            self.debug('\\n &lt; -!- WARNING SKIPPED (' + str(len(self.skipped)) + ') DATA FOUND ---&gt;')\n        self.Logs_to_file(currentDate + self.name + ' Spider Logs.txt')\n        if len(self.skipped) &gt; 0:\n            self.debug(self.skipped)\n            self.skipHandler(currentDate)      \n        self.driver.quit()\n\n    #This handles the extraction request for the inputed product \n    def requestExtraction(self, product):\n        self.count = 0\n        errors = 0\n        start = time.time()\n        self.debug(\"Starting \"+ product.name)    \n        for trying in range(self.attempts):\n            try:\n                self.makeRequest(product)\n                self.debug(product.name + \" Finished\")    \n                self.log('\\n&lt; --- ' + product.name + ' scrape took %s seconds with %d recoveries --- &gt;\\n' % ((time.time() - start), errors))\n                self.totalRecoveries += errors\n                return self.totalRecoveries\n            except Exception as e:\n                #Note sometimes the browser will closed unexpectedly and theres not we can do but restart the driver\n                errors += 1\n                self.debug(\"An error occurred:\", e)\n                self.debug(\"Recovering extraction and continueing\")\n                self.restart() \n        self.debug(product.name + \" Did not Finished after \" + str(self.attempts) + \" Time wasted: %s seconds\" % (time.time() - start))\n        self.totalRecoveries += errors\n        return self.totalRecoveries\n\n    #This handles the reqests for each url and adds the data to the dataframe\n    def makeRequest(self, product):\n        productUrls = product.value[1]\n        total = len(productUrls)\n        while self.count &lt; total:\n            url = productUrls[self.count]\n            self.driver.get(url)\n            self.log(\"Making a request for: \", url)\n            item = []\n            time.sleep(1) # marionette Error Fix\n            for xpath in product.value[2]:\n                #Retrying the xpath given the number of attempts\n                for attempt in range(self.attempts):\n                    data = self.javascriptXpath(xpath[0])\n                    if data in {'empty', 'skip'}:\n                        #speical case in case you need it\n                        if len(xpath) == 3:\n                            if xpath[2]:\n                                #example would be when there is actually is a '' in the xpath\n                                self.debug(\"xpath marked as speical\")\n                                item.append(None)\n                                data = 'speical'\n                                break\n                        if xpath[1] and data == 'empty':    \n                            #this is where setting the xpath to optional comes in\n                            self.debug(\"xpath wasnt avaliable\")\n                            item.append(None)\n                            break\n                        self.debug(\"Missing item retrying\")\n                    else:  #Data found\n                        item.append(data)\n                        self.log(data + ' was added to the list for: ', url)\n                        break\n                if attempt == self.attempts:\n                    data = 'skip'   \n                if data == 'skip':  #To help clean the data we skip the item with gaps of data \n                    self.debug(\"An Item has been skipped for: \", url)  \n                    item = ['SKIPPED']\n                    #Taking the product name  dataframe number and index added as well as the url \n                    #to retry for later \n                    #This could take time to do so we do this at the very end after we made the cvs files\n                    self.skipped.append([product, self.count, url])\n                    break\n            if 'SKIPPED' in item:\n                #No point in cleaning skipped items\n                items = ['SKIPPED']*(self.dataFrames[product.value[0]].shape[1] - 1)\n                items.append(url)\n            else:\n                #We call the DataCleaner class to handle the cleaning of the data\n                #Its best to clean the data before we add it to the data frame\n                self.debug('Data cleaning started: ', item)\n                items = self.cleaner.cleanUp(item, url)\n                self.debug('Data cleaning finished: ', item)\n            self.debug('Extracted: ', items)\n            self.dataFrames[product.value[0]].loc[len(self.dataFrames[product.value[0]])] = items                    \n            self.count += 1\n            self.printer(product.name + \" item added \", self.count, \" of \", total, \":  \", items)\n\n    #Collecting the data from the xpath in JavaScript is faster and results in fewer errors than doing it in python\n    #This is where selenium shines because we can both use JavaScript and render JavaScript websites\n    #and is the only reason why we use it instead of scrapy\n    def javascriptXpath(self, xpath):\n        # if the time expires it assumes xpath wasnt found in the page\n        try: \n            #Waits for page to load \n            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n            elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))\n\n            # Runs the javascript and collects the text data from the inputed xpath\n            # We want to keep repeating if we get any of these outputs becasue the page is still \n            # loading and we dont want to skip or waste time. (for fast computers)\n            retrycount = 0\n            invalidOutputs = {\"error\", 'skip' \"$nan\", ''}\n            while retrycount &lt; self.maxRetryCount :\n                text = self.driver.execute_script(\"\"\"\n                    const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;\n                    if (!element) {\n                        return 'skip';\n                    }\n                    return element.textContent.trim();\n                \"\"\", \n                xpath)\n                checkText = text.replace(\" \", \"\").lower()\n                if checkText in invalidOutputs:\n                    retrycount+=1\n                else:\n                    self.log(retrycount, \"xpath attempts for (\", text, \")\")\n                    return text\n            self.log(\"xpath attempts count met. Problematic text (\" + text + \") for \", xpath)\n            return 'skip'\n        except TimeoutException:\n            self.log('Could not find xpath for: ', xpath)\n            return 'empty'\n\n           \n\n    #This is here to hopefully fix skipped data\n    #Best case sinarios this will never be used\n    def skipHandler(self, currentDate):\n        corrections = 0\n        # skipped format\n        # [product name, DataFrame number, DataFrame index, url]\n        while len(self.skipped) != 0:\n            #each skip \n            for index, dataSkip in enumerate(self.skipped):\n                product = dataSkip[0]\n                #Limiting the Attempts to fix while avoiding bottlenecking the problem\n                for attempt in range(self.attempts*2):\n                    product = dataSkip[0]\n                    url = dataSkip[2]\n                    self.driver.get(url)\n                    self.log(\"Making a request for: \", url)\n                    item = []\n                    for xpath in product.value[2]:\n                        for attemptIn in range(self.attempts*2):\n                            data = self.javascriptXpath(xpath[0])\n                            if data in {'empty', 'skip'}:   \n                                if xpath[1] and data == 'empty':    \n                                    #this is where setting the xpath to optional comes in\n                                    self.debug(\"xpath wasnt avaliable\")\n                                    item.append(None)\n                                    break\n                                self.debug(\"Missing item retrying\")\n                            else:  #Data found\n                                item.append(data)\n                                self.log(data + ' was added to the list for: ', url)\n                                break\n                        if attemptIn == self.attempts*2:\n                            data = 'skip'\n                            break\n                if data == 'skip':  #To help clean the data we skip the item with gaps of data \n                    self.debug(\"Item still missing attempting other skipped for now\") \n                else:\n                    items = self.cleaner.cleanUp(item, url)\n                    self.dataFrames[dataSkip[1]].loc[dataSkip[2]] = items                    \n                    self.printer(\"Fixed \" + product.name + \" item: \", items)\n                    #To avoid infinite loops and never saving our data we save the file now\n                    self.dataFrames[product.value[0]].to_csv(currentDate + \"REPAIRED Gateway Market \" + product.name + \".csv\")\n                    self.debug('\\n &lt; --- Total runtime with saving of repairs took %s seconds --- &gt;' % (time.time() - self.runTime))\n                    self.Logs_to_file(currentDate + self.name + ' Spider REPAIR Logs.txt')\n                    #To avoid fixing fixed items we pop, mark, and break\n                    self.skipped.pop(index)\n                    corrections += 1\n                    break\n        self.debug('\\n &lt; --- Total runtime with all repairs took %s seconds --- &gt;' % (time.time() - self.runTime))\n        self.Logs_to_file(currentDate + self.name + ' spider COMPLETED REPAIR Logs.txt')\n        \n# Start\n#DEBUG Switch\nSHOW = True\n\n#Spider setup\nspider = RussMarketSpider()\nspider.LOGGER = True\nspider.DEBUGGER = True\n\n#Running the spider\nspider.start_requests()\n\nif(SHOW):\n    [print(dataFrame) for dataFrame in spider.dataFrames]\n    spider.printLogs()"
  },
  {
    "objectID": "posts/Aaron_C_Week1/Aaron_C_Week1.html",
    "href": "posts/Aaron_C_Week1/Aaron_C_Week1.html",
    "title": "Week One",
    "section": "",
    "text": "Week One\nDuring week one my week goal is to learn as much as possible about Python and R in Datacamp\nLessons completed:\n\nIntroduction to R\nIntermediate R\nCleaning Data in R\nIntroduction to Python\nIntermediate Python\nData Manipulation with pandas\nAI Fundamentals\nGitHub Concepts\n\nAlso during week one I did a debriefing of project with the Employer."
  },
  {
    "objectID": "posts/Aaron_C_Week2/Aaron_C_Week2.html",
    "href": "posts/Aaron_C_Week2/Aaron_C_Week2.html",
    "title": "Week Two",
    "section": "",
    "text": "For week two my goal was to continue to learn in Datacamp\nLessons completed:\n\nWriting Efficient Python Code\nWeb Scraping in Python\n\nI also learned how to visualize data using Tidycensus with the American Community Survey Data"
  },
  {
    "objectID": "posts/Aaron_C_Week2/Importing Census Data.html",
    "href": "posts/Aaron_C_Week2/Importing Census Data.html",
    "title": "Importing Census Data",
    "section": "",
    "text": "Since I needed to extract a large amount of data from the American Community Survey (ASC) and convert it into a CSV file. In light of this I wrote this code to extract ASC data from the table codes. The Code takes in a list of names that you want the file name to be. Along with the corresponding table code. It also takes in a folder name. It then makes (if needed) and adds the CVS files to the specified folder (For a clean directory). Its important to note that where you run the R file is where a folder is made. I made this with the intentions of it being editable (and hopefully user friendly).\n\n####################\n#  Inserting Data  #\n####################\n\n\nfolder &lt;- \"Folder_Name_Here\" # &lt;---------- Change this value FIRST!\n\nACSList &lt;- c(\n    # \"Data Name\",\"DataCode\", \n    # ...\n  ) \nACSListToCSV(ACSList,folder)\n\n\n##########################################\n#  Global Variables That can be Changed  #\n##########################################\n\n#To change the get_acs() geography variable\ngeographyType &lt;- \"county\"\n\n#To change the get_acs() servay variable\nservayType &lt;- \"ACS5\"\n\n#Change to NULL if no state\nstateType &lt;- \"IA\"\n\n#This will make a geometry file as well if TRUE\nwithGeometry &lt;- FALSE\n\n#checking the year\nyear = NULL\n\n###################################\n#  Functions that make life easy  #\n###################################\n\n#Imports \nlibrary(tidycensus) #For ACS extractions\nlibrary(stringi) #For folderNameFixer()\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\n\n\n#File name changers. This will set the name of the file. Feel free to edit this\nrenameFile &lt;- function(tableTitle, tableCode, isGeometric){\n  fileName &lt;- paste(tableTitle, \" (\", tableCode, sep='')\n  if(isGeometric){\n    fileName = paste(fileName, \", \",capFirst(\"tract\"), sep='')\n  } else {\n    fileName = paste(fileName, \", \",capFirst(geographyType), sep='')\n  }\n  if(is.null(stateType) == FALSE){\n    fileName = paste(fileName, \", \", stateType, sep='')\n  }\n  if(is.null(year) == FALSE){\n    fileName = paste(fileName, \", \", year, sep='')\n  }\n  fileName = paste(fileName, \", \", servayType, sep='')\n  if(isGeometric){\n    fileName = paste(fileName, \", Geometry).csv\", sep='')\n  } else {\n    fileName = paste(fileName, \").csv\", sep='')\n  }\n  return(fileName)\n}\n\n#ACS extractions.\n#Feel free to add to this list.\ngeoACSDataFrame = function(tableCode){\n  get_acs(\n    #Add Changes here\n    geography = \"tract\",\n    table = tableCode,\n    servay = servayType,\n    state = stateType,\n    geometry = TRUE\n  )\n}\n\ndefaultACSDataFrame = function(tableCode){\n  get_acs(\n    #Add Changes here\n    geography = geographyType,\n    table = tableCode,\n    servay = servayType,\n    state = stateType\n  )\n}\n\n#Conditions for files fell free to edit this\nfileImplications &lt;- function(tableTitle, tableCode){\n  #Add a condition and apply both the ACS extraction and the File name changer\n  #Example\n  if(withGeometry){\n    fileName &lt;- renameFile(tableTitle, tableCode, TRUE)\n    #output with Geometry\n    dataToCSV(geoACSDataFrame(tableCode), fileName)\n  }\n  \n  fileName &lt;- renameFile(tableTitle, tableCode, FALSE)\n  #Default\n  #This Makes the CSV File\n  #format dataToCSV(your ACS DataFrame, File name changer() )\n  dataToCSV(defaultACSDataFrame(tableCode), fileName)\n}\n\n\n#For clarity capitalizes the first letter in a string and lowercases the rest (Feel free to Use)\ncapFirst = function(xStr){\n   paste(toupper(substring(xStr, 1, 1)), tolower(substring(xStr, 2, nchar(xStr))), sep = \"\")\n}\n#Validates and fixes folder name string (Feel free to edit)\nnameFixer &lt;- function(xStr, fixType){\n  #Replaces bad characters with ''\n  xStr &lt;- stri_replace_all_regex(xStr, \n                         pattern=c('/', ':', '\\\\*', '\"', '&lt;', '&gt;', '\\\\|'),\n                         replacement=c('-', '', '', '', '', '', ''),\n                         vectorize=FALSE)\n  if(xStr == \"\"){\n    if(capFirst(fixType) == \"Folder\"){\n      print(\"Ops the folder name has all bad characters lets fix that\")\n      xStr &lt;- \"New_Data_Folder\"\n    }\n    else{\n      print(\"Ops the file name has all bad characters lets fix that\")\n      xStr &lt;- \"New_Data_file\"\n    }\n  }\n  return(xStr)\n}\n#For bulk downloading tables\nACSListToCSV &lt;- function(bulkArray, folder){\n  if(length(bulkArray) %% 2){\n    #Scream if array is odd length. We don't need any mistakes!\n    print(\"Somethings Missing. List format should be like c( Data Name, DataCode, ... )\")\n    return(\"ERROR\")\n  }\n  #Validates folder name\n  folder &lt;- nameFixer(folder, \"Folder\")\n  #Makes a folder if needed\n  if(!file.exists(folder)){\n    dir.create(folder)\n    print(\"New Folder Made Adding Files\")\n  }\n  index &lt;- 1\n  #Adds files\n  while(index &lt; length(bulkArray)){\n    fileImplications(nameFixer(bulkArray[index], \"File\"), bulkArray[index+1]) # (title , code)\n    index &lt;- index + 2\n  }\n  print(\"All Files have been downloaded\")\n}\n#ACS Data frame to CSV file\ndataToCSV &lt;- function(data, fileName){\n  #This adds the file to the folder\n  path &lt;- paste(\".\\\\\",folder,\"\\\\\", fileName, sep='')\n  #Makes the CSV\n  write.csv(data, path)\n  print(paste(\"Added File:\",fileName))\n}"
  },
  {
    "objectID": "posts/Aaron_C_Week2/TidyCensusOverview.html",
    "href": "posts/Aaron_C_Week2/TidyCensusOverview.html",
    "title": "Census Visual",
    "section": "",
    "text": "Both a bar plot, and chorpleth map were made from the American Community Survey (ASC) data using Tidycensus library in R.\n\n#Imports for both Graph and Map\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(plotly)\nlibrary(ggiraph) \n\n#Grabing median_income for bar blot\nmedian_income &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  state = \"IA\", \n  year = 2021\n)\n\n#View the mid range of countys mean income\nmedian_income_data &lt;- median_income %&gt;%\n  #reducing the number of countys the graph can display\n  slice(floor(99 * 0.25):floor(99 * 0.75)+1) %&gt;%\n  #ordering from estimate highest to estimate lowest\n  arrange(desc(estimate))\n\n#The Bar plot\nmd_bar_plot &lt;- ggplot(median_income_data, aes(x = estimate, \n                                    y = reorder(NAME, estimate),\n                                    tooltip = estimate,\n                                    data_id = GEOID)) +\n  #Generating the error bars\n  geom_errorbar(aes(xmin = estimate - moe, \n                    xmax = estimate + moe),\n                    width = 0.5, \n                    size = 1) + \n  #Coloring the estimate dot \n  geom_point_interactive(color = \"darkblue\", size = 1.5) +\n  #Bottom Label range\n  scale_x_continuous(labels = label_dollar()) + \n  #County names and removing the Unnecessary words\n  scale_y_discrete(labels = function(x) str_remove(x, \" County, Iowa|, Iowa\")) +\n  #Graph labeling for views convince \n  labs(title = \"Median Income 2021 ACS\",\n       #subtitle = \"Counties in Iowa\",\n       caption = \"Data acquired with R and tidycensus. \\nError bars represent margin of error around estimates of Median income.\",\n       x = \"ACS Estimate Mean Income\",\n       y = \"Counties in Iowa\") + \n  #Text Sizing\n  theme_minimal(base_size = 8)\n#Making the graph interactive\nmd_bar_plot_interactive &lt;- girafe(ggobj = md_bar_plot) %&gt;% girafe_options(opts_hover(css = \"fill:purple;\"))\n\n#The Map\nmedian_income_map &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  state = \"IA\",\n  year = 2021,\n  geometry = TRUE\n)\n# The Map\nmd_chorpleth_map &lt;- ggplot(median_income_map, aes(fill = estimate)) + \n  #The map display\n  geom_sf() + \n  #Empty theme of the map\n  theme_void() + \n  #Colors the map \n  scale_fill_viridis_c(option = \"G\", n.breaks = 10) + \n  #Information\n  labs(title = \"Median Income by Census track\",\n       subtitle = \"\",\n       fill = \"ACS estimates\",\n       caption = \"Median Income by ACS tidycensus R package in 2021\")\n\n\n#This renders the bar plot \nmd_bar_plot_interactive\n\n#This renders the chorpleth map\nmd_chorpleth_map\n\n\n\n\nBar Plot\n\n\n\n\n\nChorpleth Map"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Aaron_C_Week3.html",
    "href": "posts/Aaron_C_Week3/Aaron_C_Week3.html",
    "title": "Week Three",
    "section": "",
    "text": "This week was dedicated to collecting data and doing research for our project. It was also spent to learn how to webscrapers (Spiders). At the end of the week was tasked to presented the Week three wrap up for our team.\nData Camp Lessons completed:\n\nWriting Efficient R Code\n\nWeek Three Research\nWeek Three Spider\nWeek Three Wrap Up"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html",
    "href": "posts/Aaron_C_Week3/Week3_Research.html",
    "title": "Week Three Research",
    "section": "",
    "text": "Farmers Market Prices:\nAt the 2015 PFI Annual Conference, Kay Jensen collected farmers’ vegetable prices. Recent data shows price ranges for tomatoes, ranging from $2.50 to $4 per pound.\nExpatistan:\nTells the price of 1 kg (2 lb.) of tomatoes in Des Moines IA can be used to find other prices located in IA.\nAgMRC:\nProvides a some detailed and helpful references for tomatoes.\nMarket Maker:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nLocal Harvest:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nPractical farmers:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nInstacart:\nA cite that could be of use to find Items.\nNFMD:\nMarket on Central - Farmers Market in Historic Downtown Fort Dodge\nNational Retail Report - Specialty Crops ( USDA Fruits & Vegetables Market Report ):\nAdvertised Prices for Specialty Crops at Major Retail Supermarket Outlets it contains Heirloom Tomatoes but only specifies the Midwest U.S.\nFRED Economic Data:\nContains the average Price of Tomatoes( Tomatoes, Field Grown (Cost per Pound/453.6 Grams) in U.S. City Average) All fresh field grown and vine ripened round red tomatoes. Includes organic and non-organic.\nFarmers Market Nutrition Program:\nLists farmers markets in Iowa\nIowa Food Cooperative (Iowa Food Coop):\nContains some data that could be useful\nNASS Eggs:\nContains tons of data that could be useful\nList of Egg Farms in Iowa:\ncould be useful to source eggs"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html#some-helpful-data-found",
    "href": "posts/Aaron_C_Week3/Week3_Research.html#some-helpful-data-found",
    "title": "Week Three Research",
    "section": "",
    "text": "Farmers Market Prices:\nAt the 2015 PFI Annual Conference, Kay Jensen collected farmers’ vegetable prices. Recent data shows price ranges for tomatoes, ranging from $2.50 to $4 per pound.\nExpatistan:\nTells the price of 1 kg (2 lb.) of tomatoes in Des Moines IA can be used to find other prices located in IA.\nAgMRC:\nProvides a some detailed and helpful references for tomatoes.\nMarket Maker:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nLocal Harvest:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nPractical farmers:\nA potential source to find and locate local farmers that grow Heirloom Tomatoes.\nInstacart:\nA cite that could be of use to find Items.\nNFMD:\nMarket on Central - Farmers Market in Historic Downtown Fort Dodge\nNational Retail Report - Specialty Crops ( USDA Fruits & Vegetables Market Report ):\nAdvertised Prices for Specialty Crops at Major Retail Supermarket Outlets it contains Heirloom Tomatoes but only specifies the Midwest U.S.\nFRED Economic Data:\nContains the average Price of Tomatoes( Tomatoes, Field Grown (Cost per Pound/453.6 Grams) in U.S. City Average) All fresh field grown and vine ripened round red tomatoes. Includes organic and non-organic.\nFarmers Market Nutrition Program:\nLists farmers markets in Iowa\nIowa Food Cooperative (Iowa Food Coop):\nContains some data that could be useful\nNASS Eggs:\nContains tons of data that could be useful\nList of Egg Farms in Iowa:\ncould be useful to source eggs"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html#heirloom-tomatoes",
    "href": "posts/Aaron_C_Week3/Week3_Research.html#heirloom-tomatoes",
    "title": "Week Three Research",
    "section": "Heirloom Tomatoes",
    "text": "Heirloom Tomatoes\nPlaces that might use Heirloom Tomatoes\n\nJava House\n\nStores that sell Heirloom Tomatoes that we can collect data from\n\nGateway market\nRuss’s Market\nFresh thyme\nHy-Vee\nNew Pioneer coop\n\nInvestigated Stores\n\nDogpatch: They have seeds but no tomatoes for sale\nFairway: They don’t have any heirloom tomatoes for sale\nWalmart: They have seeds but no tomatoes for sale\nTarget: They don’t have any heirloom tomatoes for sale\nCostco: They don’t have any heirloom tomatoes for sale\nWhole Foods Market: They don’t have any heirloom tomatoes for sale in Iowa\nAldi: They don’t have any heirloom tomatoes for sale\nTrader Joe’s: They don’t have any heirloom tomatoes for sale\nCampbell’s Nutrition: They don’t have any heirloom tomatoes for sale\nRamsey’s Market: They don’t have any heirloom tomatoes for sale\nGoPuff: They don’t have any heirloom tomatoes for sale\nOver 150+ Other Websites searched"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html#egg",
    "href": "posts/Aaron_C_Week3/Week3_Research.html#egg",
    "title": "Week Three Research",
    "section": "EGG",
    "text": "EGG\nStores that sell Eggs that we can collect data from\n\nGateway market\nRuss’s Market\nFresh Thyme Market\nHv-Vee\nNew Pioneer coop\ndogpatch\nFairway\nwalmart\nTarget\nWhole Foods Market\nAldi\nTrader Joe’s\nRamsey’s Market\nGoPuff"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Research.html#bacon",
    "href": "posts/Aaron_C_Week3/Week3_Research.html#bacon",
    "title": "Week Three Research",
    "section": "Bacon",
    "text": "Bacon\nStores that sell Bacon that we can collect data from\n\nGateway market\nRuss’s Market\nFresh Thyme Market\nHv-Vee\nNew Pioneer coop\ndogpatch\nFairway\nWalmart\nTarget\nCostco\nWhole Foods Market\nAldi\nTrader Joe’s\nRamsey’s Market\nGoPuff"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_Spider.html",
    "href": "posts/Aaron_C_Week3/Week3_Spider.html",
    "title": "Week Three Spiders",
    "section": "",
    "text": "During week the I started to developing spiders to extract data from webpages. This example is capable from extracting data from the privided weblink and putting the data into a pandas data frame. However this example is nowhere near finished.\n\nimport pandas as pd\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.log import configure_logging\n\nclass FreshThymeBaconSpider(scrapy.Spider):\n    name = 'Fresh Thyme Market Bacon Spider'\n\n    def start_requests( self ):\n        start_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage']\n        for url in start_urls:\n            yield scrapy.Request( url = url, callback = self.cardsParse)\n    \n    def cardsParse(self, response):\n        #Fail safe for links\n        try:\n            #grabs all cards from list and saves the link to follow\n            xpath = '//*[contains(@class,\"Listing\")]/div/a/@href'\n            listCards = response.xpath(xpath)\n            linklist.append(listCards.extract())\n            for url in listCards:\n                yield response.follow( url = url, callback = self.itemParse, meta={'link': url} )\n        except AttributeError:\n           pass\n    \n    def itemParse(self, response):\n        #xpaths to the name and price\n        nameXpath = '//*[contains(@class, \"PdpInfoTitle\")]/text()'\n        priceXpath = '//*[contains(@class, \"PdpMainPrice\")]/text()'\n        url = response.meta.get('link')\n        #Grabs the name and price from the xpaths and adds them to the bacon list\n        bacon.append({'bacon': response.xpath(nameXpath).extract(), 'price': response.xpath(priceXpath).extract()})\n\n# Start\nconfigure_logging()\nbacon = []\nlinklist = []\nprocess = CrawlerProcess()\nprocess.crawl(FreshThymeBaconSpider)\nprocess.start()\nprocess.stop()\nbaconFrame = pd.DataFrame(bacon)\nprint(baconFrame)\n\n2023-06-02 13:59:15 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: scrapybot)\n\n\n2023-06-02 13:59:15 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1s  1 Nov 2022), cryptography 37.0.1, Platform Windows-10-10.0.19044-SP0\n\n\n2023-06-02 13:59:15 [scrapy.crawler] INFO: Overridden settings:\n{}\n\n\n2023-06-02 13:59:15 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n\n\n2023-06-02 13:59:15 [scrapy.extensions.telnet] INFO: Telnet Password: c5df1fb3cf48c019\n\n\n2023-06-02 13:59:15 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.logstats.LogStats']\n\n\n2023-06-02 13:59:16 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n\n\n2023-06-02 13:59:16 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n\n\n2023-06-02 13:59:16 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n\n\n2023-06-02 13:59:16 [scrapy.core.engine] INFO: Spider opened\n\n\n2023-06-02 13:59:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n\n2023-06-02 13:59:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n\n\n2023-06-02 13:59:17 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage&gt; (referer: None)\n\n\n2023-06-02 13:59:18 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/boars-head-uncured-turkey-bacon-00042421130669&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:18 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/fresh-thyme-smoked-sliced-uncured-bacon-00841330117590&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:18 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/farm-promise-smoked-applewood-uncured-bacon-00070919021595&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:18 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/deli-sliced-thick-cut-bacon-in-bag-00201120000004&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:19 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/farm-promise-smoked-hardwood-uncured-bacon-00070919021601&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:19 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/deli-sliced-thick-cut-applewood-bacon-in-bag-00201121000003&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:19 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/applegate-naturals-uncured-hickory-smoked-sunday-bacon-00025317101004&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:19 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/deli-sliced-thick-cut-peppered-bacon-00201267000004&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:19 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/deli-sliced-thick-cut-applewood-bacon-00201249000008&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:19 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/boars-head-canadian-style-uncured-bacon-00042421500875&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:19 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/fresh-thyme-smoked-sliced-uncured-turkey-bacon-00841330117606&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:20 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/deli-sliced-thick-cut-bacon-00201247000000&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:20 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/boars-head-hot-smoked-uncured-sausage-00042421003659&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:20 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/pedersons-natural-farms-organic-no-sugar-uncured-smoked-bacon-00641227601405&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:20 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/hatfield-hardwood-smoked-uncured-bacon-00070919021564&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:20 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/boars-head-fully-cooked-traditional-bacon-00042421014808&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:20 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/boars-head-naturally-smoked-traditional-bacon-00042421225792&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:20 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/boars-head-breakfast-turkey-sausage-00042421130799&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:20 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/boars-head-bratwurst-00042421246803&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:20 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/boars-head-breakfast-chicken-sausage-links-00042421140286&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:20 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/boars-head-all-natural-honeycrisp-apple-chicken-sausages-00042421140187&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:20 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/boars-head-all-natural-robust-italian-chicken-sausages-00042421140132&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:21 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/boars-head-all-natural-chorizo-chicken-sausages-00042421140255&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:21 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/boars-head-all-natural-breakfast-pork-sausage-00042421140293&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:21 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/bilinskis-organic-wild-mushrooms-with-italian-herbs-chicken-sausage-00071728071207&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:21 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/bilinskis-organic-spinach-&-spring-greens-chicken-sausage-00071728071009&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:21 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/bilinskis-antibiotic-free-tomato-basil-&-chicken-zesty-italian-style-blended-sausage-00071728070057&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:21 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/bilinskis-organic-mild-italian-with-bell-peppers-chicken-sausage-00071728071153&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:21 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/bilinskis-antibiotic-free-spinach-&-chicken-garlic-seasoned-blended-sausage-00071728070002&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:21 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/bilinskis-antibiotic-free-cajun-style-chicken-&-peppers-blended-sausage-00071728070408&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:21 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/beyond-meat-beyond-sausage-plantbased-original-brat-links-00852629004774&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:21 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/beyond-meat-beyond-sausage-hot-italian-style-plantbased-sausages-00852629004750&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:22 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/applegate-naturals-uncured-turkey-hot-dogs-00025317775205&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:22 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/beddar-cheddar-00077782023930&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:22 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/applegate-naturals-uncured-beef-hot-dog-00025317775304&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:22 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/applegate-naturals-chicken-&-sage-breakfast-sausage-00025317006972&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:22 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/applegate-naturals-classic-pork-breakfast-sausage-00025317693004&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:22 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/applegate-naturals-gluten-free-uncured-beef-corn-dogs-00025317007214&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:22 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/applegate-naturals-do-good-dog-uncured-beef-hot-dogs-6ct-00025317218603&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:22 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/applegate-great-organic-uncured-beef-hot-dog-00025317775700&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:23 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/apple-bulk-chicken-sausage-00201465000004&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:23 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/andouille-sausage-00077782026979&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:23 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/pedersons-natural-farms-no-sugar-uncured-hickory-bacon-00641227301404&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:23 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/indiana-kitchen-hardwood-smoked-thick-cut-bacon-00759199801249&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:23 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/deli-sliced-thick-cut-peppered-bacon-in-bag-00201122000002&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:23 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/indiana-kitchen-hardwood-smoked-bacon-00759199801171&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:23 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/fresh-thyme-fully-cooked-uncured-turkey-bacon-00841330115077&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:23 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://ww2.freshthyme.com/sm/planning/rsid/951/product/applegate-organics-uncured-hickory-smoked-turkey-bacon-00025317122009&gt; (referer: https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage)\n\n\n2023-06-02 13:59:23 [scrapy.core.engine] INFO: Closing spider (finished)\n\n\n2023-06-02 13:59:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 21280,\n 'downloader/request_count': 49,\n 'downloader/request_method_count/GET': 49,\n 'downloader/response_bytes': 15558006,\n 'downloader/response_count': 49,\n 'downloader/response_status_count/200': 49,\n 'elapsed_time_seconds': 6.895619,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2023, 6, 2, 18, 59, 23, 901878),\n 'httpcompression/response_bytes': 46584003,\n 'httpcompression/response_count': 49,\n 'log_count/DEBUG': 50,\n 'log_count/INFO': 10,\n 'request_depth_max': 1,\n 'response_received_count': 49,\n 'scheduler/dequeued': 49,\n 'scheduler/dequeued/memory': 49,\n 'scheduler/enqueued': 49,\n 'scheduler/enqueued/memory': 49,\n 'start_time': datetime.datetime(2023, 6, 2, 18, 59, 17, 6259)}\n\n\n2023-06-02 13:59:23 [scrapy.core.engine] INFO: Spider closed (finished)\n\n\n                                                bacon            price\n0       [Boar's Head Uncured Turkey Bacon - 12 Ounce]          [$6.99]\n1   [Fresh Thyme Smoked Sliced Uncured Bacon - 12 ...          [$6.49]\n2   [Farm Promise Smoked Applewood Uncured Bacon -...          [$6.99]\n3      [Deli Sliced Thick Cut Bacon In Bag - 3 Pound]  [$17.97 avg/ea]\n4   [Farm Promise Smoked Hardwood Uncured Bacon - ...          [$6.99]\n5   [Deli Sliced Thick Cut Applewood Bacon In Bag ...  [$17.97 avg/ea]\n6   [Applegate Naturals Uncured Hickory Smoked Sun...          [$6.49]\n7    [Deli Sliced Thick Cut Peppered Bacon - 1 Pound]   [$5.99 avg/ea]\n8   [Deli Sliced Thick Cut Applewood Bacon - 1 Pound]   [$5.99 avg/ea]\n9   [Boar's Head Canadian Style Uncured Bacon - 6 ...          [$5.99]\n10  [Fresh Thyme Smoked Sliced Uncured Turkey Baco...          [$4.69]\n11            [Deli Sliced Thick Cut Bacon - 1 Pound]   [$5.99 avg/ea]\n12  [Boar's Head Hot Smoked Uncured Sausage - 16 O...          [$7.99]\n13  [Pederson's Natural Farms Organic No Sugar Unc...          [$7.49]\n14  [Hatfield Hardwood Smoked Uncured Bacon - 16 O...          [$6.99]\n15  [Boar's Head Fully Cooked Traditional Bacon - ...          [$7.49]\n16  [Boar's Head Naturally Smoked Traditional Baco...          [$7.99]\n17   [Boar's Head Breakfast Turkey Sausage - 6 Ounce]          [$4.99]\n18                 [Boar's Head Bratwurst - 16 Ounce]          [$8.29]\n19  [Boar's Head Breakfast Chicken Sausage Links -...          [$5.99]\n20  [Boar's Head All Natural Honeycrisp Apple Chic...          [$7.99]\n21  [Boar's Head All Natural Robust Italian Chicke...          [$8.99]\n22  [Boar's Head All Natural Chorizo Chicken Sausa...          [$8.99]\n23  [Boar's Head All Natural Breakfast Pork Sausag...          [$4.99]\n24  [Bilinski's Organic Wild Mushrooms With Italia...          [$7.49]\n25  [Bilinski's Organic Spinach & Spring Greens Ch...          [$7.49]\n26  [Bilinski's Antibiotic Free Tomato Basil & Chi...          [$4.49]\n27  [Bilinski's Organic Mild Italian With Bell Pep...          [$7.49]\n28  [Bilinski's Antibiotic Free Spinach & Chicken ...          [$4.49]\n29  [Bilinski's Antibiotic Free Cajun Style Chicke...          [$4.49]\n30  [Beyond Meat Beyond Sausage Plant-based Origin...          [$6.99]\n31  [Beyond Meat Beyond Sausage Hot Italian Style ...          [$6.99]\n32  [Applegate Naturals Uncured Turkey Hot Dogs - ...          [$5.99]\n33                        [Beddar Cheddar - 14 Ounce]          [$4.99]\n34  [Applegate Naturals Uncured Beef Hot Dog - 10 ...          [$5.79]\n35  [Applegate Naturals Chicken & Sage Breakfast S...          [$4.99]\n36  [Applegate Naturals Classic Pork Breakfast Sau...          [$4.99]\n37  [Applegate Naturals Gluten Free Uncured Beef C...          [$6.99]\n38  [Applegate Naturals Do Good Dog Uncured Beef H...          [$7.99]\n39  [Applegate Great Organic Uncured Beef Hot Dog ...          [$8.99]\n40             [Apple Bulk Chicken Sausage - 1 Pound]   [$3.99 avg/ea]\n41                   [Andouille Sausage - 13.5 Ounce]          [$4.99]\n42  [Pederson's Natural Farms No Sugar Uncured Hic...          [$7.49]\n43  [Indiana Kitchen Hardwood Smoked Thick Cut Bac...          [$5.49]\n44  [Deli Sliced Thick Cut Peppered Bacon In Bag -...  [$17.97 avg/ea]\n45  [Indiana Kitchen Hardwood Smoked Bacon - 16 Ou...          [$5.49]\n46  [Fresh Thyme Fully Cooked Uncured Turkey Bacon...          [$3.99]\n47  [Applegate Organics Uncured Hickory Smoked Tur...          [$7.49]\n\n\n\n\n\nOutput"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_WrapUp.html",
    "href": "posts/Aaron_C_Week3/Week3_WrapUp.html",
    "title": "AI/Local Food Week 3 Wrap Up",
    "section": "",
    "text": "The currents project objectives for this week was to\n\nCatch up on any additional training.\nCollect and find data on heirloom tomatoes, eggs, and bacon.\nLearning how to do web scraping in python through Datacamp.\nBuilding programs to do web scraping"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_WrapUp.html#current-project-objectives",
    "href": "posts/Aaron_C_Week3/Week3_WrapUp.html#current-project-objectives",
    "title": "AI/Local Food Week 3 Wrap Up",
    "section": "",
    "text": "The currents project objectives for this week was to\n\nCatch up on any additional training.\nCollect and find data on heirloom tomatoes, eggs, and bacon.\nLearning how to do web scraping in python through Datacamp.\nBuilding programs to do web scraping"
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_WrapUp.html#works-in-progress",
    "href": "posts/Aaron_C_Week3/Week3_WrapUp.html#works-in-progress",
    "title": "AI/Local Food Week 3 Wrap Up",
    "section": "Works in Progress",
    "text": "Works in Progress\nAn excel sheet that contains a list of small businesses of Iowa grocers that we had to go through and find places that had the data we wanted.\n\n\nThese are some examples of what we were looking for\n\n\nAlong with some manual data scraping. We started work on some data scraping programs (spiders). This code block is a spider that I recently created. However, there are still some improvements that still need to be made.\n\n#python\nimport pandas as pd\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.log import configure_logging\n\nclass FreshThymeBaconSpider(scrapy.Spider):\n    name = 'Fresh Thyme Market Bacon Spider'\n\n    def start_requests( self ):\n        start_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage']\n        for url in start_urls:\n            yield scrapy.Request( url = url, callback = self.cardsParse)\n    \n    def cardsParse(self, response):\n        #Fail safe for links\n        try:\n            #grabs all cards from list and saves the link to follow\n            xpath = '//*[contains(@class,\"Listing\")]/div/a/@href'\n            listCards = response.xpath(xpath)\n            linklist.append(listCards.extract())\n            for url in listCards:\n                yield response.follow( url = url, callback = self.itemParse, meta={'link': url} )\n        except AttributeError:\n           pass\n    \n    def itemParse(self, response):\n        #xpaths to the name and price\n        nameXpath = '//*[contains(@class, \"PdpInfoTitle\")]/text()'\n        priceXpath = '//*[contains(@class, \"PdpMainPrice\")]/text()'\n        url = response.meta.get('link')\n        #Grabs the name and price from the xpaths and adds them to the bacon list\n        bacon.append({'bacon': response.xpath(nameXpath).extract(), 'price': response.xpath(priceXpath).extract()})\n\n# Start\nconfigure_logging()\nbacon = []\nlinklist = []\nprocess = CrawlerProcess()\nprocess.crawl(FreshThymeBaconSpider)\nprocess.start()\nprocess.stop()\nbaconFrame = pd.DataFrame(bacon)\nprint(baconFrame)\n\nThis image shows an example output of data we were able to scrape."
  },
  {
    "objectID": "posts/Aaron_C_Week3/Week3_WrapUp.html#dspg-questions",
    "href": "posts/Aaron_C_Week3/Week3_WrapUp.html#dspg-questions",
    "title": "AI/Local Food Week 3 Wrap Up",
    "section": "DSPG Questions",
    "text": "DSPG Questions\n\nAre there stores or market places that would be helpful for us to look into?\nIs anyone experienced in web scraping and if so there any advice that you have for us?"
  },
  {
    "objectID": "posts/Aaron_C_Week4/Aaron_C_Week4.html",
    "href": "posts/Aaron_C_Week4/Aaron_C_Week4.html",
    "title": "Week Four",
    "section": "",
    "text": "During week four I participated in the collection of housing data in Slater Iowa for the WINVEST project. However the majority of the week was solely dedicated to learning and building spiders. At the end of the week I finished building three separate spiders though there are many improvements that need to be made. The Code for each is below. Along with some of there outputs\nFor a detailed explanation as well as my endeavors the link below will direct you to what I’ve learn.\nSpiders\nThis spider was build using scrapy\n\n# python\n\nfrom datetime import datetime\nimport pandas as pd\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.log import configure_logging\n\nclass FreshThymeSpider(scrapy.Spider):\n    name = 'Fresh Thyme Market Spider'\n\n    def start_requests( self ):\n        #Bacon Scraper part\n        bacon_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage',\n                      'https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage']\n        for url in bacon_urls:\n            yield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'bacon', 'url': url})\n\n        #Egg Scraper part\n        egg_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Eggs&take=48&f=Category%3AEggs',\n                      'https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Eggs&take=48&f=Category%3AEggs']\n        for url in egg_urls:\n            yield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'egg', 'url': url})\n\n        #Heirloom Tomatoes part\n        tomato_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=heirloom%20tomatoes',\n                       'https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=heirloom%20tomatoes']\n\n        for url in tomato_urls:\n            yield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'tomato', 'url': url})\n\n    def cardsParse(self, response):\n        #Failsafe for links\n        try:\n            #grabs the store location\n            storeXpath = '//*[contains(@class,\"HeaderSubtitle\")]/text()'\n            store = response.xpath(storeXpath).extract_first()\n            #grabs all cards from list and saves the link to follow\n            xpath = '//*[contains(@class,\"Listing\")]/div/a/@href'\n            listCards = response.xpath(xpath)\n            for url in listCards:\n                yield response.follow( url = url, callback = self.itemParse, meta={'store': store, 'type': response.meta.get('type'), 'url': response.meta.get('url')} )\n        except AttributeError:\n           pass\n    \n    def itemParse(self, response):\n        #xpaths to extract \n        nameXpath = '//*[contains(@class, \"PdpInfoTitle\")]/text()'\n        priceXpath = '//*[contains(@class, \"PdpMainPrice\")]/text()'\n        unitPriceXpath = '//*[contains(@class, \"PdpPreviousPrice\")]/text()'\n        prevPriceXpath = '//*[contains(@class, \"PdpUnitPrice\")]/text()'\n        #Adding the data to data frame\n        itemType = response.meta.get('type')\n        if(itemType == \"bacon\"):\n            baconFrame.loc[len(baconFrame)] = [response.xpath(nameXpath).extract_first(),\n                                               response.xpath(priceXpath).extract_first(), \n                                               response.xpath(unitPriceXpath).extract_first(), \n                                               response.xpath(prevPriceXpath).extract_first(), \n                                               response.meta.get('store'),\n                                               response.meta.get('url')]\n        elif(itemType == \"egg\"):\n            eggFrame.loc[len(eggFrame)] = [response.xpath(nameXpath).extract_first(),\n                                           response.xpath(priceXpath).extract_first(), \n                                           response.xpath(prevPriceXpath).extract_first(), \n                                           response.meta.get('store'),\n                                           response.meta.get('url')]\n        elif(itemType == \"tomato\"):\n            tomatoFrame.loc[len(tomatoFrame)] = [response.xpath(nameXpath).extract_first(),\n                                                 response.xpath(priceXpath).extract_first(), \n                                                 response.xpath(prevPriceXpath).extract_first(), \n                                                 response.meta.get('store'),\n                                                 response.meta.get('url')]\n\n# Start\n#DEBUG Switch\nDEBUG = 0\n\n#Data frames\nbaconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Unit Price', 'Sale', 'Store Location', 'Url'])\neggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Sale', 'Store Location', 'Url'])\ntomatoFrame = pd.DataFrame(columns=['Heirloom Tomato', 'Current Price', 'Sale', 'Store Location', 'Url'])\n\nif(DEBUG):\n    #To see the inner mechanics of the spider\n    configure_logging()\n\n#This is to start the spider\nprocess = CrawlerProcess()\nprocess.crawl(FreshThymeSpider)\nprocess.start()\nprocess.stop()\n\nif(DEBUG):\n    #To see the outputs\n    print(baconFrame)\n    print(eggFrame)\n    print(tomatoFrame)\n\n#Adds the date that the data was scraped\ncurrentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n#To CSV files\nbaconFrame.to_csv(currentDate + \"Fresh Thyme Bacon.csv\")\neggFrame.to_csv(currentDate + \"Fresh Thyme Egg.csv\")\ntomatoFrame.to_csv(currentDate + \"Fresh Thyme Heirloom Tomatoes.csv\")\n\nOutput of the Fresh Thyme Market Spider\nThis is the scraper for Hyvee made using selenium python package\n\n#python\n\n#Imports\nfrom datetime import datetime\nimport pandas as pd\n#Imports for Scraping\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.service import Service as FirefoxService\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import StaleElementReferenceException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom os import path\nimport time\n\n\nclass HyveeSpider():\n    name = \"Hyvee Spider\"\n    baconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Sale', 'Weight', 'Url'])\n    eggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Sale', 'Amount', 'Url'])\n    tomatoFrame = pd.DataFrame(columns=['Heirloom Tomato', 'Current Price', 'Sale', 'Weight', 'Url'])\n    \n    def __init__(self):\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n        self.baconUrls = ['https://www.hy-vee.com/aisles-online/p/11315/Hormel-Black-Label-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/47128/Hormel-Black-Label-Fully-Cooked-Original-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/41626/Applegate-Naturals-Uncured-Sunday-Bacon-Hickory-Smoked',\n                     'https://www.hy-vee.com/aisles-online/p/57278/HyVee-Double-Smoked-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2405550/Applegate-Naturals-No-Sugar-Uncured-Bacon-Hickory-Smoked',\n                     'https://www.hy-vee.com/aisles-online/p/57279/HyVee-Sweet-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/11366/Hormel-Black-Label-Original-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2455081/Jimmy-Dean-Premium-Hickory-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/3595492/Farmland-Bacon-Double-Smoked-Double-Thick-Cut',\n                     'https://www.hy-vee.com/aisles-online/p/47117/Hormel-Black-Label-Center-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/57277/HyVee-Center-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2199424/Country-Smokehouse-Thick-Applewood-Slab-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/77228/Hormel-Black-Label-Original-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21239/Farmland-Naturally-Hickory-Smoked-Classic-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2456254/Jimmy-Dean-Premium-Applewood-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21240/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/47159/Hormel-Black-Label-Original-Bacon-4Pk',\n                     'https://www.hy-vee.com/aisles-online/p/50315/Oscar-Mayer-Naturally-Hardwood-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/50321/Oscar-Mayer-Center-Cut-Original-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/50316/Oscar-Mayer-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2199421/Country-Smokehouse-Thick-Hickory-Smoked-Slab-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/104466/Hickory-Country-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/23975/HyVee-Hickory-House-Applewood-Naturally-Smoked-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/23949/HyVee-Sweet-Smoked-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/23963/HyVee-Fully-Cooked-Hickory-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/11173/Hormel-Black-Label-Applewood-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21317/Farmland-Naturally-Applewood-Smoked-Classic-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21238/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon-Package',\n                     'https://www.hy-vee.com/aisles-online/p/23948/HyVee-Lower-Sodium-Sweet-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/458259/Wright-Naturally-Hickory-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/11384/Hormel-Natural-Choice-Uncured-Original-Bacon-12-oz',\n                     'https://www.hy-vee.com/aisles-online/p/2476490/Jimmy-Dean-FC-Hickory-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/1646677/Smithfield-Hometown-Original-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/53849/Farmland-Naturally-Hickory-Smoked-Lower-Sodium-Classic-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/47121/Hormel-Black-Label-Maple-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/164627/Oscar-Mayer-Fully-Cooked-Original-Bacon-252-oz-Box',\n                     'https://www.hy-vee.com/aisles-online/p/23974/HyVee-Hickory-House-Hickory-Smoked-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/50319/Oscar-Mayer-Selects-Smoked-Uncured-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2471760/Jimmy-Dean-FC-Applewood-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/16239/Oscar-Mayer-Center-Cut-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2214511/Hormel-Black-Label-Original-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/1008152/Wright-Naturally-Smoked-Applewood-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/1813260/Smithfield-Naturally-Hickory-Smoked-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/23976/HyVee-Hickory-House-Peppered-Naturally-Smoked-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21320/Farmland-Naturally-Applewood-Smoked-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21253/Farmland-Naturally-Hickory-Smoked-Extra-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/1255920/Hormel-Black-Label-Cherrywood-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/57304/HyVee-Blue-Ribbon-Maple-Naturally-Smoked-Thick-Sliced-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21252/Farmland-Naturally-Hickory-Smoked-30-Less-Fat-Center-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2501872/Bourbon-And-Brown-Sugar-Slab-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/2516586/Hormel-Natural-ChoiceOriginal-Thick-Cut-Uncured-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/21319/Farmland-Naturally-Hickory-Smoked-Double-Smoked-Classic-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/317829/Des-Moines-Bacon-And-Meat-Company-Hardwood-Smoked-Uncured-Country-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/1255919/Hormel-Black-Label-Jalapeno-Thick-Cut-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/3538865/Oscar-Mayer-Bacon-Thick-Cut-Applewood',\n                     'https://www.hy-vee.com/aisles-online/p/317830/Des-Moines-Bacon-And-Meat-Company-Applewood-Smoked-Bacon',\n                     'https://www.hy-vee.com/aisles-online/p/3308731/Oscar-Mayer-Natural-Fully-Cooked-Uncured-Bacon'\n                     ]\n        self.eggsUrls = ['https://www.hy-vee.com/aisles-online/p/57236/HyVee-Grade-A-Large-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/23899/HyVee-Grade-A-Large-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/715446/Farmers-Hen-House-Free-Range-Organic-Large-Brown-Grade-A-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/2849570/Thats-Smart-Large-Shell-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/31351/Farmers-Hen-House-Free-Range-Grade-A-Large-Brown-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/23900/HyVee-Grade-A-Extra-Large-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/71297/Egglands-Best-Farm-Fresh-Grade-A-Large-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/36345/Egglands-Best-Grade-A-Large-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/3192325/HyVee-Free-Range-Large-Brown-Egg-Grade-A',\n                    'https://www.hy-vee.com/aisles-online/p/23903/HyVee-Grade-A-Jumbo-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/3192323/HyVee-Cage-Free-Large-Brown-Egg-Grade-A',\n                    'https://www.hy-vee.com/aisles-online/p/36346/Egglands-Best-Cage-Free-Brown-Grade-A-Large-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/3192322/HyVee-Cage-Free-Large-Brown-Egg-Grade-A',\n                    'https://www.hy-vee.com/aisles-online/p/858343/HyVee-Cage-Free-Omega3-Grade-A-Large-Brown-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/1901565/Farmers-Hen-House-Pasture-Raised-Organic-Grade-A-Large-Brown-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/60364/HyVee-HealthMarket-Organic-Grade-A-Large-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/71298/Egglands-Best-Extra-Large-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/23902/HyVee-Grade-A-Extra-Large-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/453006/Egglands-Best-XL-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/2668550/HyVee-One-Step-Pasture-Raised-Large-Brown-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/66622/Farmers-Hen-House-Jumbo-Brown-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/3274825/Nellies-Eggs-Brown-Free-Range-Large',\n                    'https://www.hy-vee.com/aisles-online/p/57235/HyVee-Grade-A-Medium-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/2437128/Pete-And-Gerrys-Eggs-Organic-Brown-Free-Range-Large',\n                    'https://www.hy-vee.com/aisles-online/p/36347/Egglands-Best-Organic-Cage-Free-Grade-A-Large-Brown-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/2698224/Nellies-Free-Range-Eggs-Large-Fresh-Brown-Grade-A',\n                    'https://www.hy-vee.com/aisles-online/p/57237/HyVee-Grade-A-Large-Eggs',\n                    'https://www.hy-vee.com/aisles-online/p/190508/Farmers-Hen-House-Organic-Large-Brown-Eggs'\n                   ]\n        self.tomatoesUrls = ['https://www.hy-vee.com/aisles-online/p/37174/']\n        self.count = 0\n\n    def dataWait(self, xpath):\n        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n        element = WebDriverWait(self.driver, 3, ignored_exceptions=ignored_exceptions).until(EC.visibility_of_element_located((By.XPATH, xpath))).text\n        return element\n       \n    def dataWaitForAll(self, xpath):\n            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException,)\n            elements = WebDriverWait(self.driver, 30, ignored_exceptions=ignored_exceptions).until(EC.visibility_of_all_elements_located((By.XPATH, xpath)))\n            return len(elements)\n\n    def restart(self):\n        self.driver.close()\n        self.driver.quit()\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n\n    def start_requests( self ):\n        attemps = 3\n\n        self.count = 0\n        for trying in range(attemps):\n            try:\n                self.requestBacon()\n                print(\"Bacon Finished\")\n                break\n            except:\n                print(\"Bacon Export Failed Recovering extraction and continueing\")\n                self.restart()        \n        self.count = 0\n        for trying in range(attemps):\n            try:\n                self.requestEgg()\n                print(\"Eggs Finished\")\n                break\n            except:\n                print(\"Eggs Export Failed. Recovering extraction and continueing\")\n                self.restart()  \n        \n        self.count = 0\n        for trying in range(attemps):\n            try:\n                self.requestTomato()\n                print(\"Heirloom Tomatoes Successfully Finished\")\n                break\n            except:\n                print(\"Tomatoes Export Failed Recovering extraction and continueing\")\n                self.restart()\n                \n        self.driver.close()\n        self.driver.quit()\n\n    def requestBacon( self ):\n        total = len(self.baconUrls)\n        while self.count &lt; total:\n            url = self.baconUrls[self.count]\n            self.driver.get(url)\n            time.sleep(1) # marionette Error Fix\n            pXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p'\n            nameXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/h1'\n            priceXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[1]'\n            sale = self.dataWaitForAll(pXpath) #Ensures the page is loaded\n            name = self.dataWait(nameXpath) \n            price = self.dataWait(priceXpath)\n            if sale == 2:\n                weightXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[2]'\n                weight = self.dataWait(weightXpath)\n                self.baconFrame.loc[len(self.baconFrame)] = [name,\n                                                price,\n                                                \"False\",\n                                                weight,\n                                                url]\n            elif sale == 3:\n                prevPriceXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[2]'\n                weightXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[3]'\n                prevPrice = self.dataWait(prevPriceXpath)\n                weight = self.dataWait(weightXpath)\n                self.baconFrame.loc[len(self.baconFrame)] = [name,\n                                                price,\n                                                prevPrice,\n                                                weight,\n                                                url]\n            else:\n                # Catch if there is anything missing elements\n                self.baconFrame.loc[len(self.baconFrame)] = [\"SKIPPED\",\n                                                   \"SKIPPED\",\n                                                   \"SKIPPED\",\n                                                   \"SKIPPED\",\n                                                   url]\n            self.count += 1\n            print(\"Bacon item added\", self.count,\" of \", total, \" :  \", name)\n        self.count = 0\n\n    def requestEgg(self): \n        total = len(self.eggsUrls)\n        while self.count &lt; total:\n            url = self.eggsUrls[self.count]\n            self.driver.get(url)\n            time.sleep(1) # marionette Error Fix\n            pXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p'\n            nameXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/h1'\n            priceXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[1]'\n            sale = self.dataWaitForAll(pXpath) #Ensures the page is loaded\n            name = self.dataWait(nameXpath) \n            price = self.dataWait(priceXpath)\n            if sale == 2:\n                weightXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[2]'\n                weight = self.dataWait(weightXpath)\n                self.eggFrame.loc[len(self.eggFrame)] = [name,\n                                                price,\n                                                \"False\",\n                                                weight,\n                                                url]\n            elif sale == 3:\n                prevPriceXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[2]'\n                weightXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[3]'\n                prevPrice = self.dataWait(prevPriceXpath)\n                weight = self.dataWait(weightXpath)\n                self.eggFrame.loc[len(self.eggFrame)] = [name,\n                                                price,\n                                                prevPrice,\n                                                weight,\n                                                url]\n            else:\n                # Catch if there is anything missing elements\n                self.eggFrame.loc[len(self.eggFrame)] = [\"SKIPPED\",\n                                                   \"SKIPPED\",\n                                                   \"SKIPPED\",\n                                                   \"SKIPPED\",\n                                                   url]\n            self.count += 1\n            print(\"Eggs item added\", self.count,\" of \", total, \" :  \", name)\n        self.count = 0\n\n    def requestTomato( self ):\n        total = len(self.tomatoesUrls)\n        while self.count &lt; total:\n            url = self.tomatoesUrls[self.count]\n            self.driver.get(url)\n            time.sleep(1) # marionette Error Fix\n            pXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p'\n            nameXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/h1'\n            priceXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[1]'\n            sale = self.dataWaitForAll(pXpath) #Ensures the page is loaded\n            name = self.dataWait(nameXpath) \n            price = self.dataWait(priceXpath)\n            if sale == 2:\n                weightXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[2]'\n                weight = self.dataWait(weightXpath)\n                self.tomatoFrame.loc[len(self.tomatoFrame)] = [name,\n                                                price,\n                                                \"False\",\n                                                weight,\n                                                url]\n            elif sale == 3:\n                prevPriceXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[2]'\n                weightXpath = '//*[contains(@class, \"product-details_detailsContainer\")]/p[3]'\n                prevPrice = self.dataWait(prevPriceXpath)\n                weight = self.dataWait(weightXpath)\n                self.tomatoFrame.loc[len(self.tomatoFrame)] = [name,\n                                                price,\n                                                prevPrice,\n                                                weight,\n                                                url]\n            else:\n                # Catch if there is anything missing elements\n                self.tomatoFrame.loc[len(self.tomatoFrame)] = [\"SKIPPED\",\n                                                   \"SKIPPED\",\n                                                   \"SKIPPED\",\n                                                   \"SKIPPED\",\n                                                   url]\n            self.count += 1\n            print(\"Tomato item added\", self.count,\"of\", total, \":  \", name)\n        self.count = 0\n\n# Start\nspider = HyveeSpider()\nspider.start_requests()\n#Adds the date that the data was scraped\ncurrentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n#To CSV files\nspider.baconFrame.to_csv(currentDate + \"Hyvee Bacon.csv\")\nspider.eggFrame.to_csv(currentDate + \"Hyvee Egg.csv\")\nspider.tomatoFrame.to_csv(currentDate + \"Hyvee Heirloom Tomatoes.csv\")\n\nOutput of the Hy-Vee Spider \nThis is the scraper for Gateway market was made using selenium python package as well\n\n#python\n\n#Imports\nfrom datetime import datetime\nimport pandas as pd\nfrom enum import Enum\n#Imports for Scraping\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.service import Service as FirefoxService\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import StaleElementReferenceException\nfrom selenium.common.exceptions import WebDriverException\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom os import path\nimport time\n\n#This class is here so that we can expand to differnet products easier make the spider \n#more dynamic and expandable\nclass Products(Enum):\n    #Name = Index, URL list\n    Bacon = 1, ['https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18483',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18485',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-24190',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18553',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-33732',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18521',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18548',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18469',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-33734',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-33736',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-33731',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-29349',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18524',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-24260',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-24163',\n                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18482'\n                ]\n    Eggs = 2, ['https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-22775',\n               'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-22776',\n               'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-12603',\n              ]\n    \n    HeirloomTomatoes = 3, ['https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-11820',\n                           'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-22455',\n                           'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-11896',\n                           'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-11973',\n                           'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-22343',\n                          ]\n\nclass GatewaySpider():\n    name = \"Gateway Market Spider\"\n    baconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Original Price', 'Brand', 'Location', 'Url'])\n    eggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Original Price', 'Brand', 'Location', 'Url'])\n    tomatoFrame = pd.DataFrame(columns=['Heirloom Tomato', 'Current Price', 'Original Price', 'Brand', 'Location', 'Url'])\n    spiderLogs = []\n    skipped = []\n\n    #These are methods that are available for your convences\n    def log(self, *args):\n        self.spiderLogs.append(('Logger:', args))\n        if self.LOGGER:\n            print('Logger:', *args)\n\n    def debug(self, *args):\n        self.spiderLogs.append(('Debug:', args))\n        if self.DEBUGGER:\n            print('Debug:', *args)\n    \n    def printer(self, *args):\n        self.spiderLogs.append(('Printer:', args))\n        print(*args)\n    \n    def printLogs(self):\n        print(\"\\n&lt; --- Printing Logs --- &gt;\\n\")\n        for entry in self.spiderLogs:\n            print(*entry)\n\n    def Logs_to_file(self, filename):\n        with open(filename, 'w') as file:\n            for log_entry in self.spiderLogs:\n                file.write('{} {}\\n'.format(log_entry[0], log_entry[1]))\n    \n    def __init__(self):\n        self.DEBUGGER = False #The debugger switch to see whats going on. The Default is False\n        self.LOGGER = False #When you need to see everything that happends. The Default is False\n        self.attempts = 3 #The number of attempts the spider can retry if an error occurs. Default is 3\n        self.waitTime = 10 #The number of seconds WebDriver will wait. Default is 10\n        self.count = 0 #This saves the location of the url we are going through\n        self.runTime = 0 #Total time of extractions\n        self.totalRecoveries = 0 #Number of recoveries made while running\n        #Selenium needs a webdriver to work. I chose Firefox however you can do another if you need too\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n        self.log(\"Driver started\")\n        \n    def restart(self):\n        self.driver.close()\n        self.driver.quit()\n        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))\n        self.log(\"Driver restarted\")\n    \n    def forceQuit(self):\n        self.printer(\"Browser window was closed by user. Stopping program\")\n        self.log('\\n &lt; --- Total runtime took %s seconds with %d recoveries --- &gt;' % (time.time() - self.runTime, self.totalRecoveries))\n        self.Logs_to_file(self.name + ' Logs.txt')\n        self.driver.quit()\n\n    def requestExtraction(self, productType):\n        self.count = 0\n        errors = 0\n        extractionType = productType.value[0]\n        start = time.time()\n        for trying in range(self.attempts):\n            try:\n                if extractionType == 1:\n                    self.requestBacon()\n                elif extractionType == 2:\n                    self.requestEgg()\n                elif extractionType == 3:\n                    self.requestHeirloomTomatoes()\n                # Add elif for more products here\n                else:\n                    self.debug(\"An error extractionType for \" + str(extractionType) + \" has occured\")\n                self.debug(productType.name + \" Finished\")    \n                self.log('\\n&lt; --- ' + productType.name + ' scrape took %s seconds with %d recoveries --- &gt;\\n' % ((time.time() - start), errors))\n                self.totalRecoveries += errors\n                return self.totalRecoveries\n            except WebDriverException:\n                self.forceQuit()\n                return None\n            except Exception as e:\n                errors += 1\n                self.debug(\"An error occurred:\", e)\n                self.debug(\"Recovering extraction and continueing\")\n                self.restart() \n        self.debug(productType.name + \" Did not Finished after \" + str(self.attempts) + \" Time wasted: %s seconds\" % (time.time() - start))\n        self.totalRecoveries += errors\n        return self.totalRecoveries\n\n    def start_requests( self ):\n        self.runTime = time.time()\n        self.totalRecoveries = 0 \n        result = self.requestExtraction(Products.Bacon)\n        if(result == None): return\n        result = self.requestExtraction(Products.Eggs)\n        if(result == None): return\n        result = self.requestExtraction(Products.HeirloomTomatoes)\n        if(result == None): return\n        self.driver.close()\n        self.driver.quit()\n        #Adds the date that the data was scraped\n        currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]\n        self.log(\"Exporting files\")\n        #Dataframes to CSV files\n        self.baconFrame.to_csv(currentDate + \"Gateway Market Bacon.csv\")\n        self.eggFrame.to_csv(currentDate + \"Gateway Market Egg.csv\")\n        self.tomatoFrame.to_csv(currentDate + \"Gateway Market Heirloom Tomatoes.csv\")\n        self.log('\\n', self.baconFrame.to_string())\n        self.log('\\n', self.eggFrame.to_string())\n        self.log('\\n', self.tomatoFrame.to_string())\n        self.debug('\\n &lt; --- Total runtime took %s seconds with %d recoveries --- &gt;' % (time.time() - self.runTime, self.totalRecoveries))\n        self.debug('\\n &lt; --- Number of skips ' + str(len(self.skipped)) +' ---&gt;')\n        if len(self.skipped) != 0:\n            self.debug(self.skipped)\n        self.Logs_to_file(currentDate + self.name + ' Logs.txt')\n\n\n    \n    #This handles the xpaths \n    #most websites have simular xpaths for each item. You might need to make differnet xpaths for each item \n    #if that is the case\n    #For assigning xpaths mark them if they are optional meaning it could or could not be present on the page \n    #we do this for speed up if you mark it as non optional and its not pressent it will skip the value \n    #and hurt the preformence\n    #best practice is to render the optional last so it reduces the chances of skipping \n    def xpathMaker(self):\n        #Add the xpaths here and mark if they are optional\n        nameXpath = '//*[@id=\"item-details\"]/h1[contains(@class, \"name\")]'\n        priceXpath = '//*[@id=\"item-details\"]//*[contains(@class, \"wc-pricing\")]/div[1]'\n        prevPriceXpath = '//*[@id=\"item-details\"]//*[contains(@class, \"wc-pricing\")]/div[2]/s' # optional\n        brandXpath = '//*[@id=\"item-details\"]/div[1]' # optional\n        #xpath, Optional\n        xpathList = [(nameXpath, False),\n                     (priceXpath, False),\n                     (prevPriceXpath, True),\n                     (brandXpath, True)]\n        return xpathList\n    \n    #Collecting the data from the xpath in JavaScript is faster and results in fewer errors than doing it in python\n    def javascriptXpath(self, xpath):\n        try: \n            #Waits for page to load \n            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n            elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))\n            for quickRetry in range(self.attempts): #this is for fast computers\n                #Runs the javascript and collects the text data from the inputed xpath\n                text = self.driver.execute_script(\"\"\"\n                    const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;\n                    if (!element) {\n                        return 'Skip';\n                    }\n                    return element.textContent.trim();\n                \"\"\", \n                xpath)\n                if self.checkOutput(text):\n                    time.sleep(1)\n                else:\n                    self.log('found ', text, ' for xpath: ', xpath)\n                    return text\n        except TimeoutException:\n            #This means the xpath wasnt found in the page\n            self.log('Could not find xpath for: ', xpath)\n        return 'Empty'\n\n    def checkOutput(self, check):\n        self.log('Validing input')\n        invalidOutputs = {\"error\", 'skip', \"$nan\", ''}\n        if check.lower() in invalidOutputs:\n            self.log(\"Invalid word:\", check)\n            return True\n        else:\n            self.log(\"Valid\")\n            return False\n\n    #This handles the reqests \n    def makeRequest(self, url):\n        xpathList = self.xpathMaker()\n        self.log(\"xpath list retrieved \", xpathList)\n        item = []\n        time.sleep(1) # marionette Error Fix\n        for xpath in xpathList:\n            data = 'skip'\n            #Retrying the xpath given the number of attempts\n            for attempt in range(self.attempts):\n                data = self.javascriptXpath(xpath[0])\n                if(self.checkOutput(data)): # Data not found\n                    self.debug(\"Missing item retrying\")\n                elif data == 'Empty':     \n                    if xpath[1]:\n                        self.debug(\"xpath wasnt avaliable\")\n                        item.append(None)\n                        break\n                    self.debug(\"Missing item retrying\")\n                else:  #Data found\n                    item.append(data)\n                    self.log(data + ' was added to the list for: ', url)\n                    break\n            if data == 'skip':  #To help clean the data we skip the item with gaps of data \n                self.debug(\"An Item has been skipped for: \", url)  \n                item = ['Skipped']*(len(xpathList))\n                self.skipped.append(url)\n        return self.DataCleaning(item, url)\n    \n    def requestBacon( self ):\n        baconUrls = Products.Bacon.value[1]\n        total = len(baconUrls)\n        while self.count &lt; total:\n            url = baconUrls[self.count]\n            self.driver.get(url)\n            self.log(\"Making a request for: \", url)\n            items = self.makeRequest(url) \n            self.debug('Extracted: ', items)\n            self.baconFrame.loc[len(self.baconFrame)] = items                    \n            self.count += 1\n            self.printer(\"Bacon item added \", self.count, \" of \", total, \":  \", items)\n\n    def requestEgg(self): \n        eggsUrls = Products.Eggs.value[1]\n        total = len(eggsUrls)\n        while self.count &lt; total:\n            url = eggsUrls[self.count]\n            self.driver.get(url)\n            self.log(\"Making a request for: \", url)\n            items = self.makeRequest(url) \n            self.debug('Extracted: ', items)\n            self.eggFrame.loc[len(self.eggFrame)] = items                    \n            self.count += 1\n            self.printer(\"Egg item added \", self.count, \" of \", total, \":  \", items)\n    \n    def requestHeirloomTomatoes(self):\n        tomatoesUrls = Products.HeirloomTomatoes.value[1]\n        total = len(tomatoesUrls)\n        while self.count &lt; total:\n            url = tomatoesUrls[self.count]\n            self.driver.get(url)\n            self.log(\"Making a request for: \", url)\n            items = self.makeRequest(url) \n            self.debug('Extracted: ', items)\n            self.tomatoFrame.loc[len(self.tomatoFrame)] = items                    \n            self.count += 1\n            self.printer(\"Heirloom tomato item added \", self.count, \" of \", total, \":  \", items)\n\n    #This part is a special case for this particular spider cleaning could be implemented here\n    def DataCleaning(self, item, url):\n        self.debug('Data cleaning started: ', item)\n        item.append(\"2002 Woodland Avenue Des Moines, IA 50312\")\n        item.append(url)\n        self.debug('Data cleaning finished: ', item)\n        return item\n\n# Start\n#DEBUG Switch\nSHOW = True\nspider = GatewaySpider()\nspider.LOGGER = True\nspider.DEBUGGER = True\nspider.start_requests()\nif(SHOW):\n    print(spider.baconFrame)\n    print(spider.eggFrame)\n    print(spider.tomatoFrame)\n    spider.printLogs()\n\nOutput of the Gateway Market Spider"
  },
  {
    "objectID": "posts/Aaron_C_Week5/Aaron_C_Week5.html",
    "href": "posts/Aaron_C_Week5/Aaron_C_Week5.html",
    "title": "Week Five",
    "section": "",
    "text": "During week five I attended the ITAG VI conference. However the majority of the week was dedicated to building and optimizing spiders. At the end of the week I finished building four more spiders making seven spider built for the project. There are still many small improvements that need to be made. I also started to work on implementing a cleaning program that will clean the data while the spider runs.\nHere is a link to what I’ve learn about building spiders: Spider\nHere is a link to the code of all the modified and added spiders that was made during the week: Week 5 Spider Code"
  }
]