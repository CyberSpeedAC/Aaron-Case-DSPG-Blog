---
title: "Week Five Spider Code"
author: "Aaron Case"
date: "2023-06-16"
categories: [Week Five]
execute: 
  enabled: false
---

This post show all the code that I've written for week 5. It contains 2 examples of how I find xpaths as well as seven spider/scrapers that I have made and modified.

An example of how I find data that the xpaths have. This example is for Russ Market Spider.

```{python}
# Imports for Scraping
import selenium
from selenium import webdriver
from selenium.webdriver.firefox.service import Service as FirefoxService
from webdriver_manager.firefox import GeckoDriverManager
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import StaleElementReferenceException
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from os import path
import time

#Use this and edit this to extract data from the xpaths 
#Creators Note: if any major changes that are made you need to update the spiders javascriptXpath function since 
#this is the exact function that the spider uses to collect data from the url
def javascriptXpath (driver, xpath):
    try: 
        # Waits for page to load 
        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)
        elements = WebDriverWait(driver, waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))
        # A set of all the outputs that we want to avoid
        invalidOutputs = {"error", 'skip' "$nan", ''}
        loopCount = 0
        # Runs the javascript and collects the text data from the inputed xpath
        # We want to keep repeating if we get '' becasue the page is still loading
        while loopCount < loopRepeat:
            #Running the JavaScript
            text = driver.execute_script("""
                const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;
                if (!element) {
                    return 'skip';
                }
                return element.textContent.trim();
            """, xpath)
            checkText = text.replace(" ", "").lower()
            if checkText in invalidOutputs:
                loopCount+=1
            else:
                print(loopCount, "xpath attempts for (", text, ")")
                break
        if loopCount < loopRepeat:
            print("Success for " ,xpath)         
        else: 
            return "xpath value not found"
        return text
    except TimeoutException:
        # This means the xpath wasn't found in the page
        print('Could not find xpath for: ', xpath)
        return 'Empty'

#Add test and fix xpaths here. Use multiple urls for testing  
def testXpaths(url):
    driver.get(url)
    nameXpath = '//*[@id="page-title"]//h1[contains(@class,"fp-page-header fp-page-title")]'
    print(javascriptXpath(driver, nameXpath))
    priceXpath = '//*[@id="page-title"]//*[contains(@class,"fp-item-price")]/span[contains(@class,"fp-item-base-price")]'
    print(javascriptXpath(driver, priceXpath))
    weightXpath = '//*[@id="page-title"]//*[contains(@class,"fp-item-price")]/span[contains(@class,"fp-item-size")]' 
    print(javascriptXpath(driver, weightXpath))
    saleXpath = '//*[@id="page-title"]//*[contains(@class,"fp-item-sale")]/span[contains(@class,"fp-item-sale-date")]/strong' #optional
    print(javascriptXpath(driver, saleXpath))

# setup
waitTime = 10
driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))
storeLocationUrl = 'https://www.russmarket.com/shop#!/?store_id=6158'
driver.get(storeLocationUrl)
time.sleep(5)
print("Store location set")
loopRepeat = 100

# Successful tests
url = "https://www.russmarket.com/shop/produce/fresh_vegetables/tomatoes/heirloom_tomatoes/p/12412"
testXpaths(url)
# Successful tests
url = "https://www.russmarket.com/shop/meat/bacon/prairie_fresh_signature_applewood_smoked_bacon_pork_loin_filet_27_2_oz/p/6828650"
testXpaths(url)

driver.quit()
```

This is example of how I find data that the xpaths have. This example is for New Pioneer Co-op Spider.

```{python}
# Imports for Scraping
import selenium
from selenium import webdriver
from selenium.webdriver.firefox.service import Service as FirefoxService
from webdriver_manager.firefox import GeckoDriverManager
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import StaleElementReferenceException
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from os import path
import time

#Use this and edit this to extract data from the xpaths 
#Creators Note: if any major changes that are made you need to update the spiders javascriptXpath function since 
#this is the exact function that the spider uses to collect data from the url
def javascriptXpath (driver, xpath):
    try: 
        # Waits for page to load 
        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)
        elements = WebDriverWait(driver, waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))
        # A set of all the outputs that we want to avoid
        invalidOutputs = {"error", 'skip' "$nan", ''}
        loopCount = 0
        # Runs the javascript and collects the text data from the inputed xpath
        # We want to keep repeating if we get '' becasue the page is still loading
        while loopCount < loopRepeat:
            #Running the JavaScript
            text = driver.execute_script("""
                const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;
                if (!element) {
                    return 'skip';
                }
                return element.textContent.trim();
            """, xpath)
            checkText = text.replace(" ", "").lower()
            if checkText in invalidOutputs:
                loopCount+=1
            else:
                print(loopCount, "xpath attempts for (", text, ")")
                break
        if loopCount < loopRepeat:
            print("Success for " ,xpath)         
        else: 
            return "xpath value not found"
        return text
    except TimeoutException:
        # This means the xpath wasn't found in the page
        print('Could not find xpath for: ', xpath)
        return 'Empty'

#Add test and fix xpaths here. Use multiple urls for testing  
def testXpaths(url):
    driver.get(url)
    nameXpath = '//*[@id="products"]//*[contains(@class,"fp-item-detail")]//*[contains(@class,"fp-item-name")]'
    print(javascriptXpath(driver, nameXpath))
    priceXpath = '//*[@id="products"]//*[contains(@class,"fp-item-detail")]//*[contains(@class,"fp-item-price")]//span[contains(@class,"fp-item-base-price")]'
    print(javascriptXpath(driver, priceXpath))
    weightXpath = '//*[@id="products"]//*[contains(@class,"fp-item-detail")]//*[contains(@class,"fp-item-price")]//span[contains(@class,"fp-item-size")]'
    print(javascriptXpath(driver, weightXpath))
    saleXpath = '//*[@id="products"]//*[contains(@class,"fp-item-detail")]//*[contains(@class,"fp-item-sale")]//span[contains(@class,"fp-item-sale-price")]' # optional
    print(javascriptXpath(driver, saleXpath))

driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))
url = "https://shop.newpi.coop/shop/meat/bacon/sliced/applegate_natural_hickory_smoked_uncured_sunday_bacon_8_oz/p/19959#!/?department_id=1322093"
testXpaths(url)

url = 'https://shop.newpi.coop/shop/just_ice_tea_green_tea_original_unsweetened_16_fl_oz/p/1564405684713463723'
testXpaths(url)
```

This is the code for the Fresh Thyme Spider.

```{python}
from datetime import datetime
import pandas as pd
import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.log import configure_logging

class FreshThymeSpider(scrapy.Spider):
    name = 'Fresh Thyme Market Spider'

    def start_requests( self ):
        #Bacon Scraper part
        bacon_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage',
                      'https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Bacon&take=48&f=Category%3AHot+Dogs%2C+Bacon+%26+Sausage']
        for url in bacon_urls:
            yield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'bacon', 'url': url})

        #Egg Scraper part
        egg_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=Eggs&take=48&f=Category%3AEggs',
                      'https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=Eggs&take=48&f=Category%3AEggs']
        for url in egg_urls:
            yield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'egg', 'url': url})

        #Heirloom Tomatoes part
        tomato_urls = ['https://ww2.freshthyme.com/sm/planning/rsid/951/results?q=heirloom%20tomatoes',
                       'https://ww2.freshthyme.com/sm/planning/rsid/952/results?q=heirloom%20tomatoes']

        for url in tomato_urls:
            yield scrapy.Request( url = url, callback = self.cardsParse, meta={'type': 'tomato', 'url': url})

    def cardsParse(self, response):
        #Failsafe for links
        try:
            #grabs the store location
            storeXpath = '//*[contains(@class,"HeaderSubtitle")]/text()'
            store = response.xpath(storeXpath).extract_first()
            #grabs all cards from list and saves the link to follow
            xpath = '//*[contains(@class,"Listing")]/div/a/@href'
            listCards = response.xpath(xpath)
            for url in listCards:
                yield response.follow( url = url, callback = self.itemParse, meta={'store': store, 'type': response.meta.get('type'), 'url': response.meta.get('url')} )
        except AttributeError:
           pass
    
    def itemParse(self, response):
        #xpaths to extract 
        nameXpath = '//*[contains(@class, "PdpInfoTitle")]/text()'
        priceXpath = '//*[contains(@class, "PdpMainPrice")]/text()'
        prevPriceXpath = '//*[contains(@class, "PdpPreviousPrice")]/text()'
        unitPriceXpath = '//*[contains(@class, "PdpUnitPrice")]/text()'
        #Adding the data to data frame
        itemType = response.meta.get('type')
        if(itemType == "bacon"):
            baconFrame.loc[len(baconFrame)] = [response.xpath(nameXpath).extract_first(),
                                               response.xpath(priceXpath).extract_first(), 
                                               response.xpath(unitPriceXpath).extract_first(), 
                                               response.xpath(prevPriceXpath).extract_first(), 
                                               response.meta.get('store'),
                                               response.meta.get('url')]
        elif(itemType == "egg"):
            eggFrame.loc[len(eggFrame)] = [response.xpath(nameXpath).extract_first(),
                                           response.xpath(priceXpath).extract_first(), 
                                           response.xpath(prevPriceXpath).extract_first(), 
                                           response.meta.get('store'),
                                           response.meta.get('url')]
        elif(itemType == "tomato"):
            tomatoFrame.loc[len(tomatoFrame)] = [response.xpath(nameXpath).extract_first(),
                                                 response.xpath(priceXpath).extract_first(), 
                                                 response.xpath(prevPriceXpath).extract_first(), 
                                                 response.meta.get('store'),
                                                 response.meta.get('url')]

# Start
#DEBUG Switch
DEBUG = 0

#Data frames
baconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Unit Price', 'Sale', 'Store Location', 'Url'])
eggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Sale', 'Store Location', 'Url'])
tomatoFrame = pd.DataFrame(columns=['Heirloom Tomato', 'Current Price', 'Sale', 'Store Location', 'Url'])

if(DEBUG):
    #To see the inner mechanics of the spider
    configure_logging()

#This is to start the spider
process = CrawlerProcess()
process.crawl(FreshThymeSpider)
process.start()
process.stop()

if(DEBUG):
    #To see the outputs
    print(baconFrame)
    print(eggFrame)
    print(tomatoFrame)

#Adds the date that the data was scraped
currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]
#To CSV files
baconFrame.to_csv(currentDate + "Fresh Thyme Bacon.csv")
eggFrame.to_csv(currentDate + "Fresh Thyme Egg.csv")
tomatoFrame.to_csv(currentDate + "Fresh Thyme Heirloom Tomatoes.csv")
```

This is the code for the Iowa Food Hub Spider.

```{python}
from datetime import datetime
import pandas as pd
import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.log import configure_logging

class IowaFoodHubSpider(scrapy.Spider):
    name = 'Iowa Food Hub'
    currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]
    def start_requests( self ):

        iowaFoodHubBaconUrl = 'https://iowa-food-hub.myshopify.com/search?q=bacon'
        yield scrapy.Request( url = iowaFoodHubBaconUrl, callback = self.iowaFoodHubSearch, meta={'url': iowaFoodHubBaconUrl, 'type': 'bacon'})

        iowaFoodHubEggsUrl = 'https://iowa-food-hub.myshopify.com/search?q=Egg'
        yield scrapy.Request( url = iowaFoodHubEggsUrl, callback = self.iowaFoodHubSearch, meta={'url': iowaFoodHubEggsUrl, 'type': 'eggs'})

    def iowaFoodHubSearch(self, response):
        #Failsafe for links
        try:
            #grabs all cards from list and saves the link to follow
            xpath = '//*[@id="MainContent"]//a[contains(@class,"list-view-item")]/@href'
            linkList = response.xpath(xpath)
            productType = response.meta.get('type')
            if productType == 'bacon':
                for url in linkList:
                    yield response.follow( url = url, callback = self.iowaFoodHubBacon, meta={'url': response.meta.get('url')}, dont_filter=True )
            elif productType == 'eggs':
                for url in linkList:
                    yield response.follow( url = url, callback = self.iowaFoodHubEggs, meta={'url': response.meta.get('url')}, dont_filter=True )
        except AttributeError:
           pass

    def iowaFoodHubBacon(self, response):
        #validating the name
        nameXpath = '//*[@id="ProductSection-product-template"]//*[contains(@class, "product-single__title")]/text()'
        name = response.xpath(nameXpath).extract_first()
        desiredNames = {"bacon"}
        if not self.containsWord(name, desiredNames):
            return 
        #The other areas we are interested in
        venderXpath = '//*[@id="ProductSection-product-template"]//*[contains(@class, "product-single__vendor")]/text()'
        priceXpath = '//*[@id="ProductPrice-product-template"]/text()'
        
        #getting the product discription
        discXpath = '//*[@id="ProductSection-product-template"]//*[contains(@class, "product-single__description") and @itemprop="description"]/*'
        description = response.xpath(discXpath)
        descriptionText = ''
        for text in description:
            descriptionText += "".join(text.xpath('.//text()').extract_first().strip())
        
        #Adding product to data frame    
        IowaFoodHubBaconDataFrame.loc[len(IowaFoodHubBaconDataFrame)] = [name,
                                                                         response.xpath(venderXpath).extract_first(), 
                                                                         response.xpath(priceXpath).extract_first(), 
                                                                         descriptionText,                                       
                                                                         self.currentDate,
                                                                         response.meta.get('url')
                                                                        ]
                
    def iowaFoodHubEggs(self, response):
        #validating the name
        nameXpath = '//*[@id="ProductSection-product-template"]//*[contains(@class, "product-single__title") and @itemprop="name"]/text()'       
        name = response.xpath(nameXpath).extract_first()
        desiredNames = {"egg"}
        if not self.containsWord(name, desiredNames):
            return 
        #The other areas we are interested in
        venderXpath = '//*[@id="ProductSection-product-template"]//*[contains(@class, "product-single__vendor") and @itemprop="brand"]/text()'
        priceXpath = '//*[@id="ProductPrice-product-template" and @itemprop="price"]/text()'

        #getting the product discription
        discXpath = '//*[@id="ProductSection-product-template"]//*[contains(@class, "product-single__description") and @itemprop="description"]/*'
        description = response.xpath(discXpath)
        descriptionText = ''
        for text in description:
            descriptionText += "".join(text.xpath('.//text()').extract_first()).strip()
        
        #Adding product to data frame
        IowaFoodHubEggDataFrame.loc[len(IowaFoodHubEggDataFrame)] = [name,
                                                                     response.xpath(venderXpath).extract_first(), 
                                                                     response.xpath(priceXpath).extract_first(), 
                                                                     descriptionText,                                       
                                                                     self.currentDate,
                                                                     response.meta.get('url')
                                                                    ]
    def containsWord(self, string, validWords):
        checkText = string.replace(" ", "").lower()
        for word in validWords:
            if word in checkText:
                return True
        return False

DEBUG = False
#Data frames
IowaFoodHubBaconDataFrame = pd.DataFrame(columns=['Bacon', 'Vender', 'Price', 'Weight', 'Extraction Date', 'Url'])
IowaFoodHubEggDataFrame = pd.DataFrame(columns=['Eggs', 'Vender', 'Price', 'Amount', 'Extraction Date', 'Url'])
if(DEBUG):
    #To see the inner mechanics of the spider
    configure_logging()

#This is to start the spider
process = CrawlerProcess()
process.crawl(IowaFoodHubSpider)
process.start()
process.stop()
currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]

IowaFoodHubBaconDataFrame.to_csv(currentDate + "Iowa Food Hub Bacon.csv")
IowaFoodHubEggDataFrame.to_csv(currentDate + "Iowa Food Hub Eggs.csv")

if(DEBUG):
    #To see the outputs
    print(IowaFoodHubBaconDataFrame)
    print(IowaFoodHubEggDataFrame)
```

This is the code for the Joia Food Farm Spider.

```{python}
from datetime import datetime
import pandas as pd
import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.log import configure_logging

# Note this spider does not go through all the products only the provided links

class JoiaFoodFarmSpider(scrapy.Spider):
    name = 'Joia Food Farm'
    currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]
    def start_requests( self ):
        #Bacon Scraper part
         
        JoiaFoodFarmBaconUrls = ['https://www.joiafoodfarm.com/farmstore/bacon',
                                 'https://www.joiafoodfarm.com/farmstore/jowl-bacon',
                                 'https://www.joiafoodfarm.com/farmstore/danish-bacon',
                                 'https://www.joiafoodfarm.com/farmstore/cottage-bacon'
                                ]
        for url in JoiaFoodFarmBaconUrls:
            yield scrapy.Request( url = url, callback = self.JoiaFoodFarmBacon, meta={'url': url})
        
        JoiaFoodFarmEggsUrls = 'https://www.joiafoodfarm.com/farmstore/1mln8u54udaby8zgy3me268ekeos07'
        yield scrapy.Request( url = JoiaFoodFarmEggsUrls, callback = self.JoiaFoodFarmEggs, meta={'url': JoiaFoodFarmEggsUrls})


    def JoiaFoodFarmBacon(self, response):
        nameXpath = '//*[contains(@class, "ProductItem-summary")]//h1[contains(@class, "ProductItem-details-title")]/text()'
        priceXpath = '//*[contains(@class, "ProductItem-summary")]//*[contains(@class, "product-price")]/text()'        
        Xlist = ['//*[contains(@class, "ProductItem-summary")]//*[contains(@class, "ProductItem-details-excerpt")]/p[2]/em/text()', #p2
                 '//*[contains(@class, "ProductItem-summary")]//*[contains(@class, "ProductItem-details-excerpt")]/p[3]/em/text()', #p3
                 '//*[contains(@class, "ProductItem-summary")]//*[contains(@class, "ProductItem-details-excerpt")]/p[4]/em/text()'  #p4
                ]
        data = [response.xpath(xpath).extract_first() for xpath in Xlist if response.xpath(xpath).extract_first() is not None]
        data = self.fill_list(data, 2)        
        JoiaFoodFarmBaconDataFrame.loc[len(JoiaFoodFarmBaconDataFrame)] = [response.xpath(nameXpath).extract_first(),
                                                                           response.xpath(priceXpath).extract_first(), 
                                                                           data[0], 
                                                                           data[1], 
                                                                           "2038 March Avenue, Charles City, IA, 50616",
                                                                           self.currentDate,
                                                                           response.meta.get('url')]

    def JoiaFoodFarmEggs(self, response):
        nameXpath = '//*[contains(@class, "ProductItem-summary")]//h1[contains(@class, "ProductItem-details-title")]/text()'
        priceXpath = '//*[contains(@class, "ProductItem-summary")]//*[contains(@class, "product-price")]/text()'
        JoiaFoodFarmEggsDataFrame.loc[len(JoiaFoodFarmEggsDataFrame)] = [response.xpath(nameXpath).extract_first(),
                                                                         response.xpath(priceXpath).extract_first(), 
                                                                         "2038 March Avenue, Charles City, IA, 50616",
                                                                         self.currentDate,
                                                                         response.meta.get('url')]

    def fill_list(self, lst, length):
        if len(lst) < length:
            lst += [None] * (length - len(lst))
        return lst


DEBUG = False
#Data frames
JoiaFoodFarmBaconDataFrame = pd.DataFrame(columns=['Bacon', 'Price', 'Unit Price', 'weight', 'Store Location', 'Url', 'extraction date'])
JoiaFoodFarmEggsDataFrame = pd.DataFrame(columns=['Eggs', 'Price', 'Store Location', 'Url', 'extraction date'])
currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]

if(DEBUG):
    #To see the inner mechanics of the spider
    configure_logging()

#This is to start the spider
process = CrawlerProcess()
process.crawl(JoiaFoodFarmSpider)
process.start()
process.stop()
currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]

if(DEBUG):
    #To see the inner mechanics of the spider
    configure_logging()

JoiaFoodFarmBaconDataFrame.to_csv(currentDate + "Joia Food Farm Bacon.csv")
JoiaFoodFarmEggsDataFrame.to_csv(currentDate + "Joia Food Farm Eggs.csv")

if(DEBUG):
    #To see the outputs
    print(JoiaFoodFarmBaconDataFrame)
    print(JoiaFoodFarmEggsDataFrame)
```

This is the code for the Gateway Market Scraper

```{python}
#Imports
from datetime import datetime
import pandas as pd
from enum import Enum
#Imports for Scraping
import selenium
from selenium import webdriver
from selenium.webdriver.firefox.service import Service as FirefoxService
from webdriver_manager.firefox import GeckoDriverManager
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import StaleElementReferenceException
from selenium.common.exceptions import WebDriverException
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from os import path
import time
import sys


#Creator's Note: Products(Enum) and ProductsLoader is probably the only classes you need to edit 
#unless you need to change the way the data is cleaned. Which handled in the DataCleaner class

#These class is here so that we can expand to differnet products easier making the spider more dynamic and expandable
class Products(Enum):
    #Add products like this ProductName = index iteration, [], [] 
    #the 2 empty list will be filled in using the ProductsLoader class
    Bacon = 0, [], []
    Eggs = 1, [], []
    HeirloomTomatoes = 2, [], []

    # Helper method to reduce code for adding to the products and weed out duplicate inputs
    # if you type something in really wrong code will stop the setup is important 
    # correct index inputs are correct index number, url, urls, xpath, xpaths
    def addToProduct(self, items, index):
        product = None
        if isinstance(index, int):
            product = self.value[index]
        elif isinstance(index, str):
            if index.lower() in ['urls', 'url']:
                product = self.value[1]
            elif index.lower() in ['xpaths', 'xpath']:
                product = self.value[2]
        if product == None:
            raise ValueError(f"Invalid index input for ({index}) for input: {items}")
        #Sets are fast at finding dups so we use them for speed
        product_set = set(product)
        for item in items:
            if item not in product_set:
                product.append(item)
                product_set.add(item)

#this class loads the xpaths and urls to the Products Enum and adds dataframes to the spider
class ProductsLoader():
    DataFrames = []
    def __init__(self):
        self.dataFrameAdder()
        self.urlsAdder()
        self.xpathMaker()

    #This adds the dataframe to the spider on load
    def dataFrameAdder(self):
        #Dataframes (You can add more here)


        baconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Orignal Price', 'Weight in lbs', 'True Weight', 'Brand', 'Local', 'Address', 'State', 'City', 'Zip Code', 'Date Collected', 'Url'])
        eggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Orignal Price', 'Amount in dz', 'True Amount', 'Brand', 'Local', 'Address', 'State', 'City', 'Zip Code', 'Date Collected', 'Url'])
        tomatoFrame = pd.DataFrame(columns=['Heirloom Tomatoes', 'Current Price', 'Orignal Price', 'Weight in lbs', 'True Weight', 'Brand', 'Organic', 'Local', 'Address', 'State', 'City', 'Zip Code', 'Date Collected', 'Url'])

        self.DataFrames = [baconFrame,
                           eggFrame,
                           tomatoFrame
                          ]

    #Adding Urls to products
    def urlsAdder(self):
        BaconUrls = ['https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18483',
                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18485',
                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-24190',
                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18553',
                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-33732',
                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18521',
                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18548',
                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18469',
                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-33734',
                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-33736',
                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-33731',
                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-29349',
                     'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-18524'
                    ]
        EggUrls = ['https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-22775',
                   'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-22776',
                   'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-12603',
                  ]
        HeirloomTomatoesUrls = ['https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-11820',
                                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-22455',
                                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-11896',
                                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-11973',
                                'https://gatewaymarket.storebyweb.com/s/1000-1/i/INV-1000-22343',
                               ]
        Products.Bacon.addToProduct(BaconUrls,'urls')
        Products.Eggs.addToProduct(EggUrls,'urls')
        Products.HeirloomTomatoes.addToProduct(HeirloomTomatoesUrls,'urls')

    #This handles the xpaths by adding to the Products class
    #most websites have simular xpaths for each item. You might need to make differnet xpaths for each item 
    #if that is the case
    #For assigning xpaths mark them if they are optional meaning it could or could not be present on the page 
    #we do this for speed up if you mark it as non optional and its not pressent it will skip the value 
    #and hurt the preformence
    #best practice is to render the optional last so it reduces the chances of skipping 
    #Note spiecal cases do happen but they are extremely rare a good indiaction of finding one 
    #is by using skipHandler method and tracking/watching the logs  
    #IMPORTANT < -!- NOT ALL XPATHS ARE THE SAME FOR EACH PRODUCT -!->
    def xpathMaker(self):
        #Add the xpaths here and mark if they are optional
        #Format [xpath, optional, speical]
        nameXpath = '//*[@id="item-details"]/h1[contains(@class, "name")]'
        priceXpath = '//*[@id="item-details"]//*[contains(@class, "wc-pricing")]//*[contains(@aria-describedby, "priceDesc")]'
        prevPriceXpath = '//*[@id="item-details"]//*[contains(@class, "wc-pricing")]/div[contains(@class, "text-muted")]/s' # optional
        brandXpath = '//*[@id="item-details"]/div[1]' # optional 
        weightXpath = '//*[@id="item-details"]//*[contains(@class, "wc-sold-by-avg-weight")]'# optional
        sizeXpath = '//*[@id="item-details"]//*[contains(@class, "card-body")]//*[@class="size"]'
        #xpath, Optional
        xpathList = [(nameXpath, False),
                     (priceXpath, False),
                     (prevPriceXpath, True),
                     (brandXpath, True)
                     (sizeXpath,True)]
        Products.Bacon.addToProduct(xpathList,'xpath')
        Products.Eggs.addToProduct(xpathList,'xpath')
        xpathList = [(nameXpath, False),
                    (priceXpath, False),
                    (prevPriceXpath, True),
                    (brandXpath, True, True),
                    (weightXpath, True),
                    (sizeXpath,True)]
        Products.HeirloomTomatoes.addToProduct(xpathList,'xpath')


class DataCleaner():
    DataArray = []


    def cleanUp(self, item, productName, url):
        self.DataArray = item
        self.DataArray.append("2002 Woodland Avenue Des Moines, IA 50312")
        self.DataArray.append(url)
        return self.DataArray
    

class GatewaySpider():
    name = "Gateway Market" #The store name 
    spiderLogs = []         #The logs 
    skipped = []            #Skipped data 

    #These are methods that are available for your convences
    def log(self, *args):
        self.spiderLogs.append(('Logger:', args))
        if self.LOGGER:
            print('Logger:', *args)

    def debug(self, *args):
        self.spiderLogs.append(('Debug:', args))
        if self.DEBUGGER:
            print('Debug:', *args)
    
    def printer(self, *args):
        self.spiderLogs.append(('Printer:', args))
        print(*args)
    
    def printLogs(self):
        print("\n< --- Printing Logs --- >\n")
        for entry in self.spiderLogs:
            print(*entry)

    def Logs_to_file(self, filename):
        with open(filename, 'w') as file:
            for log_entry in self.spiderLogs:
                file.write('{} {}\n'.format(log_entry[0], log_entry[1]))
    
    def __init__(self):
        self.DEBUGGER = False #The debugger switch to see whats going on. The Default is False
        self.LOGGER = False #When you need to see everything that happends. The Default is False
        self.attempts = 3 #The number of attempts the spider can retry if an error occurs. Default is 3
        self.waitTime = 10 #The number of seconds WebDriver will wait. Default is 10
        self.count = 0 #This saves the location of the url we are going through
        self.runTime = 0 #Total time of extractions
        self.totalRecoveries = 0 #Number of recoveries made while running
        self.maxRetryCount = 100 #Number of retrys the javascript can make Defualt is 100
        self.cleaner = DataCleaner() #Loads the cleaner
        #Selenium needs a webdriver to work. I chose Firefox however you can do another if you need too
        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))
        self.log("Driver started")
    
    #This handles the restart in case we run into an error
    def restart(self):
        self.driver.quit()
        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))
        self.log("Driver restarted")
    
    #This starts the spider
    def start_requests( self ):
        self.runTime = time.time()
        self.log("Loading from ProductsLoader Class")
        load = ProductsLoader() #Loads all products
        self.dataFrames = load.DataFrames #Adds all dataframes
        self.debug("Products Loaded and Data Frames Added")
        self.debug('\n < --- Setup runtime is %s seconds --- >' % (time.time() - self.runTime))
        self.totalRecoveries = 0 
        #Sweeps through all products
        for product in (Products):
            result = self.requestExtraction(product)
        #Adds the date that the data was scraped
        currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]
        self.log("Exporting files")
        #Dataframes to CSV files
        for df, product in zip(self.dataFrames, (Products)):
            df.to_csv(currentDate + self.name +" " + product.name + ".csv")
            self.log('\n', df.to_string())
        self.debug('\n < --- Total runtime took %s seconds with %d recoveries --- >' % (time.time() - self.runTime, self.totalRecoveries))
        if len(self.skipped) != 0:
            self.debug('\n < -!- WARNING SKIPPED (' + str(len(self.skipped)) + ') DATA FOUND --->')
        self.Logs_to_file(currentDate + self.name + ' Spider Logs.txt')
        if len(self.skipped) > 0:
            self.debug(self.skipped)
            self.skipHandler(currentDate)      
        self.driver.quit()

    #This handles the extraction request for the inputed product 
    def requestExtraction(self, product):
        self.count = 0
        errors = 0
        start = time.time()
        self.debug("Starting "+ product.name)    
        for trying in range(self.attempts):
            try:
                self.makeRequest(product)
                self.debug(product.name + " Finished")    
                self.log('\n< --- ' + product.name + ' scrape took %s seconds with %d recoveries --- >\n' % ((time.time() - start), errors))
                self.totalRecoveries += errors
                return self.totalRecoveries
            except Exception as e:
                #Note sometimes the browser will closed unexpectedly and theres not we can do but restart the driver
                errors += 1
                self.debug("An error occurred:", e)
                self.debug("Recovering extraction and continueing")
                self.restart() 
        self.debug(product.name + " Did not Finished after " + str(self.attempts) + " Time wasted: %s seconds" % (time.time() - start))
        self.totalRecoveries += errors
        return self.totalRecoveries

    #This handles the reqests for each url and adds the data to the dataframe
    def makeRequest(self, product):
        productUrls = product.value[1]
        total = len(productUrls)
        while self.count < total:
            url = productUrls[self.count]
            self.driver.get(url)
            self.log("Making a request for: ", url)
            item = []
            time.sleep(1) # marionette Error Fix
            for xpath in product.value[2]:
                #Retrying the xpath given the number of attempts
                for attempt in range(self.attempts):
                    data = self.javascriptXpath(xpath[0])
                    if data in {'empty', 'skip'}:
                        #speical case in case you need it
                        if len(xpath) == 3:
                            if xpath[2]:
                                #example would be when there is actually is a '' in the xpath
                                self.debug("xpath marked as speical")
                                item.append(None)
                                data = 'speical'
                                break
                        if xpath[1] and data == 'empty':    
                            #this is where setting the xpath to optional comes in
                            self.debug("xpath wasnt avaliable")
                            item.append(None)
                            break
                        self.debug("Missing item retrying")
                    else:  #Data found
                        item.append(data)
                        self.log(data + ' was added to the list for: ', url)
                        break
                if attempt == self.attempts:
                    data = 'skip'
                if data == 'skip':  #To help clean the data we skip the item with gaps of data 
                    self.debug("An Item has been skipped for: ", url)  
                    item = ['SKIPPED']
                    #Taking the product name  dataframe number and index added as well as the url 
                    #to retry for later 
                    #This could take time to do so we do this at the very end after we made the cvs files
                    self.skipped.append([product, self.count, url])
                    break
            if 'SKIPPED' in item:
                #No point in cleaning skipped items
                items = ['SKIPPED']*(self.dataFrames[product.value[0]].shape[1] - 1)
                items.append(url)
            else:
                #We call the DataCleaner class to handle the cleaning of the data
                #Its best to clean the data before we add it to the data frame
                self.debug('Data cleaning started: ', item)
                items = self.cleaner.cleanUp(item, product.name, url)
                self.debug('Data cleaning finished: ', items)
                if items == None:
                        self.printer("Data cleaner not configured to ", product.name)
            self.debug('Extracted: ', items)
            self.dataFrames[product.value[0]].loc[len(self.dataFrames[product.value[0]])] = items                    
            self.count += 1
            self.printer(product.name + " item added ", self.count, " of ", total, ":  ", items)

    #Collecting the data from the xpath in JavaScript is faster and results in fewer errors than doing it in python
    #This is where selenium shines because we can both use JavaScript and render JavaScript websites
    #and is the only reason why we use it instead of scrapy
    def javascriptXpath(self, xpath):
        # if the time expires it assumes xpath wasnt found in the page
        try: 
            #Waits for page to load 
            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)
            elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))

            # Runs the javascript and collects the text data from the inputed xpath
            # We want to keep repeating if we get any of these outputs becasue the page is still 
            # loading and we dont want to skip or waste time. (for fast computers)
            retrycount = 0
            invalidOutputs = {"error", 'skip' "$nan", ''}
            while retrycount < self.maxRetryCount :
                text = self.driver.execute_script("""
                    const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;
                    if (!element) {
                        return 'skip';
                    }
                    return element.textContent.trim();
                """, 
                xpath)
                checkText = text.replace(" ", "").lower()
                if checkText in invalidOutputs:
                    retrycount+=1
                else:
                    self.log(retrycount, "xpath attempts for (", text, ")")
                    return text
            self.log("xpath attempts count met. Problematic text (" + text + ") for ", xpath)
            return 'skip'
        except TimeoutException:
            self.log('Could not find xpath for: ', xpath)
            return 'empty'

           

    #This is here to hopefully fix skipped data
    #Best case sinarios this will never be used
    def skipHandler(self, currentDate):
        corrections = 0
        # skipped format
        # [product name, DataFrame number, DataFrame index, url]
        while len(self.skipped) != 0:
            #each skip 
            for index, dataSkip in enumerate(self.skipped):
                product = dataSkip[0]
                #Limiting the Attempts to fix while avoiding bottlenecking the problem
                for attempt in range(self.attempts*2):
                    product = dataSkip[0]
                    url = dataSkip[2]
                    self.driver.get(url)
                    self.log("Making a request for: ", url)
                    item = []
                    for xpath in product.value[2]:
                        for attemptIn in range(self.attempts*2):
                            data = self.javascriptXpath(xpath[0])
                            if data in {'empty', 'skip'}:   
                                if xpath[1] and data == 'empty':    
                                    #this is where setting the xpath to optional comes in
                                    self.debug("xpath wasnt avaliable")
                                    item.append(None)
                                    break
                                self.debug("Missing item retrying")
                            else:  #Data found
                                item.append(data)
                                self.log(data + ' was added to the list for: ', url)
                                break
                        if attemptIn == self.attempts*2:
                            data = 'skip'
                            break
                if data == 'skip':  #To help clean the data we skip the item with gaps of data 
                    self.debug("Item still missing attempting other skipped for now") 
                else:
                    items = self.cleaner.cleanUp(item, product.name, url)
                    if items == None:
                        self.printer("Data cleaner not configured to ", product.name)
                    self.dataFrames[dataSkip[1]].loc[dataSkip[2]] = items                    
                    self.printer("Fixed " + product.name + " item: ", items)
                    #To avoid infinite loops and never saving our data we save the file now
                    self.dataFrames[product.value[0]].to_csv(currentDate + "REPAIRED Gateway Market " + product.name + ".csv")
                    self.debug('\n < --- Total runtime with saving of repairs took %s seconds --- >' % (time.time() - self.runTime))
                    self.Logs_to_file(currentDate + self.name + ' Spider REPAIR Logs.txt')
                    #To avoid fixing fixed items we pop, mark, and break
                    self.skipped.pop(index)
                    corrections += 1
                    break
        self.debug('\n < --- Total runtime with all repairs took %s seconds --- >' % (time.time() - self.runTime))
        self.Logs_to_file(currentDate + self.name + ' spider COMPLETED REPAIR Logs.txt')
        
# Start
#DEBUG Switch
SHOW = True

#Spider setup
spider = GatewaySpider()
spider.LOGGER = True
spider.DEBUGGER = True

#Running the spider
spider.start_requests()

if(SHOW):
    [print(dataFrame) for dataFrame in spider.dataFrames]
    spider.printLogs()
```

This is the code for the HyVee Scraper

```{python}
#Imports
from datetime import datetime
import pandas as pd
from enum import Enum
#Imports for Scraping
import selenium
from selenium import webdriver
from selenium.webdriver.firefox.service import Service as FirefoxService
from webdriver_manager.firefox import GeckoDriverManager
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import StaleElementReferenceException
from selenium.common.exceptions import WebDriverException
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from os import path
import time
import sys


#Creator's Note: Products(Enum) and ProductsLoader is probably the only classes you need to edit 
#unless you need to change the way the data is cleaned. Which handled in the DataCleaner class

#These class is here so that we can expand to differnet products easier making the spider more dynamic and expandable
class Products(Enum):
    #Add products like this ProductName = index iteration, [], [] 
    #the 2 empty list will be filled in using the ProductsLoader class
    Bacon = 0, [], []
    Eggs = 1, [], []
    HeirloomTomatoes = 2, [], []

    # Helper method to reduce code for adding to the products and weed out duplicate inputs
    # if you type something in really wrong code will stop the setup is important 
    # correct index inputs are correct index number, url, urls, xpath, xpaths
    def addToProduct(self, items, index):
        product = None
        if isinstance(index, int):
            product = self.value[index]
        elif isinstance(index, str):
            if index.lower() in ['urls', 'url']:
                product = self.value[1]
            elif index.lower() in ['xpaths', 'xpath']:
                product = self.value[2]
        if product == None:
            raise ValueError(f"Invalid index input for ({index}) for input: {items}")
        #Sets are fast at finding dups so we use them for speed
        product_set = set(product)
        for item in items:
            if item not in product_set:
                product.append(item)
                product_set.add(item)

#This class loads the xpaths and urls to the Products Enum and adds dataframes to the spider
class ProductsLoader():
    DataFrames = []
    def __init__(self):
        self.dataFrameAdder()
        self.urlsAdder()
        self.xpathMaker()

    #This adds the dataframe to the spider on load
    def dataFrameAdder(self):
        #Dataframes (You can add more here)
        baconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Sale', 'Weight', 'Url'])
        eggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Sale', 'Amount', 'Url'])
        tomatoFrame = pd.DataFrame(columns=['Heirloom Tomato', 'Current Price', 'Sale', 'Weight', 'Url'])
        self.DataFrames = [baconFrame,
                           eggFrame,
                           tomatoFrame
                          ]

    #Adding Urls to products
    def urlsAdder(self):
        BaconUrls = ['https://www.hy-vee.com/aisles-online/p/11315/Hormel-Black-Label-Thick-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/47128/Hormel-Black-Label-Fully-Cooked-Original-Thick-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/41626/Applegate-Naturals-Uncured-Sunday-Bacon-Hickory-Smoked',
                     'https://www.hy-vee.com/aisles-online/p/57278/HyVee-Double-Smoked-Thick-Sliced-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/2405550/Applegate-Naturals-No-Sugar-Uncured-Bacon-Hickory-Smoked',
                     'https://www.hy-vee.com/aisles-online/p/57279/HyVee-Sweet-Smoked-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/11366/Hormel-Black-Label-Original-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/2455081/Jimmy-Dean-Premium-Hickory-Smoked-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/3595492/Farmland-Bacon-Double-Smoked-Double-Thick-Cut',
                     'https://www.hy-vee.com/aisles-online/p/47117/Hormel-Black-Label-Center-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/57277/HyVee-Center-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/2199424/Country-Smokehouse-Thick-Applewood-Slab-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/77228/Hormel-Black-Label-Original-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/21239/Farmland-Naturally-Hickory-Smoked-Classic-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/2456254/Jimmy-Dean-Premium-Applewood-Smoked-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/21240/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/47159/Hormel-Black-Label-Original-Bacon-4Pk',
                     'https://www.hy-vee.com/aisles-online/p/50315/Oscar-Mayer-Naturally-Hardwood-Smoked-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/50321/Oscar-Mayer-Center-Cut-Original-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/50316/Oscar-Mayer-Thick-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/2199421/Country-Smokehouse-Thick-Hickory-Smoked-Slab-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/104466/Hickory-Country-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/23975/HyVee-Hickory-House-Applewood-Naturally-Smoked-Thick-Sliced-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/23949/HyVee-Sweet-Smoked-Thick-Sliced-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/23963/HyVee-Fully-Cooked-Hickory-Smoked-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/11173/Hormel-Black-Label-Applewood-Thick-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/21317/Farmland-Naturally-Applewood-Smoked-Classic-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/21238/Farmland-Naturally-Hickory-Smoked-Thick-Cut-Bacon-Package',
                     'https://www.hy-vee.com/aisles-online/p/23948/HyVee-Lower-Sodium-Sweet-Smoked-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/458259/Wright-Naturally-Hickory-Smoked-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/11384/Hormel-Natural-Choice-Uncured-Original-Bacon-12-oz',
                     'https://www.hy-vee.com/aisles-online/p/2476490/Jimmy-Dean-FC-Hickory-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/1646677/Smithfield-Hometown-Original-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/53849/Farmland-Naturally-Hickory-Smoked-Lower-Sodium-Classic-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/47121/Hormel-Black-Label-Maple-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/164627/Oscar-Mayer-Fully-Cooked-Original-Bacon-252-oz-Box',
                     'https://www.hy-vee.com/aisles-online/p/23974/HyVee-Hickory-House-Hickory-Smoked-Thick-Sliced-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/50319/Oscar-Mayer-Selects-Smoked-Uncured-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/2471760/Jimmy-Dean-FC-Applewood-Smoked-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/16239/Oscar-Mayer-Center-Cut-Thick-Sliced-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/2214511/Hormel-Black-Label-Original-Thick-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/1008152/Wright-Naturally-Smoked-Applewood-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/1813260/Smithfield-Naturally-Hickory-Smoked-Thick-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/23976/HyVee-Hickory-House-Peppered-Naturally-Smoked-Thick-Sliced-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/21320/Farmland-Naturally-Applewood-Smoked-Thick-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/21253/Farmland-Naturally-Hickory-Smoked-Extra-Thick-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/1255920/Hormel-Black-Label-Cherrywood-Thick-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/57304/HyVee-Blue-Ribbon-Maple-Naturally-Smoked-Thick-Sliced-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/21252/Farmland-Naturally-Hickory-Smoked-30-Less-Fat-Center-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/2501872/Bourbon-And-Brown-Sugar-Slab-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/2516586/Hormel-Natural-ChoiceOriginal-Thick-Cut-Uncured-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/21319/Farmland-Naturally-Hickory-Smoked-Double-Smoked-Classic-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/317829/Des-Moines-Bacon-And-Meat-Company-Hardwood-Smoked-Uncured-Country-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/1255919/Hormel-Black-Label-Jalapeno-Thick-Cut-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/3538865/Oscar-Mayer-Bacon-Thick-Cut-Applewood',
                     'https://www.hy-vee.com/aisles-online/p/317830/Des-Moines-Bacon-And-Meat-Company-Applewood-Smoked-Bacon',
                     'https://www.hy-vee.com/aisles-online/p/3308731/Oscar-Mayer-Natural-Fully-Cooked-Uncured-Bacon'
                    ]
        EggUrls = ['https://www.hy-vee.com/aisles-online/p/57236/HyVee-Grade-A-Large-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/23899/HyVee-Grade-A-Large-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/715446/Farmers-Hen-House-Free-Range-Organic-Large-Brown-Grade-A-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/2849570/Thats-Smart-Large-Shell-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/31351/Farmers-Hen-House-Free-Range-Grade-A-Large-Brown-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/23900/HyVee-Grade-A-Extra-Large-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/71297/Egglands-Best-Farm-Fresh-Grade-A-Large-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/36345/Egglands-Best-Grade-A-Large-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/3192325/HyVee-Free-Range-Large-Brown-Egg-Grade-A',
                   'https://www.hy-vee.com/aisles-online/p/23903/HyVee-Grade-A-Jumbo-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/3192323/HyVee-Cage-Free-Large-Brown-Egg-Grade-A',
                   'https://www.hy-vee.com/aisles-online/p/36346/Egglands-Best-Cage-Free-Brown-Grade-A-Large-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/3192322/HyVee-Cage-Free-Large-Brown-Egg-Grade-A',
                   'https://www.hy-vee.com/aisles-online/p/858343/HyVee-Cage-Free-Omega3-Grade-A-Large-Brown-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/1901565/Farmers-Hen-House-Pasture-Raised-Organic-Grade-A-Large-Brown-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/60364/HyVee-HealthMarket-Organic-Grade-A-Large-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/71298/Egglands-Best-Extra-Large-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/23902/HyVee-Grade-A-Extra-Large-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/453006/Egglands-Best-XL-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/2668550/HyVee-One-Step-Pasture-Raised-Large-Brown-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/66622/Farmers-Hen-House-Jumbo-Brown-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/3274825/Nellies-Eggs-Brown-Free-Range-Large',
                   'https://www.hy-vee.com/aisles-online/p/57235/HyVee-Grade-A-Medium-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/2437128/Pete-And-Gerrys-Eggs-Organic-Brown-Free-Range-Large',
                   'https://www.hy-vee.com/aisles-online/p/36347/Egglands-Best-Organic-Cage-Free-Grade-A-Large-Brown-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/2698224/Nellies-Free-Range-Eggs-Large-Fresh-Brown-Grade-A',
                   'https://www.hy-vee.com/aisles-online/p/57237/HyVee-Grade-A-Large-Eggs',
                   'https://www.hy-vee.com/aisles-online/p/190508/Farmers-Hen-House-Organic-Large-Brown-Eggs'
                  ]
        HeirloomTomatoesUrls = ['https://www.hy-vee.com/aisles-online/p/37174/']

        Products.Bacon.addToProduct(BaconUrls,'urls')
        Products.Eggs.addToProduct(EggUrls,'urls')
        Products.HeirloomTomatoes.addToProduct(HeirloomTomatoesUrls,'urls')

    #This handles the xpaths by adding to the Products class
    #most websites have simular xpaths for each item. You might need to make differnet xpaths for each item 
    #if that is the case
    #For assigning xpaths mark them if they are optional meaning it could or could not be present on the page 
    #we do this for speed up if you mark it as non optional and its not pressent it will skip the value 
    #and hurt the preformence
    #best practice is to render the optional last so it reduces the chances of skipping 
    #Note spiecal cases do happen but they are extremely rare a good indiaction of finding one 
    #is by using skipHandler method and tracking/watching the logs  
    #IMPORTANT < -!- NOT ALL XPATHS ARE THE SAME FOR EACH PRODUCT -!->
    def xpathMaker(self):
        #Add the xpaths here and mark if they are optional
        nameXpath = '//*[contains(@class, "product-details_detailsContainer")]/h1'
        priceXpath = '//*[contains(@class, "product-details_detailsContainer")]/p[1]'
        prevPriceXpath = '//*[contains(@class, "product-details_detailsContainer")]/p[2]'
        weightXpath = '//*[contains(@class, "product-details_detailsContainer")]/p[3]' # optional

        #xpath, Optional
        xpathList = [(nameXpath, False),
                     (priceXpath, False),
                     (prevPriceXpath, False),
                     (weightXpath, True)]

        Products.Bacon.addToProduct(xpathList,'xpath')
        Products.Eggs.addToProduct(xpathList,'xpath')
        Products.HeirloomTomatoes.addToProduct(xpathList,'xpath')


class DataCleaner():
    DataArray = []
    def cleanUp(self, item, url):
        self.DataArray = item
        if self.DataArray[3] == None:
            self.swap_elements(2, 3)
        self.DataArray.append(url)
        return self.DataArray
    
    def swap_elements(self, idx1, idx2):
        # Make a copy of the input list to avoid modifying it
        new_lst = self.DataArray.copy()
        # Swap the elements at the two indices
        new_lst[idx1], new_lst[idx2] = new_lst[idx2], new_lst[idx1]
        self.DataArray = new_lst

class HyveeSpider():
    name = "Hyvee"  #The store name 
    spiderLogs = []         #The logs 
    skipped = []            #Skipped data 

    #These are methods that are available for your convences
    def log(self, *args):
        self.spiderLogs.append(('Logger:', args))
        if self.LOGGER:
            print('Logger:', *args)

    def debug(self, *args):
        self.spiderLogs.append(('Debug:', args))
        if self.DEBUGGER:
            print('Debug:', *args)
    
    def printer(self, *args):
        self.spiderLogs.append(('Printer:', args))
        print(*args)
    
    def printLogs(self):
        print("\n< --- Printing Logs --- >\n")
        for entry in self.spiderLogs:
            print(*entry)

    def Logs_to_file(self, filename):
        with open(filename, 'w') as file:
            for log_entry in self.spiderLogs:
                file.write('{} {}\n'.format(log_entry[0], log_entry[1]))
    
    def __init__(self):
        self.DEBUGGER = False #The debugger switch to see whats going on. The Default is False
        self.LOGGER = False #When you need to see everything that happends. The Default is False
        self.attempts = 3 #The number of attempts the spider can retry if an error occurs. Default is 3
        self.waitTime = 10 #The number of seconds WebDriver will wait. Default is 10
        self.count = 0 #This saves the location of the url we are going through
        self.runTime = 0 #Total time of extractions
        self.totalRecoveries = 0 #Number of recoveries made while running
        self.maxRetryCount = 100 #Number of retrys the javascript can make Defualt is 100
        self.cleaner = DataCleaner() #Loads the cleaner
        #Selenium needs a webdriver to work. I chose Firefox however you can do another if you need too
        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))
        self.log("Driver started")
    
    #This handles the restart in case we run into an error
    def restart(self):
        self.driver.quit()
        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))
        self.log("Driver restarted")
    
    #This starts the spider
    def start_requests( self ):
        self.runTime = time.time()
        self.log("Loading from ProductsLoader Class")
        load = ProductsLoader() #Loads all products
        self.dataFrames = load.DataFrames #Adds all dataframes
        self.debug("Products Loaded and Data Frames Added")
        self.debug('\n < --- Setup runtime is %s seconds --- >' % (time.time() - self.runTime))
        self.totalRecoveries = 0 
        #Sweeps through all products
        for product in (Products):
            result = self.requestExtraction(product)
        #Adds the date that the data was scraped
        currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]
        self.log("Exporting files")
        #Dataframes to CSV files
        for df, product in zip(self.dataFrames, (Products)):
            df.to_csv(currentDate + self.name +" " + product.name + ".csv")
            self.log('\n', df.to_string())
        self.debug('\n < --- Total runtime took %s seconds with %d recoveries --- >' % (time.time() - self.runTime, self.totalRecoveries))
        if len(self.skipped) != 0:
            self.debug('\n < -!- WARNING SKIPPED (' + str(len(self.skipped)) + ') DATA FOUND --->')
        self.Logs_to_file(currentDate + self.name + ' Spider Logs.txt')
        if len(self.skipped) > 0:
            self.debug(self.skipped)
            self.skipHandler(currentDate)      
        self.driver.quit()

    #This handles the extraction request for the inputed product 
    def requestExtraction(self, product):
        self.count = 0
        errors = 0
        start = time.time()
        self.debug("Starting "+ product.name)    
        for trying in range(self.attempts):
            try:
                self.makeRequest(product)
                self.debug(product.name + " Finished")    
                self.log('\n< --- ' + product.name + ' scrape took %s seconds with %d recoveries --- >\n' % ((time.time() - start), errors))
                self.totalRecoveries += errors
                return self.totalRecoveries
            except Exception as e:
                #Note sometimes the browser will closed unexpectedly and theres not we can do but restart the driver
                errors += 1
                self.debug("An error occurred:", e)
                self.debug("Recovering extraction and continueing")
                self.restart() 
        self.debug(product.name + " Did not Finished after " + str(self.attempts) + " Time wasted: %s seconds" % (time.time() - start))
        self.totalRecoveries += errors
        return self.totalRecoveries

    #This handles the reqests for each url and adds the data to the dataframe
    def makeRequest(self, product):
        productUrls = product.value[1]
        total = len(productUrls)
        while self.count < total:
            url = productUrls[self.count]
            self.driver.get(url)
            self.log("Making a request for: ", url)
            item = []
            time.sleep(1) # marionette Error Fix
            for xpath in product.value[2]:
                #Retrying the xpath given the number of attempts
                for attempt in range(self.attempts):
                    data = self.javascriptXpath(xpath[0])
                    if data in {'empty', 'skip'}:
                        #speical case in case you need it
                        if len(xpath) == 3:
                            if xpath[2]:
                                #example would be when there is actually is a '' in the xpath
                                self.debug("xpath marked as speical")
                                item.append(None)
                                data = 'speical'
                                break
                        if xpath[1] and data == 'empty':    
                            #this is where setting the xpath to optional comes in
                            self.debug("xpath wasnt avaliable")
                            item.append(None)
                            break
                        self.debug("Missing item retrying")
                    else:  #Data found
                        item.append(data)
                        self.log(data + ' was added to the list for: ', url)
                        break
                if attempt == self.attempts:
                    data = 'skip'
                if data == 'skip':  #To help clean the data we skip the item with gaps of data 
                    self.debug("An Item has been skipped for: ", url)  
                    item = ['SKIPPED']
                    #Taking the product name  dataframe number and index added as well as the url 
                    #to retry for later 
                    #This could take time to do so we do this at the very end after we made the cvs files
                    self.skipped.append([product, self.count, url])
                    break
            if 'SKIPPED' in item:
                #No point in cleaning skipped items
                items = ['SKIPPED']*(self.dataFrames[product.value[0]].shape[1] - 1)
                items.append(url)
            else:
                #We call the DataCleaner class to handle the cleaning of the data
                #Its best to clean the data before we add it to the data frame
                self.debug('Data cleaning started: ', item)
                items = self.cleaner.cleanUp(item, url)
                self.debug('Data cleaning finished: ', item)
            self.debug('Extracted: ', items)
            self.dataFrames[product.value[0]].loc[len(self.dataFrames[product.value[0]])] = items                    
            self.count += 1
            self.printer(product.name + " item added ", self.count, " of ", total, ":  ", items)

    #Collecting the data from the xpath in JavaScript is faster and results in fewer errors than doing it in python
    #This is where selenium shines because we can both use JavaScript and render JavaScript websites
    #and is the only reason why we use it instead of scrapy
    def javascriptXpath(self, xpath):
        # if the time expires it assumes xpath wasnt found in the page
        try: 
            #Waits for page to load 
            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)
            elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))

            # Runs the javascript and collects the text data from the inputed xpath
            # We want to keep repeating if we get any of these outputs becasue the page is still 
            # loading and we dont want to skip or waste time. (for fast computers)
            retrycount = 0
            invalidOutputs = {"error", 'skip' "$nan", ''}
            while retrycount < self.maxRetryCount :
                text = self.driver.execute_script("""
                    const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;
                    if (!element) {
                        return 'skip';
                    }
                    return element.textContent.trim();
                """, 
                xpath)
                checkText = text.replace(" ", "").lower()
                if checkText in invalidOutputs:
                    retrycount+=1
                else:
                    self.log(retrycount, "xpath attempts for (", text, ")")
                    return text
            self.log("xpath attempts count met. Problematic text (" + text + ") for ", xpath)
            return 'skip'
        except TimeoutException:
            self.log('Could not find xpath for: ', xpath)
            return 'empty'

           

    #This is here to hopefully fix skipped data
    #Best case sinarios this will never be used
    def skipHandler(self, currentDate):
        corrections = 0
        # skipped format
        # [product name, DataFrame number, DataFrame index, url]
        while len(self.skipped) != 0:
            #each skip 
            for index, dataSkip in enumerate(self.skipped):
                product = dataSkip[0]
                #Limiting the Attempts to fix while avoiding bottlenecking the problem
                for attempt in range(self.attempts*2):
                    product = dataSkip[0]
                    url = dataSkip[2]
                    self.driver.get(url)
                    self.log("Making a request for: ", url)
                    item = []
                    for xpath in product.value[2]:
                        for attemptIn in range(self.attempts*2):
                            data = self.javascriptXpath(xpath[0])
                            if data in {'empty', 'skip'}:   
                                if xpath[1] and data == 'empty':    
                                    #this is where setting the xpath to optional comes in
                                    self.debug("xpath wasnt avaliable")
                                    item.append(None)
                                    break
                                self.debug("Missing item retrying")
                            else:  #Data found
                                item.append(data)
                                self.log(data + ' was added to the list for: ', url)
                                break
                        if attemptIn == self.attempts*2:
                            data = 'skip'
                            break
                if data == 'skip':  #To help clean the data we skip the item with gaps of data 
                    self.debug("Item still missing attempting other skipped for now") 
                else:
                    items = self.cleaner.cleanUp(item, url)
                    self.dataFrames[dataSkip[1]].loc[dataSkip[2]] = items                    
                    self.printer("Fixed " + product.name + " item: ", items)
                    #To avoid infinite loops and never saving our data we save the file now
                    self.dataFrames[product.value[0]].to_csv(currentDate + "REPAIRED Gateway Market " + product.name + ".csv")
                    self.debug('\n < --- Total runtime with saving of repairs took %s seconds --- >' % (time.time() - self.runTime))
                    self.Logs_to_file(currentDate + self.name + ' Spider REPAIR Logs.txt')
                    #To avoid fixing fixed items we pop, mark, and break
                    self.skipped.pop(index)
                    corrections += 1
                    break
        self.debug('\n < --- Total runtime with all repairs took %s seconds --- >' % (time.time() - self.runTime))
        self.Logs_to_file(currentDate + self.name + ' spider COMPLETED REPAIR Logs.txt')
        
# Start
#DEBUG Switch
SHOW = True

#Spider setup
spider = HyveeSpider()
spider.LOGGER = True
spider.DEBUGGER = True

#Running the spider
spider.start_requests()

if(SHOW):
    [print(dataFrame) for dataFrame in spider.dataFrames]
    spider.printLogs()
```

This is the code for the New Pioneer Co-op Scraper

```{python}
#Imports
from datetime import datetime
import pandas as pd
from enum import Enum
#Imports for Scraping
import selenium
from selenium import webdriver
from selenium.webdriver.firefox.service import Service as FirefoxService
from webdriver_manager.firefox import GeckoDriverManager
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import StaleElementReferenceException
from selenium.common.exceptions import WebDriverException
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from os import path
import time
import sys


#Creator's Note: Products(Enum) and ProductsLoader is probably the only classes you need to edit 
#unless you need to change the way the data is cleaned. Which handled in the DataCleaner class

#These class is here so that we can expand to differnet products easier making the spider more dynamic and expandable
class Products(Enum):
    #Add products like this ProductName = index iteration, [], [] 
    #the 2 empty list will be filled in using the ProductsLoader class
    Bacon = 0, [], []
    Eggs = 1, [], []
    HeirloomTomatoes = 2, [], []

    # Helper method to reduce code for adding to the products and weed out duplicate inputs
    # if you type something in really wrong code will stop the setup is important 
    # correct index inputs are correct index number, url, urls, xpath, xpaths
    def addToProduct(self, items, index):
        product = None
        if isinstance(index, int):
            product = self.value[index]
        elif isinstance(index, str):
            if index.lower() in ['urls', 'url']:
                product = self.value[1]
            elif index.lower() in ['xpaths', 'xpath']:
                product = self.value[2]
        if product == None:
            raise ValueError(f"Invalid index input for ({index}) for input: {items}")
        #Sets are fast at finding dups so we use them for speed
        product_set = set(product)
        for item in items:
            if item not in product_set:
                product.append(item)
                product_set.add(item)

#This class loads the xpaths and urls to the Products Enum and adds dataframes to the spider
class ProductsLoader():
    DataFrames = []
    storeXpaths = []
    def __init__(self):
        self.dataFrameAdder()
        self.setStoreXpaths()
        self.urlsAdder()
        self.xpathMaker()

    #This adds the dataframe to the spider on load
    def dataFrameAdder(self):
        #Dataframes (You can add more here)
        baconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Weight' ,'Sale', 'Store Location', 'Url'])
        eggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Weight' ,'Sale', 'Store Location', 'Url'])
        tomatoFrame = pd.DataFrame(columns=['Heirloom Tomato', 'Current Price', 'Weight' ,'Sale', 'Store Location', 'Url'])
        self.DataFrames = [baconFrame,
                           eggFrame,
                           tomatoFrame
                          ]

    def setStoreXpaths(self):
        CoralvilleButtonXpath = '//*[@id="store-locator"]//*[contains(@data-store-id,"2843") and contains(@class,"fp-btn fp-btn-default fp-btn-mystore ") and contains(@role,"button")]'
        IowaCityButtonXpath = '//*[@id="store-locator"]//*[contains(@data-store-id,"2844") and contains(@class,"fp-btn fp-btn-default fp-btn-mystore ") and contains(@role,"button")]'
        CedarRapidsButtonXpath = '//*[@id="store-locator"]//*[contains(@data-store-id,"2845") and contains(@class,"fp-btn fp-btn-default fp-btn-mystore ") and contains(@role,"button")]'
        self.storeXpaths = [CedarRapidsButtonXpath,
                            IowaCityButtonXpath,
                            CoralvilleButtonXpath
                            ]

    #Adding Urls to products
    def urlsAdder(self):
        BaconUrls = ['https://shop.newpi.coop/shop/meat/bacon/sliced/applegate_natural_hickory_smoked_uncured_sunday_bacon_8_oz/p/19959#!/?department_id=1322093',
                     'https://shop.newpi.coop/shop/meat/bacon/beeler_hickory_smoked_bacon/p/7703726#!/?department_id=1322093',
                     'https://shop.newpi.coop/shop/meat/bacon/beeler_bacon_ends_and_pieces/p/1564405684703446698#!/?department_id=1322093',
                     'https://shop.newpi.coop/shop/meat/bacon/beeler_hickory_smoked_bacon/p/7791059#!/?department_id=1322093',
                     'https://shop.newpi.coop/shop/meat/bacon/garrett_valley_pork_bacon_classic_dry_rubbed_uncured/p/7703238#!/?department_id=1322093',
                     'https://shop.newpi.coop/shop/meat/bacon/turkey/garrett_valley_turkey_bacon_sugar_free_paleo/p/7703237#!/?department_id=1322093',
                     'https://shop.newpi.coop/shop/meat/bacon/beeler_pepper_bacon/p/1564405684702577823#!/?department_id=1322093',
                     'https://shop.newpi.coop/shop/meat/bacon/turkey/plainville_farms_turkey_bacon_uncured/p/4750634#!/?department_id=1322093',
                     'https://shop.newpi.coop/shop/meat/bacon/garrett_valley_pork_bacon_8_oz/p/6572556#!/?department_id=1322093',
                     'https://shop.newpi.coop/shop/refrigerated/meat_alternatives/herbivorous_butcher_hickory_maple_bacon/p/1564405684704334152#!/?department_id=1322093',
                     'https://shop.newpi.coop/shop/meat/pork/new_pi_bulk_bacon/p/1564405684704337543#!/?department_id=1322171',
                     'https://shop.newpi.coop/shop/meat/pork/niman_ranch_uncured_bacon_12_oz/p/7276#!/?department_id=1322171'
                    ]
        EggUrls = ['https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/farmers_hen_house_eggs_grade_a_free_range_large_brown_12_ea/p/7110637',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/farmers_hen_house_eggs_white_cage_free_large/p/7110638',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/farmers_hen_house_eggs_large_brown_free_range/p/7613595',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/vital_farms_eggs_organic_pasture_raised_large_12_ea/p/5637123',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/hotz_eggs_dozen_xtra_large/p/1564405684714084840',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/farmers_hen_house_eggs_jumbo_brown/p/7613596',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/cedar_ridge_farm_grade_a_large_eggs/p/1564405684704338616',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/farmers_hen_house_eggs_og_pasture_lrg_brwn/p/7613597',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/farmers_hen_house_eggs_large_brown_free_range/p/1564405684703497142',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/vital_farms_large_pasture_raised_eggs_12_ea/p/5323128',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/vital_farms_eggs_pasture_raised_large_18_ea/p/1564405684690018196',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/organic/organic_valley_free_range_brown_large_eggs_12_ea/p/48765',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/cedar_ridge_farm_grade_a_extra_large_eggs/p/1564405684704338617',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/steinecke_family_farm_duck_eggs/p/1564405684711593802',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/cosgrove_rd_farm_eggs_pasture_raised/p/1564405684710338102',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/cage_free/cedar_ridge_farm_grade_a_jumbo_eggs/p/1564405684704338619',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/hotz_eggs_large/p/1564405684704684702',
                   'https://shop.newpi.coop/shop/refrigerated/eggs/hotz_eggs_medium/p/1564405684713746940',
                  ]
        HeirloomTomatoesUrls = ['https://shop.newpi.coop/shop/produce/fresh_vegetables/tomatoes/heirloom_tomatoes/p/2311736']

        Products.Bacon.addToProduct(BaconUrls,'urls')
        Products.Eggs.addToProduct(EggUrls,'urls')
        Products.HeirloomTomatoes.addToProduct(HeirloomTomatoesUrls,'urls')

    #This handles the xpaths by adding to the Products class
    #most websites have simular xpaths for each item. You might need to make differnet xpaths for each item 
    #if that is the case
    #For assigning xpaths mark them if they are optional meaning it could or could not be present on the page 
    #we do this for speed up if you mark it as non optional and its not pressent it will skip the value 
    #and hurt the preformence
    #best practice is to render the optional last so it reduces the chances of skipping 
    #Note spiecal cases do happen but they are extremely rare a good indiaction of finding one 
    #is by using skipHandler method and tracking/watching the logs  
    #IMPORTANT < -!- NOT ALL XPATHS ARE THE SAME FOR EACH PRODUCT -!->
    def xpathMaker(self):
        #Add the xpaths here and mark if they are optional
        nameXpath = '//*[@id="products"]//*[contains(@class,"fp-item-detail")]//*[contains(@class,"fp-item-name")]' #special because the store can be not carrying the product
        priceXpath = '//*[@id="products"]//*[contains(@class,"fp-item-detail")]//*[contains(@class,"fp-item-price")]//span[contains(@class,"fp-item-base-price")]'
        weightXpath = '//*[@id="products"]//*[contains(@class,"fp-item-detail")]//*[contains(@class,"fp-item-price")]//span[contains(@class,"fp-item-size")]'
        saleXpath = '//*[@id="products"]//*[contains(@class,"fp-item-detail")]//*[contains(@class,"fp-item-sale")]//span[contains(@class,"fp-item-sale-price")]' # optional
        #xpath, Optional, special
        xpathList = [(nameXpath, False, True),
                     (priceXpath, False),
                     (weightXpath, False),
                     (saleXpath, True)]

        Products.Bacon.addToProduct(xpathList,'xpath')
        Products.Eggs.addToProduct(xpathList,'xpath')
        Products.HeirloomTomatoes.addToProduct(xpathList,'xpath')


class DataCleaner():
    DataArray = []
    storeLocation = ''
    def cleanUp(self, item, url):
        self.DataArray = item
        self.DataArray.append(self.storeLocation)
        self.DataArray.append(url)
        return self.DataArray
    
    def setStore(self, storeIndex):
        cases = {
            0: "3338 Center Point Road Northeast Cedar Rapids, IA 52402",
            1: "22 South Van Buren Street Iowa City, IA 52240",
            2: "1101 2nd St Coralville, IA 52241"
        }
        self.storeLocation = cases.get(storeIndex)

    

class NewPioneerSpider():
    name = "New Pioneer Co-op"  #The store name 
    spiderLogs = []         #The logs 
    skipped = []            #Skipped data 

    #These are methods that are available for your convences
    def log(self, *args):
        self.spiderLogs.append(('Logger:', args))
        if self.LOGGER:
            print('Logger:', *args)

    def debug(self, *args):
        self.spiderLogs.append(('Debug:', args))
        if self.DEBUGGER:
            print('Debug:', *args)
    
    def printer(self, *args):
        self.spiderLogs.append(('Printer:', args))
        print(*args)
    
    def printLogs(self):
        print("\n< --- Printing Logs --- >\n")
        for entry in self.spiderLogs:
            print(*entry)

    def Logs_to_file(self, filename):
        with open(filename, 'w') as file:
            for log_entry in self.spiderLogs:
                file.write('{} {}\n'.format(log_entry[0], log_entry[1]))
    
    def __init__(self):
        self.DEBUGGER = False #The debugger switch to see whats going on. The Default is False
        self.LOGGER = False #When you need to see everything that happends. The Default is False
        self.attempts = 3 #The number of attempts the spider can retry if an error occurs. Default is 3
        self.waitTime = 10 #The number of seconds WebDriver will wait. Default is 10
        self.count = 0 #This saves the location of the url we are going through
        self.runTime = 0 #Total time of extractions
        self.totalRecoveries = 0 #Number of recoveries made while running
        self.maxRetryCount = 100 #Number of retrys the javascript can make Defualt is 100
        self.cleaner = DataCleaner() #Loads the cleaner
        self.load = ProductsLoader() #Loads all products
        #Selenium needs a webdriver to work. I chose Firefox however you can do another if you need too
        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))
        self.log("Driver started")
    
    #This handles the restart in case we run into an error
    def restart(self):
        self.driver.quit()
        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))
        self.log("Driver restarted")
        self.setStoreLocation()
    
    #Some stores need to have a location set
    def setStoreLocation(self):
        storeLocationUrl = 'https://shop.newpi.coop/my-store/store-locator'
        self.driver.get(storeLocationUrl)
        time.sleep(5) #Wait for the page to set
        xpath = self.load.storeXpaths[self.storeIndex]
        ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)
        elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))
        elements[0].click()
        time.sleep(5) #Wait for the page to set
        self.log("Store location set")


    #This starts the spider
    def start_requests( self ):
        self.runTime = time.time()
        self.log("Loading from ProductsLoader Class")
        self.dataFrames = self.load.DataFrames #Adds all dataframes
        self.debug("Products Loaded and Data Frames Added")
        self.debug('\n < --- Setup runtime is %s seconds --- >' % (time.time() - self.runTime))
        self.totalRecoveries = 0 
        #This sweeps through every inputed store
        for index in range(len(self.load.storeXpaths)):
            self.storeIndex = index
            self.cleaner.setStore(self.storeIndex)
            self.setStoreLocation()
            #Sweeps through all products
            for product in (Products):
                result = self.requestExtraction(product)
            self.debug("New store location data added")

        #Adds the date that the data was scraped
        currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]
        self.log("Exporting files")
        #Dataframes to CSV files
        for df, product in zip(self.dataFrames, (Products)):
            df.to_csv(currentDate + self.name +" " + product.name + ".csv")
            self.log('\n', df.to_string())
        self.debug('\n < --- Total runtime took %s seconds with %d recoveries --- >' % (time.time() - self.runTime, self.totalRecoveries))
        if len(self.skipped) != 0:
            self.debug('\n < -!- WARNING SKIPPED (' + str(len(self.skipped)) + ') DATA FOUND --->')
        self.Logs_to_file(currentDate + self.name + ' Spider Logs.txt')
        if len(self.skipped) > 0:
            self.debug(self.skipped)
            self.skipHandler(currentDate)      
        self.driver.quit()

    #This handles the extraction request for the inputed product 
    def requestExtraction(self, product):
        self.count = 0
        errors = 0
        start = time.time()
        self.debug("Starting "+ product.name)    
        for trying in range(self.attempts):
            try:
                self.makeRequest(product)
                self.debug(product.name + " Finished")    
                self.log('\n< --- ' + product.name + ' scrape took %s seconds with %d recoveries --- >\n' % ((time.time() - start), errors))
                self.totalRecoveries += errors
                return self.totalRecoveries
            except Exception as e:
                #Note sometimes the browser will closed unexpectedly and theres not we can do but restart the driver
                errors += 1
                self.debug("An error occurred:", e)
                self.debug("Recovering extraction and continueing")
                self.restart() 
        self.debug(product.name + " Did not Finished after " + str(self.attempts) + " Time wasted: %s seconds" % (time.time() - start))
        self.totalRecoveries += errors
        return self.totalRecoveries

    #This handles the reqests for each url and adds the data to the dataframe
    def makeRequest(self, product):
        productUrls = product.value[1]
        total = len(productUrls)
        while self.count < total:
            url = productUrls[self.count]
            self.driver.get(url)
            self.log("Making a request for: ", url)
            item = []
            time.sleep(1) # marionette Error Fix
            breakout = False
            for xpath in product.value[2]:
                #Retrying the xpath given the number of attempts
                for attempt in range(self.attempts):
                    data = self.javascriptXpath(xpath[0])
                    if data in {'empty', 'skip'}:
                        #speical case
                        if len(xpath) == 3:
                            #the first attempt shouldnt go through in case it when through to fast
                            #this will slow the fuction down however Accuracy > Speed 
                            if xpath[2]:
                                if attempt == 0:
                                    self.debug("Found a missing item in store. Double checking")
                                    continue
                                #example would be when there is actually is a '' in the xpath
                                self.debug("xpath marked as speical")
                                notFoundXpath = '//*[@id="products"]//*[contains(@class,"fp-text-center fp-not-found")]//*[contains(@class,"fp-text-center")]'
                                data = self.javascriptXpath(notFoundXpath)
                                if data in {'empty', 'skip'}:
                                    self.debug("Missing item retrying")
                                else:
                                    self.debug("An Item not in stock for: ", url) 
                                    item.append(data)
                                    df = self.dataFrames[product.value[0]]
                                    num = len(df.columns) - len(item) % len(df.columns)
                                    item += ["None in stock"] * (num - 2)
                                    breakout = True
                                    break
                        if xpath[1] and data == 'empty':    
                            #this is where setting the xpath to optional comes in
                            self.debug("xpath wasnt avaliable")
                            item.append(None)
                            break
                        self.debug("Missing item retrying")
                    else:  #Data found
                        item.append(data)
                        self.log(data + ' was added to the list for: ', url)
                        break
                if breakout:
                    break
                if attempt == self.attempts:
                    data = 'skip'
                if data == 'skip':  #To help clean the data we skip the item with gaps of data 
                    self.debug("An Item has been skipped for: ", url)  
                    item = ['SKIPPED']
                    #Taking the product name  dataframe number and index added as well as the url 
                    #to retry for later 
                    #This could take time to do so we do this at the very end after we made the cvs files
                    self.skipped.append([product, self.count, url])
                    break
            if 'SKIPPED' in item:
                #No point in cleaning skipped items
                items = ['SKIPPED']*(self.dataFrames[product.value[0]].shape[1] - 1)
                items.append(url)
            else:
                #We call the DataCleaner class to handle the cleaning of the data
                #Its best to clean the data before we add it to the data frame
                self.debug('Data cleaning started: ', item)
                items = self.cleaner.cleanUp(item, url)
                self.debug('Data cleaning finished: ', item)
            self.debug('Extracted: ', items)
            self.dataFrames[product.value[0]].loc[len(self.dataFrames[product.value[0]])] = items                    
            self.count += 1
            self.printer(product.name + " item added ", self.count, " of ", total, ":  ", items)

    #Collecting the data from the xpath in JavaScript is faster and results in fewer errors than doing it in python
    #This is where selenium shines because we can both use JavaScript and render JavaScript websites
    #and is the only reason why we use it instead of scrapy
    def javascriptXpath(self, xpath):
        # if the time expires it assumes xpath wasnt found in the page
        try: 
            #Waits for page to load 
            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)
            elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))

            # Runs the javascript and collects the text data from the inputed xpath
            # We want to keep repeating if we get any of these outputs becasue the page is still 
            # loading and we dont want to skip or waste time. (for fast computers)
            retrycount = 0
            invalidOutputs = {"error", 'skip' "$nan", ''}
            while retrycount < self.maxRetryCount :
                text = self.driver.execute_script("""
                    const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;
                    if (!element) {
                        return 'skip';
                    }
                    return element.textContent.trim();
                """, 
                xpath)
                checkText = text.replace(" ", "").lower()
                if checkText in invalidOutputs:
                    retrycount+=1
                else:
                    self.log(retrycount, "xpath attempts for (", text, ")")
                    return text
            self.log("xpath attempts count met. Problematic text (" + text + ") for ", xpath)
            return 'skip'
        except TimeoutException:
            self.log('Could not find xpath for: ', xpath)
            return 'empty'

           

    #This is here to hopefully fix skipped data
    #Best case sinarios this will never be used
    def skipHandler(self, currentDate):
        corrections = 0
        # skipped format
        # [product name, DataFrame number, DataFrame index, url]
        while len(self.skipped) != 0:
            #each skip 
            for index, dataSkip in enumerate(self.skipped):
                product = dataSkip[0]
                #Limiting the Attempts to fix while avoiding bottlenecking the problem
                for attempt in range(self.attempts*2):
                    product = dataSkip[0]
                    url = dataSkip[2]
                    self.driver.get(url)
                    self.log("Making a request for: ", url)
                    item = []
                    breakout = False
                    for xpath in product.value[2]:
                        for attemptIn in range(self.attempts*2):
                            if data in {'empty', 'skip'}:
                                #speical case
                                if len(xpath) == 3:
                                    #the first attempt shouldnt go through in case it when through to fast
                                    #this will slow the fuction down however Accuracy > Speed 
                                    if xpath[2]:
                                        if attempt == 0:
                                            self.debug("Found a missing item in store. Double checking")
                                            continue
                                        #example would be when there is actually is a '' in the xpath
                                        self.debug("xpath marked as speical")
                                        notFoundXpath = '//*[@id="products"]//*[contains(@class,"fp-text-center fp-not-found")]//*[contains(@class,"fp-text-center")]'
                                        data = self.javascriptXpath(notFoundXpath)
                                        if data in {'empty', 'skip'}:
                                            self.debug("Missing item retrying")
                                        else:
                                            self.debug("An Item not in stock for: ", url) 
                                            item.append(data)
                                            df = self.dataFrames[product.value[0]]
                                            num = len(df.columns) - len(item) % len(df.columns)
                                            item += ["None in stock"] * (num - 2)
                                            breakout = True
                                            break
                                if xpath[1] and data == 'empty':    
                                    #this is where setting the xpath to optional comes in
                                    self.debug("xpath wasnt avaliable")
                                    item.append(None)
                                    break
                                self.debug("Missing item retrying")
                            else:  #Data found
                                item.append(data)
                                self.log(data + ' was added to the list for: ', url)
                                break
                        if breakout:
                            break
                    if breakout:
                        break
                    if attemptIn == self.attempts*2:
                        data = 'skip'
                        break
                if data == 'skip':  #To help clean the data we skip the item with gaps of data 
                    self.debug("Item still missing attempting other skipped for now") 
                else:
                    items = self.cleaner.cleanUp(item, url)
                    self.dataFrames[dataSkip[1]].loc[dataSkip[2]] = items                    
                    self.printer("Fixed " + product.name + " item: ", items)
                    #To avoid infinite loops and never saving our data we save the file now
                    self.dataFrames[product.value[0]].to_csv(currentDate + "REPAIRED Gateway Market " + product.name + ".csv")
                    self.debug('\n < --- Total runtime with saving of repairs took %s seconds --- >' % (time.time() - self.runTime))
                    self.Logs_to_file(currentDate + self.name + ' Spider REPAIR Logs.txt')
                    #To avoid fixing fixed items we pop, mark, and break
                    self.skipped.pop(index)
                    corrections += 1
                    break
        self.debug('\n < --- Total runtime with all repairs took %s seconds --- >' % (time.time() - self.runTime))
        self.Logs_to_file(currentDate + self.name + ' spider COMPLETED REPAIR Logs.txt')

# Start
#DEBUG Switch
SHOW = True

#Spider setup
spider = NewPioneerSpider()
spider.LOGGER = True
spider.DEBUGGER = True

#Running the spider
spider.start_requests()

if(SHOW):
    [print(dataFrame) for dataFrame in spider.dataFrames]
    spider.printLogs()
```

This is the code for the Russ's Market Scraper

```{python}
#Imports
from datetime import datetime
import pandas as pd
from enum import Enum
#Imports for Scraping
import selenium
from selenium import webdriver
from selenium.webdriver.firefox.service import Service as FirefoxService
from webdriver_manager.firefox import GeckoDriverManager
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import StaleElementReferenceException
from selenium.common.exceptions import WebDriverException
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from os import path
import time
import sys


#Creator's Note: Products(Enum) and ProductsLoader is probably the only classes you need to edit 
#unless you need to change the way the data is cleaned. Which handled in the DataCleaner class

#These class is here so that we can expand to differnet products easier making the spider more dynamic and expandable
class Products(Enum):
    #Add products like this ProductName = index iteration, [], [] 
    #the 2 empty list will be filled in using the ProductsLoader class
    Bacon = 0, [], []
    Eggs = 1, [], []
    HeirloomTomatoes = 2, [], []

    # Helper method to reduce code for adding to the products and weed out duplicate inputs
    # if you type something in really wrong code will stop the setup is important 
    # correct index inputs are correct index number, url, urls, xpath, xpaths
    def addToProduct(self, items, index):
        product = None
        if isinstance(index, int):
            product = self.value[index]
        elif isinstance(index, str):
            if index.lower() in ['urls', 'url']:
                product = self.value[1]
            elif index.lower() in ['xpaths', 'xpath']:
                product = self.value[2]
        if product == None:
            raise ValueError(f"Invalid index input for ({index}) for input: {items}")
        #Sets are fast at finding dups so we use them for speed
        product_set = set(product)
        for item in items:
            if item not in product_set:
                product.append(item)
                product_set.add(item)

#this class loads the xpaths and urls to the Products Enum and adds dataframes to the spider
class ProductsLoader():
    DataFrames = []
    def __init__(self):
        self.dataFrameAdder()
        self.urlsAdder()
        self.xpathMaker()

    #This adds the dataframe to the spider on load
    def dataFrameAdder(self):
        #Dataframes (You can add more here)
        baconFrame = pd.DataFrame(columns=['Bacon', 'Current Price', 'Weight', 'Sale Price', 'Location', 'Url'])
        eggFrame = pd.DataFrame(columns=['Egg', 'Current Price', 'Weight', 'Sale Price', 'Location', 'Url'])
        tomatoFrame = pd.DataFrame(columns=['Heirloom Tomato', 'Current Price', 'Weight', 'Sale Price', 'Location', 'Url'])
        self.DataFrames = [baconFrame,
                           eggFrame,
                           tomatoFrame
                          ]

    #Adding Urls to products
    def urlsAdder(self):
        BaconUrls = ['https://www.russmarket.com/shop/meat/bacon/sliced/hormel_black_label_thick_cut_bacon_1_lb/p/229335',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/hormel_black_label_original_bacon_16_ounce/p/2349369',
                     'https://www.russmarket.com/shop/meat/bacon/mullan_road_bacon/p/5220311',
                     'https://www.russmarket.com/shop/meat/bacon/prairie_fresh_signature_applewood_smoked_bacon_pork_loin_filet_27_2_oz/p/6828650',
                     'https://www.russmarket.com/shop/meat/bacon/farmland_bacon_thick_cut_naturally_hickory_smoked/p/571658',
                     'https://www.russmarket.com/shop/meat/bacon/sliced_slab_bacon/p/1564405684712590572',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/smithfield_naturally_hickory_smoked_hometown_original_bacon/p/3142755',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/jamestown_economy_sliced_bacon_16_oz/p/180026',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/smithfield_naturally_hickory_smoked_thick_cut_bacon_12_oz_pack/p/3142757',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/smithfield_bacon_thick_cut_12_oz/p/2101085',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/farmland_naturally_hickory_smoked_classic_cut_bacon_16_oz/p/2376191',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/farmland_naturally_hickory_smoked_thick_cut_bacon_16_oz/p/95721',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/farmland_naturally_applewood_smoked_bacon_16_oz/p/585815',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/hormel_black_label_microwave_ready_bacon/p/26151',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/oscar_mayer_original_bacon_16_oz/p/32303',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/big_buy_hardwood_smoked_bacon/p/2101073',
                     'https://www.russmarket.com/shop/meat/bacon/turkey/oscar_mayer_turkey_bacon_original_12_oz/p/39809',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/farmland_bacon_low_sodium_hickory_smoked_16_oz/p/2376192',
                     'https://www.russmarket.com/shop/meat/bacon/canadian/farmland_bacon_double_smoked_double_thick_cut_12_oz/p/1564405684713224952',
                     'https://www.russmarket.com/shop/meat/bacon/canadian/hormel_black_label_naturally_hardwood_smoked_97_fat_free_canadian_bacon_6_oz_zip_pak/p/26168',
                     'https://www.russmarket.com/shop/meat/bacon/sliced/oscar_mayer_bacon_original_8_oz/p/32302',
                    ]

        EggUrls = [
                   'https://www.russmarket.com/shop/dairy/eggs/everyday/best_choice_grade_a_large_eggs/p/3139172',
                   'https://www.russmarket.com/shop/dairy/eggs/everyday/best_choice_large_eggs/p/3139176',
                   'https://www.russmarket.com/shop/dairy/eggs/everyday/best_choice_jumbo_eggs/p/3139173',
                   'https://www.russmarket.com/shop/dairy/eggs/everyday/best_choice_extra_large_eggs/p/3139174',
                   'https://www.russmarket.com/shop/dairy/eggs/everyday/best_choice_large_eggs/p/3139192',
                   'https://www.russmarket.com/shop/dairy/eggs/everyday/eggland_s_best_large_eggs_12_ea/p/54283',
                   'https://www.russmarket.com/shop/dairy/eggs/everyday/eggland_s_best_large_eggs_18_ea/p/54279',
                   'https://www.russmarket.com/shop/dairy/cheese/artisan_and_specialty/best_choice_hard_cook_eggs_6_ct/p/1564405684702358019',
                  ]
        HeirloomTomatoesUrls = ['https://www.russmarket.com/shop/produce/fresh_vegetables/tomatoes/heirloom_tomatoes/p/12412']
        
        Products.Bacon.addToProduct(BaconUrls,'urls')
        Products.Eggs.addToProduct(EggUrls,'urls')
        Products.HeirloomTomatoes.addToProduct(HeirloomTomatoesUrls,'urls')

    #This handles the xpaths by adding to the Products class
    #most websites have simular xpaths for each item. You might need to make differnet xpaths for each item 
    #if that is the case
    #For assigning xpaths mark them if they are optional meaning it could or could not be present on the page 
    #we do this for speed up if you mark it as non optional and its not pressent it will skip the value 
    #and hurt the preformence
    #best practice is to render the optional last so it reduces the chances of skipping 
    #Note spiecal cases do happen but they are extremely rare a good indiaction of finding one 
    #is by using skipHandler method and tracking/watching the logs  
    #IMPORTANT < -!- NOT ALL XPATHS ARE THE SAME FOR EACH PRODUCT -!->
    def xpathMaker(self):
        #Add the xpaths here and mark if they are optional
        #Format [xpath, optional, speical]
        #xpath, Optional
        
        nameXpath = '//*[@id="page-title"]//h1[contains(@class,"fp-page-header fp-page-title")]'
        priceXpath = '//*[@id="page-title"]//*[contains(@class,"fp-item-price")]/span[contains(@class,"fp-item-base-price")]'
        weightXpath = '//*[@id="page-title"]//*[contains(@class,"fp-item-price")]/span[contains(@class,"fp-item-size")]' 
        saleXpath = '//*[@id="page-title"]//*[contains(@class,"fp-item-sale")]/span[contains(@class,"fp-item-sale-date")]/strong' #optional

        xpathList = [(nameXpath, False),
                     (priceXpath, False),
                     (weightXpath, False),
                     (saleXpath, True)]

        Products.Bacon.addToProduct(xpathList,'xpath')
        Products.Eggs.addToProduct(xpathList,'xpath')
        Products.HeirloomTomatoes.addToProduct(xpathList,'xpath')

class DataCleaner():
    DataArray = []
    def cleanUp(self, item, url):
        self.DataArray = item
        self.DataArray.append("900 South Locust Street Glenwood, IA 51534 ")
        self.DataArray.append(url)
        return self.DataArray


class RussMarketSpider():
    name = "Russ Market"    #The store name 
    spiderLogs = []         #The logs 
    skipped = []            #Skipped data 

    #These are methods that are available for your convences
    def log(self, *args):
        self.spiderLogs.append(('Logger:', args))
        if self.LOGGER:
            print('Logger:', *args)

    def debug(self, *args):
        self.spiderLogs.append(('Debug:', args))
        if self.DEBUGGER:
            print('Debug:', *args)
    
    def printer(self, *args):
        self.spiderLogs.append(('Printer:', args))
        print(*args)
    
    def printLogs(self):
        print("\n< --- Printing Logs --- >\n")
        for entry in self.spiderLogs:
            print(*entry)

    def Logs_to_file(self, filename):
        with open(filename, 'w') as file:
            for log_entry in self.spiderLogs:
                file.write('{} {}\n'.format(log_entry[0], log_entry[1]))
    
    def __init__(self):
        self.DEBUGGER = False #The debugger switch to see whats going on. The Default is False
        self.LOGGER = False #When you need to see everything that happends. The Default is False
        self.attempts = 3 #The number of attempts the spider can retry if an error occurs. Default is 3
        self.waitTime = 10 #The number of seconds WebDriver will wait. Default is 10
        self.count = 0 #This saves the location of the url we are going through
        self.runTime = 0 #Total time of extractions
        self.totalRecoveries = 0 #Number of recoveries made while running
        self.maxRetryCount = 100 #Number of retrys the javascript can make Defualt is 100
        self.cleaner = DataCleaner() #Loads the cleaner
        #Selenium needs a webdriver to work. I chose Firefox however you can do another if you need too
        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))
        self.log("Driver started")
    
    #This handles the restart in case we run into an error
    def restart(self):
        self.driver.quit()
        self.driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))
        self.log("Driver restarted")
        self.setStoreLocation()

    #Some stores need to have a location set
    def setStoreLocation(self):
        storeLocationUrl = 'https://www.russmarket.com/shop#!/?store_id=6158'
        self.driver.get(storeLocationUrl)
        time.sleep(5) #Wait for the page to set
        self.log("Store location set")

    #This starts the spider
    def start_requests( self ):
        self.runTime = time.time()
        self.setStoreLocation()
        self.log("Loading from ProductsLoader Class")
        load = ProductsLoader() #Loads all products
        self.dataFrames = load.DataFrames #Adds all dataframes
        self.debug("Products Loaded and Data Frames Added")
        self.debug('\n < --- Setup runtime is %s seconds --- >' % (time.time() - self.runTime))
        self.totalRecoveries = 0 
        #Sweeps through all products
        for product in (Products):
            result = self.requestExtraction(product)
        #Adds the date that the data was scraped
        currentDate = str(datetime(datetime.today().year, datetime.today().month, datetime.today().day))[:-8]
        self.log("Exporting files")
        #Dataframes to CSV files
        for df, product in zip(self.dataFrames, (Products)):
            df.to_csv(currentDate + self.name +" " + product.name + ".csv")
            self.log('\n', df.to_string())
        self.debug('\n < --- Total runtime took %s seconds with %d recoveries --- >' % (time.time() - self.runTime, self.totalRecoveries))
        if len(self.skipped) != 0:
            self.debug('\n < -!- WARNING SKIPPED (' + str(len(self.skipped)) + ') DATA FOUND --->')
        self.Logs_to_file(currentDate + self.name + ' Spider Logs.txt')
        if len(self.skipped) > 0:
            self.debug(self.skipped)
            self.skipHandler(currentDate)      
        self.driver.quit()

    #This handles the extraction request for the inputed product 
    def requestExtraction(self, product):
        self.count = 0
        errors = 0
        start = time.time()
        self.debug("Starting "+ product.name)    
        for trying in range(self.attempts):
            try:
                self.makeRequest(product)
                self.debug(product.name + " Finished")    
                self.log('\n< --- ' + product.name + ' scrape took %s seconds with %d recoveries --- >\n' % ((time.time() - start), errors))
                self.totalRecoveries += errors
                return self.totalRecoveries
            except Exception as e:
                #Note sometimes the browser will closed unexpectedly and theres not we can do but restart the driver
                errors += 1
                self.debug("An error occurred:", e)
                self.debug("Recovering extraction and continueing")
                self.restart() 
        self.debug(product.name + " Did not Finished after " + str(self.attempts) + " Time wasted: %s seconds" % (time.time() - start))
        self.totalRecoveries += errors
        return self.totalRecoveries

    #This handles the reqests for each url and adds the data to the dataframe
    def makeRequest(self, product):
        productUrls = product.value[1]
        total = len(productUrls)
        while self.count < total:
            url = productUrls[self.count]
            self.driver.get(url)
            self.log("Making a request for: ", url)
            item = []
            time.sleep(1) # marionette Error Fix
            for xpath in product.value[2]:
                #Retrying the xpath given the number of attempts
                for attempt in range(self.attempts):
                    data = self.javascriptXpath(xpath[0])
                    if data in {'empty', 'skip'}:
                        #speical case in case you need it
                        if len(xpath) == 3:
                            if xpath[2]:
                                #example would be when there is actually is a '' in the xpath
                                self.debug("xpath marked as speical")
                                item.append(None)
                                data = 'speical'
                                break
                        if xpath[1] and data == 'empty':    
                            #this is where setting the xpath to optional comes in
                            self.debug("xpath wasnt avaliable")
                            item.append(None)
                            break
                        self.debug("Missing item retrying")
                    else:  #Data found
                        item.append(data)
                        self.log(data + ' was added to the list for: ', url)
                        break
                if attempt == self.attempts:
                    data = 'skip'   
                if data == 'skip':  #To help clean the data we skip the item with gaps of data 
                    self.debug("An Item has been skipped for: ", url)  
                    item = ['SKIPPED']
                    #Taking the product name  dataframe number and index added as well as the url 
                    #to retry for later 
                    #This could take time to do so we do this at the very end after we made the cvs files
                    self.skipped.append([product, self.count, url])
                    break
            if 'SKIPPED' in item:
                #No point in cleaning skipped items
                items = ['SKIPPED']*(self.dataFrames[product.value[0]].shape[1] - 1)
                items.append(url)
            else:
                #We call the DataCleaner class to handle the cleaning of the data
                #Its best to clean the data before we add it to the data frame
                self.debug('Data cleaning started: ', item)
                items = self.cleaner.cleanUp(item, url)
                self.debug('Data cleaning finished: ', item)
            self.debug('Extracted: ', items)
            self.dataFrames[product.value[0]].loc[len(self.dataFrames[product.value[0]])] = items                    
            self.count += 1
            self.printer(product.name + " item added ", self.count, " of ", total, ":  ", items)

    #Collecting the data from the xpath in JavaScript is faster and results in fewer errors than doing it in python
    #This is where selenium shines because we can both use JavaScript and render JavaScript websites
    #and is the only reason why we use it instead of scrapy
    def javascriptXpath(self, xpath):
        # if the time expires it assumes xpath wasnt found in the page
        try: 
            #Waits for page to load 
            ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)
            elements = WebDriverWait(self.driver, self.waitTime, ignored_exceptions=ignored_exceptions).until(EC.presence_of_all_elements_located((By.XPATH, xpath)))

            # Runs the javascript and collects the text data from the inputed xpath
            # We want to keep repeating if we get any of these outputs becasue the page is still 
            # loading and we dont want to skip or waste time. (for fast computers)
            retrycount = 0
            invalidOutputs = {"error", 'skip' "$nan", ''}
            while retrycount < self.maxRetryCount :
                text = self.driver.execute_script("""
                    const element = document.evaluate(arguments[0], document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;
                    if (!element) {
                        return 'skip';
                    }
                    return element.textContent.trim();
                """, 
                xpath)
                checkText = text.replace(" ", "").lower()
                if checkText in invalidOutputs:
                    retrycount+=1
                else:
                    self.log(retrycount, "xpath attempts for (", text, ")")
                    return text
            self.log("xpath attempts count met. Problematic text (" + text + ") for ", xpath)
            return 'skip'
        except TimeoutException:
            self.log('Could not find xpath for: ', xpath)
            return 'empty'

           

    #This is here to hopefully fix skipped data
    #Best case sinarios this will never be used
    def skipHandler(self, currentDate):
        corrections = 0
        # skipped format
        # [product name, DataFrame number, DataFrame index, url]
        while len(self.skipped) != 0:
            #each skip 
            for index, dataSkip in enumerate(self.skipped):
                product = dataSkip[0]
                #Limiting the Attempts to fix while avoiding bottlenecking the problem
                for attempt in range(self.attempts*2):
                    product = dataSkip[0]
                    url = dataSkip[2]
                    self.driver.get(url)
                    self.log("Making a request for: ", url)
                    item = []
                    for xpath in product.value[2]:
                        for attemptIn in range(self.attempts*2):
                            data = self.javascriptXpath(xpath[0])
                            if data in {'empty', 'skip'}:   
                                if xpath[1] and data == 'empty':    
                                    #this is where setting the xpath to optional comes in
                                    self.debug("xpath wasnt avaliable")
                                    item.append(None)
                                    break
                                self.debug("Missing item retrying")
                            else:  #Data found
                                item.append(data)
                                self.log(data + ' was added to the list for: ', url)
                                break
                        if attemptIn == self.attempts*2:
                            data = 'skip'
                            break
                if data == 'skip':  #To help clean the data we skip the item with gaps of data 
                    self.debug("Item still missing attempting other skipped for now") 
                else:
                    items = self.cleaner.cleanUp(item, url)
                    self.dataFrames[dataSkip[1]].loc[dataSkip[2]] = items                    
                    self.printer("Fixed " + product.name + " item: ", items)
                    #To avoid infinite loops and never saving our data we save the file now
                    self.dataFrames[product.value[0]].to_csv(currentDate + "REPAIRED Gateway Market " + product.name + ".csv")
                    self.debug('\n < --- Total runtime with saving of repairs took %s seconds --- >' % (time.time() - self.runTime))
                    self.Logs_to_file(currentDate + self.name + ' Spider REPAIR Logs.txt')
                    #To avoid fixing fixed items we pop, mark, and break
                    self.skipped.pop(index)
                    corrections += 1
                    break
        self.debug('\n < --- Total runtime with all repairs took %s seconds --- >' % (time.time() - self.runTime))
        self.Logs_to_file(currentDate + self.name + ' spider COMPLETED REPAIR Logs.txt')
        
# Start
#DEBUG Switch
SHOW = True

#Spider setup
spider = RussMarketSpider()
spider.LOGGER = True
spider.DEBUGGER = True

#Running the spider
spider.start_requests()

if(SHOW):
    [print(dataFrame) for dataFrame in spider.dataFrames]
    spider.printLogs()
```
