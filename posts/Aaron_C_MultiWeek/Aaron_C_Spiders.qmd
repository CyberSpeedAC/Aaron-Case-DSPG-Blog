---
title: "Spiders"
author: "Aaron Case"
date: "2023-06-16"
categories: [Week Four, Week Five, Code]
execute: 
  enabled: false
---

Web scraping in essence is the process of extracting data from websites using automated software. As for the process, the web-scraping pipeline goes as follows.

1.  Setup - Understand what we want to do and find sources to help us do it.
2.  Acquisition
    -   Read in the raw data from online.
    -   Format these data to be usable.
    -   Accessing the data.
    -   Parsing this information.
    -   Extracting these data into meaningful and useful data structures.
3.  Processing - There are many options to this but the main goal is to run the downloaded data through whatever analyses or processes needed to achieve the desired goal.

There are many web scraping tools available, but two of the one's that I've the most success in are Scrapy and Selenium.

Scrapy is a Python package that is primarily used for web scraping. Scrapy provides a framework that offers a simple web crawling tool to extract data from websites. These tools are commonly known as spiders and allow for easy extraction of desired information. A spider program can crawl through websites and extracts data from them (Just like real spiders crawl through there webs). Scrapy works by sending HTTP requests to websites and receiving HTML responses, then the spider parses the HTML to extract the data it needs. There is a Data camp course that is currently available to use and is highly recommended watching. These are some of benefit of using Scrapy that I've found:

1.  Scrapy is designed to be fast and can handle large volumes of data. It's built on top of Twisted, an asynchronous networking framework, which in simple terms makes it very fast. It can handle thousands of requests per second without slowing down. This makes it really handy for the scope of this project.
2.  Scrapy is highly customizable and can be configured to work with many different websites and data formats. Allowing us to specify how the spider should act when navigating through websites and extract data that we told it to collect. Scrapy also provides features for handling cookies, redirects, and other HTTP features.
3.  Scrapy comes with built-in support for XPath and CSS selectors. With the right skill set these tools provide a powerful and easy way to extract data from HTML and XML documents.
4.  Scrapy automatically comes with built-in support for data cleaning, Making the the cleaning and normalize scraped data much easier.

As for the cons of Scrapy:

1.  Scrapy does have a relatively steep learning curve to it. Especially if you are new to web scraping, XPath/CSS selectors, or asynchronous programming. Unless you have more than a week to spend learning and doing web scrapping it is best to spend the time doing another method. Not to mention the fact that you need to have some knowledge of Python to use it effectively
2.  Scrapy does not have built-in support for JavaScript, which means that it cannot scrape websites and other dynamic web content that rely heavily on JavaScript for rendering content. Which a vast amount of websites rely on to function.
3.  This tool is not perfect nor undetectable. Some websites may block Scrapy from accessing their content, especially if they detect that the traffic is coming from a bot.
4.  Scrapy does help you in cleaning data however it is not perfect. You still need to spend some time cleaning the data that was collected and may require additional libraries, such as Pandas or NumPy, to process and analyze scraped data.

Selenium is another Python package which is used for automating web browsers. It allows you to control a browser programatically, which mimics the actions of a what a user will do. Selenium uses a web driver to control your web browser and can interact with several different browsers (such as Chrome, Firefox, and Safari). Selenium works by opening up a web browser and navigates to the target website. The user can then interact with the website using Selenium commands. Just like Scrapy, Selenium can also extract data from the website using XPath and CSS selectors. As for some of benefit of using Selenium:

1.  Selenium has the ability to handle JavaScript and other dynamic web content. Allowing it to handle websites that Scrapy is unable too.
2.  Selenium can handle more complex interactions with websites, such as clicking buttons, filling out forms, scrolling, etc. which can simulate user behavior. Making Selenium more diverse and a power tool to use when doing specific tasks that require a more human like approach.

For the Cons of Selenium:

1.  Selenium can be really slow and very resource-intensive. Especially when automating complex tasks or interacting with multiple pages. It requires your web browser to be opened to run meaning If your internet or computer is slow it has a really hard time doing anything you want it. This time that you have to wait for the program to preform any simple task is not ideal when working through large data sets.
2.  It is prone to errors and requires extensive debugging and testing. For example, if say that you close your browser or if it freezes (for long periods of time) the program will not function correctly and throw an error.
3.  It can be very challenging to set up and configure. Not to mention that It can be difficult to maintain test scripts when the web application is updated or changed. Meaning if you are not dedicated in learning or maintaining your code it will come with a bunch of headaches down the road.

Ideally for this project I strongly recommend using Scrapy spiders were ever possible. Scrapy's speed and efficiency and overall better performance makes this part of the process much more tolerable in building spiders. However when it comes to more complex tasks and websites Selenium is the way to go. I would also like to acknowledge that there are other tools and resources that could be explored and used which may be better or more effenect for the scope of the project. Theses two tools are just some that I found and was able to implement in the time frame I was given. For example, there is a python package called Scrapy Splash. This works as an extension for Scrapy which provides a JavaScript rendering service. Allowing Scrapy to handle JavaScript and other dynamic web content, similar to Selenium. I wasn't able to master this tool nor could I successfully get it this tool to work. If I had more time to develop a clear comprehension of this utility and fully understood how to get it to work. It would have be the tool of choice to handle such websites. I used Selenium since I had more success in getting it to work in comparison.

This code covers the bare minimum of Scrapy and is rather (hopefully) intuitive. This can be edited/modified to fit your needs and is just an example.

```{Python}
# python
# Imports for Scrapy Example
import scrapy #Required Import
from scrapy.crawler import CrawlerProcess
from scrapy.utils.log import configure_logging # For debugging

# The Class name can be anything but you need to call this later
class SpiderClassName(scrapy.Spider):
    name = "spider name" 

    def start_requests( self ): # This is required to start the spider
        # For one url
        Desired_Website = [ 'https://www.example.com' ]
        yield scrapy.Request( url = Desired_Website, callback = self.Some_Function_Name)

        # For many urls
        urls = [ 'https://www.examples.com', ... ]
        for url in urls:
            yield scrapy.Request( url = url, callback = self.parseAndFollow)

        # When passing in varables from one function to the next you use meta
        for url in urls:
            yield scrapy.Request( url = url, callback = self.Meta_Example, meta={'Meta_Name': data, ...})
    
    def parseAndFollow( self, response ):
        # Direct to the links in the xpath
        Extrated_XPath = '//tag-name(@attribute, "attrib info")'
        XPath_links = Extrated_XPath.xpath( './@href' )
        # Anoter way
        XPath_links = Extrated_XPath.xpath('//tag-name(@attribute, "attrib info")/@href')
        # Extract the links (as a list of strings)
        links_to_follow = XPath_links.extract()
        # Follow the links to the next parser
        for url in links_to_follow:
            yield response.follow( url = url, callback = self.parse )
	
    def parse( self, response ):
        # To get the text of the xpath
        Title_Example = response.xpath('//h1[contains(@class,"title")]/text()') 
        # Extract text title
        Title_Extracted = Title_Example.extract_first()
        # Clean the title text
        Title_Text = Title_Extracted.strip()
        # Saving the text so that we dont lose what we just did
        Text_Element_List.append(Title_Text)

    def Meta_Example( self, response ):
        #This is how pass in your arguments
        Meta_Data = response.meta.get('Meta_Name')
    
Text_Element_List = []
# To see the inner mechanics of the spider helpful to have not required
configure_logging()
# initiate a CrawlerProcess
process = CrawlerProcess()
# Tell the process which spider to use
process.crawl(SpiderClassName)
# Start the crawling process
process.start()
# This is to stop the spider
process.stop()
# At this point the spider is finished and now we have everything that was saved
print(Text_Element_List)

```

As for Selenium this code covers the bare minimum and serves as a deminstration of how to handle JavaScript websites. Once again this can be edited/modified to fit your needs and is just an example.

```{Python}
# python
# Imports for Scraping
import selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Since I use Firefox this is how I would typically set it up.
# Note: This is not the only way to do it. Its just how I do this part 
from selenium.webdriver.firefox.service import Service as FirefoxService
from webdriver_manager.firefox import GeckoDriverManager
from os import path
driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install(), log_path=path.devnull))

# Another example of how you can do it 
# Set the path to the driver
driver_path = "/path/to/chromedriver"
# Create a new instance of the Chrome driver
driver = webdriver.Chrome(driver_path)

# Navigate to the website page
driver.get("https://example.com/")

# Setup
waitTime = 10

# I like using XPaths because I am most comfortable using them 
Title_XPath = '//h1(@class,"title")'
# We need to wait for the page to render before we do anything otherwise the data we want wont be present.
elements = WebDriverWait(driver, waitTime).until(EC.visibility_of_element_located((By.XPATH, Title_XPath)))
# You can also do it like this for CSS
elements = WebDriverWait(driver, waitTime).until(EC.visibility_of_element_located((By.CSS_SELECTOR, Title_CSS)))
# Extracts the text of the element
Text = elements[0].text

# Close the browser
driver.quit()

#Data Has been extracted
print(Text)
```
